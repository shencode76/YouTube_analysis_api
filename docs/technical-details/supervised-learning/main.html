<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>DSAN-5000: Project – Supervised Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../assets/gu-logo.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">DSAN-5000: Project</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../report/report.html"> 
<span class="menu-text">Report</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-technical-details" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Technical details</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-technical-details">    
        <li>
    <a class="dropdown-item" href="../../technical-details/data-collection/main.html">
 <span class="dropdown-text">Data-collection</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../technical-details/data-cleaning/main.html">
 <span class="dropdown-text">Data-cleaning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../technical-details/eda/main.html">
 <span class="dropdown-text">Exploratory Data Analysis</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../technical-details/unsupervised-learning/main.html">
 <span class="dropdown-text">Unsupervised Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../technical-details/supervised-learning/main.html">
 <span class="dropdown-text">Supervised Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../technical-details/progress-log.html">
 <span class="dropdown-text">Progress Log</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../technical-details/llm-usage-log.html">
 <span class="dropdown-text">LLM usage Log</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-and-motivation" id="toc-introduction-and-motivation" class="nav-link active" data-scroll-target="#introduction-and-motivation">Introduction and Motivation:</a></li>
  <li><a href="#overview-of-methods-gpt4o" id="toc-overview-of-methods-gpt4o" class="nav-link" data-scroll-target="#overview-of-methods-gpt4o">Overview of Methods<span class="citation" data-cites="gpt4o"><sup>1</sup></span></a>
  <ul class="collapse">
  <li><a href="#regression-section" id="toc-regression-section" class="nav-link" data-scroll-target="#regression-section">Regression Section</a>
  <ul class="collapse">
  <li><a href="#random-forest-regressor" id="toc-random-forest-regressor" class="nav-link" data-scroll-target="#random-forest-regressor">Random Forest Regressor</a></li>
  <li><a href="#linear-regressor" id="toc-linear-regressor" class="nav-link" data-scroll-target="#linear-regressor">Linear Regressor</a></li>
  <li><a href="#gradient-boosting-regressor" id="toc-gradient-boosting-regressor" class="nav-link" data-scroll-target="#gradient-boosting-regressor">Gradient Boosting Regressor</a></li>
  </ul></li>
  <li><a href="#binary-classification-section" id="toc-binary-classification-section" class="nav-link" data-scroll-target="#binary-classification-section">Binary Classification Section</a>
  <ul class="collapse">
  <li><a href="#random-forest-classifier" id="toc-random-forest-classifier" class="nav-link" data-scroll-target="#random-forest-classifier">Random Forest Classifier</a></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression">Logistic Regression</a></li>
  <li><a href="#gradient-boosting-classifier" id="toc-gradient-boosting-classifier" class="nav-link" data-scroll-target="#gradient-boosting-classifier">Gradient Boosting Classifier</a></li>
  </ul></li>
  <li><a href="#multi-class-classification-section" id="toc-multi-class-classification-section" class="nav-link" data-scroll-target="#multi-class-classification-section">Multi-Class Classification Section</a>
  <ul class="collapse">
  <li><a href="#gradient-boosting-classifier-1" id="toc-gradient-boosting-classifier-1" class="nav-link" data-scroll-target="#gradient-boosting-classifier-1">Gradient Boosting Classifier</a></li>
  <li><a href="#k-nearest-neighbors-knn" id="toc-k-nearest-neighbors-knn" class="nav-link" data-scroll-target="#k-nearest-neighbors-knn">K-Nearest Neighbors (KNN)</a></li>
  <li><a href="#decision-tree-classifier" id="toc-decision-tree-classifier" class="nav-link" data-scroll-target="#decision-tree-classifier">Decision Tree Classifier</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#code" id="toc-code" class="nav-link" data-scroll-target="#code">Code</a></li>
  <li><a href="#regression-section-1" id="toc-regression-section-1" class="nav-link" data-scroll-target="#regression-section-1">Regression section</a>
  <ul class="collapse">
  <li><a href="#random-forest-regressor-1" id="toc-random-forest-regressor-1" class="nav-link" data-scroll-target="#random-forest-regressor-1">Random Forest Regressor</a></li>
  <li><a href="#linear-regressor-1" id="toc-linear-regressor-1" class="nav-link" data-scroll-target="#linear-regressor-1">Linear Regressor</a></li>
  <li><a href="#gradient-boosting-regressor-1" id="toc-gradient-boosting-regressor-1" class="nav-link" data-scroll-target="#gradient-boosting-regressor-1">Gradient Boosting Regressor</a></li>
  </ul></li>
  <li><a href="#binary-classification" id="toc-binary-classification" class="nav-link" data-scroll-target="#binary-classification">Binary classification</a>
  <ul class="collapse">
  <li><a href="#random-forest-classifier-1" id="toc-random-forest-classifier-1" class="nav-link" data-scroll-target="#random-forest-classifier-1">Random Forest classifier</a></li>
  <li><a href="#logistic-regression-1" id="toc-logistic-regression-1" class="nav-link" data-scroll-target="#logistic-regression-1">Logistic Regression</a></li>
  <li><a href="#gradient-boosting-classifier-2" id="toc-gradient-boosting-classifier-2" class="nav-link" data-scroll-target="#gradient-boosting-classifier-2">Gradient Boosting Classifier</a></li>
  </ul></li>
  <li><a href="#multi-class-classification-section-1" id="toc-multi-class-classification-section-1" class="nav-link" data-scroll-target="#multi-class-classification-section-1">Multi-class classification section</a>
  <ul class="collapse">
  <li><a href="#gradient-boosting-classifier-3" id="toc-gradient-boosting-classifier-3" class="nav-link" data-scroll-target="#gradient-boosting-classifier-3">Gradient Boosting Classifier</a></li>
  <li><a href="#decision-tree-classifier-1" id="toc-decision-tree-classifier-1" class="nav-link" data-scroll-target="#decision-tree-classifier-1">Decision Tree Classifier</a></li>
  <li><a href="#k-nearest-neighbors-knn-1" id="toc-k-nearest-neighbors-knn-1" class="nav-link" data-scroll-target="#k-nearest-neighbors-knn-1">K-Nearest Neighbors (KNN)</a></li>
  </ul></li>
  <li><a href="#summary-and-interpretation-of-results" id="toc-summary-and-interpretation-of-results" class="nav-link" data-scroll-target="#summary-and-interpretation-of-results">Summary and Interpretation of Results</a>
  <ul class="collapse">
  <li><a href="#regression-section-2" id="toc-regression-section-2" class="nav-link" data-scroll-target="#regression-section-2">Regression Section</a>
  <ul class="collapse">
  <li><a href="#random-forest-regressor-2" id="toc-random-forest-regressor-2" class="nav-link" data-scroll-target="#random-forest-regressor-2">Random Forest Regressor</a></li>
  <li><a href="#linear-regression" id="toc-linear-regression" class="nav-link" data-scroll-target="#linear-regression">Linear Regression</a></li>
  <li><a href="#gradient-boosting-regressor-2" id="toc-gradient-boosting-regressor-2" class="nav-link" data-scroll-target="#gradient-boosting-regressor-2">Gradient Boosting Regressor</a></li>
  <li><a href="#discussion-for-regression-section" id="toc-discussion-for-regression-section" class="nav-link" data-scroll-target="#discussion-for-regression-section">Discussion for Regression Section</a></li>
  </ul></li>
  <li><a href="#binary-classification-section-1" id="toc-binary-classification-section-1" class="nav-link" data-scroll-target="#binary-classification-section-1">Binary Classification Section</a>
  <ul class="collapse">
  <li><a href="#random-forest-classifier-2" id="toc-random-forest-classifier-2" class="nav-link" data-scroll-target="#random-forest-classifier-2">Random Forest Classifier</a></li>
  <li><a href="#logistic-regression-2" id="toc-logistic-regression-2" class="nav-link" data-scroll-target="#logistic-regression-2">Logistic Regression</a></li>
  <li><a href="#gradient-boosting-classifier-4" id="toc-gradient-boosting-classifier-4" class="nav-link" data-scroll-target="#gradient-boosting-classifier-4">Gradient Boosting Classifier</a></li>
  <li><a href="#discussion-for-binary-classification-section" id="toc-discussion-for-binary-classification-section" class="nav-link" data-scroll-target="#discussion-for-binary-classification-section">Discussion for Binary Classification Section</a></li>
  </ul></li>
  <li><a href="#multi-class-classification-section-2" id="toc-multi-class-classification-section-2" class="nav-link" data-scroll-target="#multi-class-classification-section-2">Multi-class Classification Section</a>
  <ul class="collapse">
  <li><a href="#gradient-boosting-classifier-5" id="toc-gradient-boosting-classifier-5" class="nav-link" data-scroll-target="#gradient-boosting-classifier-5">Gradient Boosting Classifier</a></li>
  <li><a href="#decision-tree-classifier-2" id="toc-decision-tree-classifier-2" class="nav-link" data-scroll-target="#decision-tree-classifier-2">Decision Tree Classifier</a></li>
  <li><a href="#k-nearest-neighbors-knn-2" id="toc-k-nearest-neighbors-knn-2" class="nav-link" data-scroll-target="#k-nearest-neighbors-knn-2">K-Nearest Neighbors (KNN)</a></li>
  <li><a href="#discussion-for-multi-class-classification-section" id="toc-discussion-for-multi-class-classification-section" class="nav-link" data-scroll-target="#discussion-for-multi-class-classification-section">Discussion for Multi-class Classification Section</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#key-features-in-youtube-video-popularity" id="toc-key-features-in-youtube-video-popularity" class="nav-link" data-scroll-target="#key-features-in-youtube-video-popularity">Key Features in YouTube Video Popularity</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Supervised Learning</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- After digesting the instructions, you can delete this cell, these are assignment instructions and do not need to be included in your final submission.  -->
<!-- {{< include instructions.qmd >}}  -->
<section id="introduction-and-motivation" class="level1">
<h1>Introduction and Motivation:</h1>
<p>In this analysis, we aim to predict video view counts and classify video popularity using supervised learning models. Our predictions and classifications are based on five key features highly correlated with viewership: “like count,” “duration,” “definition,” “topic categories,” and “popularity.”</p>
<p>The report is divided into four sections: regression, binary classification, multi-class classification and the key importance features</p>
<p>In the regression section, we employ supervised learning models to predict view counts. Model performance is evaluated using metrics such as RMSE, MAE, and R². Additionally, we visualize the predicted view counts against the actual view counts to assess the models’ predictive accuracy and reliability.</p>
<p>In the binary classification section, we utilize supervised learning models to classify videos as “high” or “low” popularity based on their view counts. Videos with over 6 million views are categorized as “high popularity,” while those below 6 million are considered “low popularity.” Metrics such as accuracy, precision, recall, F1 score, and ROC AUC are used to evaluate the models’ performance.</p>
<p>In the multi-class classification section, supervised learning models are applied to classify videos into “high,” “medium,” or “low” popularity categories. Videos with over 6 million views are classified as “high popularity,” those with view counts between 1 million and 6 million as “medium popularity,” and those with fewer than 1 million views as “low popularity.” The models are assessed using metrics such as macro and weighted precision, recall, F1 score, log loss, and Cohen Kappa score to evaluate their ability to effectively handle multi-class classification tasks.</p>
<p>In the final section, we identify the most important features influencing video popularity. Using the best-performing model, we analyze and visualize feature importance, providing insights into the factors that most significantly impact viewership and popularity.</p>
</section>
<section id="overview-of-methods-gpt4o" class="level1">
<h1>Overview of Methods<span class="citation" data-cites="gpt4o"><sup><a href="#ref-gpt4o" role="doc-biblioref">1</a></sup></span></h1>
<hr>
<section id="regression-section" class="level2">
<h2 class="anchored" data-anchor-id="regression-section">Regression Section</h2>
<section id="random-forest-regressor" class="level3">
<h3 class="anchored" data-anchor-id="random-forest-regressor">Random Forest Regressor</h3>
<ol type="1">
<li><p><strong>What it is</strong>:<br>
Random Forest is an ensemble learning method that combines multiple decision trees to improve predictive accuracy and control overfitting.</p></li>
<li><p><strong>How it works</strong>:<br>
Each tree is trained on a random subset of data and features. The final prediction is the average of predictions from all trees.</p></li>
<li><p><strong>Inputs</strong>:<br>
Features include both numerical and categorical variables:</p>
<ul>
<li><code>likeCount</code>: Numeric (e.g., 12,000).<br>
</li>
<li><code>duration</code>: Numeric (e.g., 300 seconds).<br>
</li>
<li><code>definition</code>: Categorical, binary encoded (<code>1</code> for <code>hd</code>, <code>0</code> for <code>sd</code>).<br>
</li>
<li><code>topicCategories</code>: Categorical, one-hot encoded or frequency counts.<br>
</li>
<li><code>popularity</code>: Categorical, binary encoded (<code>1</code> for <code>high</code>, <code>0</code> for <code>low</code>).</li>
</ul></li>
<li><p><strong>Outputs</strong>:<br>
Continuous numerical predictions representing estimated view counts.<br>
<strong>Example</strong>: For the above inputs, the model might predict 1,200,000 views.</p></li>
<li><p><strong>Key Hyperparameters</strong>:</p>
<ul>
<li><code>n_estimators</code>: Number of trees.<br>
</li>
<li><code>max_depth</code>: Maximum depth of trees.<br>
</li>
<li><code>min_samples_split</code>: Minimum samples required to split a node.</li>
</ul></li>
</ol>
<hr>
</section>
<section id="linear-regressor" class="level3">
<h3 class="anchored" data-anchor-id="linear-regressor">Linear Regressor</h3>
<ol type="1">
<li><p><strong>What it is</strong>:<br>
A simple, interpretable model predicting continuous outcomes, such as video view counts.</p></li>
<li><p><strong>How it works</strong>:<br>
Assigns weights to each feature to predict the target variable by minimizing the difference between actual and predicted values.</p></li>
<li><p><strong>Inputs</strong>:</p>
<ul>
<li><code>likeCount</code>: Numeric (e.g., 12,000).<br>
</li>
<li><code>duration</code>: Numeric (e.g., 300 seconds).<br>
</li>
<li><code>definition</code>: Categorical, binary encoded (<code>1</code> for <code>hd</code>, <code>0</code> for <code>sd</code>).<br>
</li>
<li><code>topicCategories</code>: Categorical, one-hot encoded or frequency counts.<br>
</li>
<li><code>popularity</code>: Categorical, binary encoded (<code>1</code> for <code>high</code>, <code>0</code> for <code>low</code>).</li>
</ul></li>
<li><p><strong>Outputs</strong>:<br>
Continuous numerical predictions of view counts.<br>
<strong>Example</strong>: For the above inputs, the model might predict 1,200,000 views.</p></li>
<li><p><strong>Key Hyperparameters</strong>:</p>
<ul>
<li><code>alpha</code>: Regularization strength.<br>
</li>
<li><code>solver</code>: Optimization method.<br>
</li>
<li><code>max_iter</code>: Maximum number of iterations.</li>
</ul></li>
</ol>
<hr>
</section>
<section id="gradient-boosting-regressor" class="level3">
<h3 class="anchored" data-anchor-id="gradient-boosting-regressor">Gradient Boosting Regressor</h3>
<ol type="1">
<li><p><strong>What it is</strong>:<br>
Gradient Boosting is an ensemble method that builds a series of models sequentially, correcting errors from previous models.</p></li>
<li><p><strong>How it works</strong>:<br>
Sequentially adds decision trees trained on residual errors. The final prediction is the weighted sum of tree predictions.</p></li>
<li><p><strong>Inputs</strong>:</p>
<ul>
<li><code>likeCount</code>: Numeric (e.g., 12,000).<br>
</li>
<li><code>duration</code>: Numeric (e.g., 300 seconds).<br>
</li>
<li><code>definition</code>: Categorical, binary encoded (<code>1</code> for <code>hd</code>, <code>0</code> for <code>sd</code>).<br>
</li>
<li><code>topicCategories</code>: Categorical, one-hot encoded or frequency counts.<br>
</li>
<li><code>popularity</code>: Categorical, binary encoded (<code>1</code> for <code>high</code>, <code>0</code> for <code>low</code>).</li>
</ul></li>
<li><p><strong>Outputs</strong>:<br>
Predicted view counts.<br>
<strong>Example</strong>: For the above inputs, the model might predict 1,200,000 views.</p></li>
<li><p><strong>Key Hyperparameters</strong>:</p>
<ul>
<li><code>n_estimators</code>: Number of trees.<br>
</li>
<li><code>max_depth</code>: Tree depth.<br>
</li>
<li><code>learning_rate</code>: Contribution of each tree.<br>
</li>
<li><code>subsample</code>: Fraction of data used for each tree.</li>
</ul></li>
</ol>
<hr>
</section>
</section>
<section id="binary-classification-section" class="level2">
<h2 class="anchored" data-anchor-id="binary-classification-section">Binary Classification Section</h2>
<section id="random-forest-classifier" class="level3">
<h3 class="anchored" data-anchor-id="random-forest-classifier">Random Forest Classifier</h3>
<ol type="1">
<li><p><strong>What it is</strong>:<br>
An ensemble learning method for classification tasks that aggregates predictions from multiple decision trees.</p></li>
<li><p><strong>How it works</strong>:<br>
Each tree votes for a class, and the majority vote determines the final output.</p></li>
<li><p><strong>Inputs</strong>:</p>
<ul>
<li><code>likeCount</code>: Numeric (e.g., 12,000).<br>
</li>
<li><code>duration</code>: Numeric (e.g., 300 seconds).<br>
</li>
<li><code>definition</code>: Binary encoded (<code>1</code> for <code>hd</code>, <code>0</code> for <code>sd</code>).<br>
</li>
<li><code>topicCategories</code>: One-hot encoded or frequency counts.</li>
</ul></li>
<li><p><strong>Outputs</strong>:<br>
Binary class labels (<code>0</code> for low, <code>1</code> for high popularity).</p></li>
<li><p><strong>Key Hyperparameters</strong>:</p>
<ul>
<li><code>n_estimators</code>: Number of trees.<br>
</li>
<li><code>max_depth</code>: Tree depth.<br>
</li>
<li><code>min_samples_split</code>: Minimum samples required to split a node.</li>
</ul></li>
</ol>
<hr>
</section>
<section id="logistic-regression" class="level3">
<h3 class="anchored" data-anchor-id="logistic-regression">Logistic Regression</h3>
<ol type="1">
<li><p><strong>What it is</strong>:<br>
A statistical method for binary classification tasks.</p></li>
<li><p><strong>How it works</strong>:<br>
Estimates probabilities using a logistic function and classifies based on a threshold (default: 0.5).</p></li>
<li><p><strong>Inputs</strong>:</p>
<ul>
<li><code>likeCount</code>: Numeric (e.g., 12,000).<br>
</li>
<li><code>duration</code>: Numeric (e.g., 300 seconds).<br>
</li>
<li><code>definition</code>: Binary encoded (<code>1</code> for <code>hd</code>, <code>0</code> for <code>sd</code>).<br>
</li>
<li><code>topicCategories</code>: One-hot encoded.</li>
</ul></li>
<li><p><strong>Outputs</strong>:</p>
<ul>
<li><strong>Class Labels</strong>: <code>0</code> or <code>1</code>.<br>
</li>
<li><strong>Class Probabilities</strong>: Probabilities for each class.<br>
<strong>Example</strong>: For the above inputs, the model might predict <code>1</code> with a probability of 0.85.</li>
</ul></li>
<li><p><strong>Key Hyperparameters</strong>:</p>
<ul>
<li><code>penalty</code>: Regularization type (<code>l1</code>, <code>l2</code>, etc.).<br>
</li>
<li><code>C</code>: Inverse of regularization strength.<br>
</li>
<li><code>max_iter</code>: Maximum iterations.</li>
</ul></li>
</ol>
<hr>
</section>
<section id="gradient-boosting-classifier" class="level3">
<h3 class="anchored" data-anchor-id="gradient-boosting-classifier">Gradient Boosting Classifier</h3>
<ol type="1">
<li><p><strong>What it is</strong>:<br>
An ensemble method combining decision trees to classify multi-class problems effectively.</p></li>
<li><p><strong>How it works</strong>:<br>
Sequentially trains trees to minimize classification error, with predictions as weighted averages.</p></li>
<li><p><strong>Inputs</strong>:</p>
<ul>
<li><code>likeCount</code>: Numeric.<br>
</li>
<li><code>duration</code>: Numeric.<br>
</li>
<li><code>definition</code>: Binary encoded.<br>
</li>
<li><code>topicCategories</code>: One-hot encoded.</li>
</ul></li>
<li><p><strong>Outputs</strong>:</p>
<ul>
<li><strong>Class Labels</strong>: Predicted categories.<br>
</li>
<li><strong>Class Probabilities</strong>: Probabilities for each class.</li>
</ul></li>
<li><p><strong>Key Hyperparameters</strong>:</p>
<ul>
<li><code>n_estimators</code>: Number of trees.<br>
</li>
<li><code>learning_rate</code>: Step size for boosting iterations.<br>
</li>
<li><code>subsample</code>: Fraction of data used per tree.</li>
</ul></li>
</ol>
<hr>
</section>
</section>
<section id="multi-class-classification-section" class="level2">
<h2 class="anchored" data-anchor-id="multi-class-classification-section">Multi-Class Classification Section</h2>
<section id="gradient-boosting-classifier-1" class="level3">
<h3 class="anchored" data-anchor-id="gradient-boosting-classifier-1">Gradient Boosting Classifier</h3>
<ol type="1">
<li><p><strong>What it is</strong>:<br>
An ensemble method combining decision trees to classify multi-class problems effectively.</p></li>
<li><p><strong>How it works</strong>:<br>
Sequentially trains trees to minimize classification error, with predictions as weighted averages.</p></li>
<li><p><strong>Inputs</strong>:</p>
<ul>
<li><code>likeCount</code>: Numeric.<br>
</li>
<li><code>duration</code>: Numeric.<br>
</li>
<li><code>definition</code>: Binary encoded.<br>
</li>
<li><code>topicCategories</code>: One-hot encoded.</li>
</ul></li>
<li><p><strong>Outputs</strong>:</p>
<ul>
<li><strong>Class Labels</strong>: Predicted categories(multi-class popularity label).<br>
</li>
<li><strong>Class Probabilities</strong>: Probabilities for each class.</li>
</ul></li>
<li><p><strong>Key Hyperparameters</strong>:</p>
<ul>
<li><code>n_estimators</code>: Number of trees.<br>
</li>
<li><code>learning_rate</code>: Step size for boosting iterations.<br>
</li>
<li><code>subsample</code>: Fraction of data used per tree.</li>
</ul></li>
</ol>
<hr>
</section>
<section id="k-nearest-neighbors-knn" class="level3">
<h3 class="anchored" data-anchor-id="k-nearest-neighbors-knn">K-Nearest Neighbors (KNN)</h3>
<ol type="1">
<li><p><strong>What it is</strong>:<br>
A non-parametric method that classifies data points based on the majority vote of their <code>k</code> nearest neighbors.</p></li>
<li><p><strong>How it works</strong>:<br>
Calculates distances to all training points, identifies the nearest <code>k</code>, and predicts the majority class.</p></li>
<li><p><strong>Inputs</strong>:</p>
<ul>
<li><code>likeCount</code>, <code>duration</code>, <code>definition</code>, and <code>topicCategories</code>, scaled for distance calculations.</li>
</ul></li>
<li><p><strong>Outputs</strong>:</p>
<ul>
<li><strong>Class Labels</strong>: Predicted categories.<br>
</li>
<li><strong>Class Probabilities</strong>: Proportions of nearest neighbors for each class.</li>
</ul></li>
<li><p><strong>Key Hyperparameters</strong>:</p>
<ul>
<li><code>n_neighbors</code>: Number of neighbors.<br>
</li>
<li><code>weights</code>: Voting method.<br>
</li>
<li><code>metric</code>: Distance metric (<code>euclidean</code>, <code>manhattan</code>, etc.).</li>
</ul></li>
</ol>
<hr>
</section>
<section id="decision-tree-classifier" class="level3">
<h3 class="anchored" data-anchor-id="decision-tree-classifier">Decision Tree Classifier</h3>
<ol type="1">
<li><p><strong>What it is</strong>: A supervised machine learning algorithm used for classification and regression tasks. It splits the dataset into smaller subsets based on feature values, forming a tree-like structure where each node represents a decision rule and each leaf node represents an outcome.</p></li>
<li><p><strong>How it works</strong>: The dataset is split into branches based on feature values, applying criteria like Gini Impurity, Entropy, or Log Loss, until a stopping criterion is met, and predictions are made by traversing the tree to a leaf node that provides the final class or value.</p></li>
<li><p><strong>Inputs</strong>:</p>
<ul>
<li><code>likeCount</code>, <code>duration</code>, <code>definition</code>, and <code>topicCategories</code>, scaled for distance calculations.</li>
</ul></li>
<li><p><strong>Outputs</strong>:</p>
<ul>
<li><strong>Class Labels:</strong> Predicted categories for <code>popularity_multi_class</code>.<br>
</li>
<li><strong>Class Probabilities:</strong> The probabilities for each class, calculated from the proportion of samples in a leaf node.</li>
</ul></li>
<li><p><strong>Key Hyperparameters</strong>:</p>
<ul>
<li><strong>Criterion:</strong> The function to measure the quality of a split (e.g., Gini Impurity, Entropy, or Log Loss).<br>
</li>
<li><strong>Max Depth:</strong> Maximum depth of the tree to prevent overfitting.<br>
</li>
<li><strong>Min Samples Split:</strong> Minimum number of samples required to split an internal node.<br>
</li>
<li><strong>Min Samples Leaf:</strong> Minimum number of samples required to be at a leaf node.<br>
</li>
<li><strong>Max Features:</strong> The number of features to consider when looking for the best split.</li>
</ul></li>
</ol>
</section>
</section>
</section>
<section id="code" class="level1">
<h1>Code</h1>
<p>Provide the source code used for this section of the project here.</p>
<p>If you’re using a package for code organization, you can import it at this point. However, make sure that the <strong>actual workflow steps</strong>—including data processing, analysis, and other key tasks—are conducted and clearly demonstrated on this page. The goal is to show the technical flow of your project, highlighting how the code is executed to achieve your results.</p>
<p>If relevant, link to additional documentation or external references that explain any complex components. This section should give readers a clear view of how the project is implemented from a technical perspective.</p>
<p>Remember, this page is a technical narrative, NOT just a notebook with a collection of code cells, include in-line Prose, to describe what is going on.</p>
<div id="cell-5" class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(<span class="st">"../../data/processed-data/cleaned_data.csv"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="regression-section-1" class="level1">
<h1>Regression section</h1>
<section id="random-forest-regressor-1" class="level2">
<h2 class="anchored" data-anchor-id="random-forest-regressor-1">Random Forest Regressor</h2>
<div id="cell-8" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, RandomizedSearchCV</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, r2_score, mean_absolute_error</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> randint</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Load and preprocess data</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">"likeCount"</span>, <span class="st">"duration"</span>, <span class="st">"definition"</span>, <span class="st">"topicCategories"</span>,<span class="st">"popularity"</span>]</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[features]</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">"viewCount"</span>]</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove outliers using percentile</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>lower_bound <span class="op">=</span> y.quantile(<span class="fl">0.01</span>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>upper_bound <span class="op">=</span> y.quantile(<span class="fl">0.99</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data[(y <span class="op">&gt;=</span> lower_bound) <span class="op">&amp;</span> (y <span class="op">&lt;=</span> upper_bound)]</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[features]</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">"viewCount"</span>]</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature engineering</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>X.loc[:, <span class="st">"topicCategories"</span>] <span class="op">=</span> X[<span class="st">"topicCategories"</span>].<span class="bu">map</span>(X[<span class="st">"topicCategories"</span>].value_counts())</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>X.loc[:, <span class="st">"definition"</span>] <span class="op">=</span> (X[<span class="st">"definition"</span>] <span class="op">==</span> <span class="st">"hd"</span>).astype(<span class="bu">int</span>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>X.loc[:, <span class="st">"popularity"</span>] <span class="op">=</span> (X[<span class="st">"popularity"</span>] <span class="op">==</span> <span class="st">"high"</span>).astype(<span class="bu">int</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Train-test split</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Random Forest hyperparameter search</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>param_dist <span class="op">=</span> {</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_estimators'</span>: randint(<span class="dv">100</span>, <span class="dv">500</span>),  <span class="co"># Increase range for the number of trees</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: [<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>, <span class="va">None</span>],   <span class="co"># Add 'None' for unlimited depth</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_split'</span>: randint(<span class="dv">2</span>, <span class="dv">20</span>),</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_leaf'</span>: randint(<span class="dv">1</span>, <span class="dv">10</span>),</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_features'</span>: [<span class="st">'sqrt'</span>, <span class="st">'log2'</span>, <span class="va">None</span>],  <span class="co"># Add 'None' to consider all features</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>    <span class="st">'bootstrap'</span>: [<span class="va">True</span>, <span class="va">False</span>],  <span class="co"># Use bootstrap sampling</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> RandomForestRegressor(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>random_search <span class="op">=</span> RandomizedSearchCV(</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>clf,</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>    param_distributions<span class="op">=</span>param_dist,</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>    n_iter<span class="op">=</span><span class="dv">100</span>,  <span class="co"># Increase iterations for better parameter search</span></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">'neg_mean_squared_error'</span>,</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span>  <span class="co"># Show progress</span></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the RandomizedSearchCV</span></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>random_search.fit(X_train, y_train)</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Best parameters</span></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best Parameters:"</span>, random_search.best_params_)</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the best model</span></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>best_clf <span class="op">=</span> random_search.best_estimator_</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>best_clf.fit(X_train, y_train)</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict</span></span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> best_clf.predict(X_test)</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate regression performance</span></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>rmse <span class="op">=</span> np.sqrt(mean_squared_error(y_test, y_pred))</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>mae <span class="op">=</span> mean_absolute_error(y_test, y_pred)</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>r2 <span class="op">=</span> r2_score(y_test, y_pred)</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Regression Metrics:"</span>)</span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"RMSE: </span><span class="sc">{</span>rmse<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MAE: </span><span class="sc">{</span>mae<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"R-squared: </span><span class="sc">{</span>r2<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualization: Actual vs Predicted</span></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>plt.scatter(y_test, y_pred, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>plt.plot([y_test.<span class="bu">min</span>(), y_test.<span class="bu">max</span>()], [y_test.<span class="bu">min</span>(), y_test.<span class="bu">max</span>()], </span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>         color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">"Perfect Fit"</span>)</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Actual View Count"</span>)</span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Predicted View Count"</span>)</span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Actual vs Predicted View Counts"</span>)</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best Parameters: {'bootstrap': True, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 5, 'n_estimators': 445}
Regression Metrics:
RMSE: 0.24950307870026947
MAE: 0.10995794979659819
R-squared: 0.7712702907389977</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-3-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="linear-regressor-1" class="level2">
<h2 class="anchored" data-anchor-id="linear-regressor-1">Linear Regressor</h2>
<div id="cell-10" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, GridSearchCV</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, r2_score, mean_absolute_error</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Load and preprocess data</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">"likeCount"</span>, <span class="st">"duration"</span>, <span class="st">"definition"</span>, <span class="st">"topicCategories"</span>,<span class="st">"popularity"</span>]</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[features]</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">"viewCount"</span>]</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove outliers using percentile</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>lower_bound <span class="op">=</span> y.quantile(<span class="fl">0.01</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>upper_bound <span class="op">=</span> y.quantile(<span class="fl">0.99</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data[(y <span class="op">&gt;=</span> lower_bound) <span class="op">&amp;</span> (y <span class="op">&lt;=</span> upper_bound)]</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[features]</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">"viewCount"</span>]</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Handle missing values</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X.fillna(<span class="dv">0</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature engineering</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>X.loc[:, <span class="st">"topicCategories"</span>] <span class="op">=</span> X[<span class="st">"topicCategories"</span>].<span class="bu">map</span>(X[<span class="st">"topicCategories"</span>].value_counts())</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>X.loc[:, <span class="st">"definition"</span>] <span class="op">=</span> (X[<span class="st">"definition"</span>] <span class="op">==</span> <span class="st">"hd"</span>).astype(<span class="bu">int</span>)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>X.loc[:, <span class="st">"popularity"</span>] <span class="op">=</span> (X[<span class="st">"popularity"</span>] <span class="op">==</span> <span class="st">"high"</span>).astype(<span class="bu">int</span>)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardize features</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> scaler.fit_transform(X)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Train-test split</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameter tuning for Ridge Regression</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    <span class="st">'alpha'</span>: [<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">1000</span>],  <span class="co"># Regularization strength</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">'solver'</span>: [<span class="st">'auto'</span>, <span class="st">'svd'</span>, <span class="st">'cholesky'</span>, <span class="st">'lsqr'</span>]  <span class="co"># Avoid sag and saga for stability</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>ridge <span class="op">=</span> Ridge(max_iter<span class="op">=</span><span class="dv">5000</span>)  <span class="co"># Increase max_iter for convergence</span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(estimator<span class="op">=</span>ridge, param_grid<span class="op">=</span>param_grid, scoring<span class="op">=</span><span class="st">'neg_mean_squared_error'</span>, cv<span class="op">=</span><span class="dv">5</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>grid_search.fit(X_train, y_train)</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Best parameters</span></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best Parameters:"</span>, grid_search.best_params_)</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the best model</span></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>best_regressor <span class="op">=</span> grid_search.best_estimator_</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>best_regressor.fit(X_train, y_train)</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict</span></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> best_regressor.predict(X_test)</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate regression performance</span></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>rmse <span class="op">=</span> np.sqrt(mean_squared_error(y_test, y_pred))</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>mae <span class="op">=</span> mean_absolute_error(y_test, y_pred)</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>r2 <span class="op">=</span> r2_score(y_test, y_pred)</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Regression Metrics:"</span>)</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"RMSE: </span><span class="sc">{</span>rmse<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MAE: </span><span class="sc">{</span>mae<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"R-squared: </span><span class="sc">{</span>r2<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualization: Actual vs Predicted</span></span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>plt.scatter(y_test, y_pred, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>plt.plot([y_test.<span class="bu">min</span>(), y_test.<span class="bu">max</span>()], [y_test.<span class="bu">min</span>(), y_test.<span class="bu">max</span>()], </span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>         color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">"Perfect Fit"</span>)</span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Actual View Count"</span>)</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Predicted View Count"</span>)</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Actual vs Predicted View Counts"</span>)</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best Parameters: {'alpha': 10, 'solver': 'auto'}
Regression Metrics:
RMSE: 0.2056597055110518
MAE: 0.12407352920784746
R-squared: 0.8032625833891794</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-4-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="gradient-boosting-regressor-1" class="level2">
<h2 class="anchored" data-anchor-id="gradient-boosting-regressor-1">Gradient Boosting Regressor</h2>
<div id="cell-12" class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingRegressor</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, GridSearchCV</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, r2_score, mean_absolute_error</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Load and preprocess data</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">"likeCount"</span>, <span class="st">"duration"</span>, <span class="st">"definition"</span>, <span class="st">"topicCategories"</span>,<span class="st">"popularity"</span>]</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[features]</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">"viewCount"</span>]</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove outliers using percentile</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>lower_bound <span class="op">=</span> y.quantile(<span class="fl">0.01</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>upper_bound <span class="op">=</span> y.quantile(<span class="fl">0.99</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data[(y <span class="op">&gt;=</span> lower_bound) <span class="op">&amp;</span> (y <span class="op">&lt;=</span> upper_bound)]</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[features]</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">"viewCount"</span>]</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Handle missing values and ensure proper data types</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X.fillna(<span class="dv">0</span>)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature engineering</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>X.loc[:, <span class="st">"topicCategories"</span>] <span class="op">=</span> X[<span class="st">"topicCategories"</span>].<span class="bu">map</span>(X[<span class="st">"topicCategories"</span>].value_counts()).fillna(<span class="dv">0</span>).astype(<span class="bu">int</span>)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>X.loc[:, <span class="st">"definition"</span>] <span class="op">=</span> (X[<span class="st">"definition"</span>] <span class="op">==</span> <span class="st">"hd"</span>).astype(<span class="bu">int</span>)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>X[<span class="st">"popularity"</span>] <span class="op">=</span> X[<span class="st">"popularity"</span>].fillna(<span class="st">"low"</span>)  <span class="co"># Fill missing values</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>X[<span class="st">"popularity"</span>] <span class="op">=</span> (X[<span class="st">"popularity"</span>] <span class="op">==</span> <span class="st">"high"</span>).astype(<span class="bu">int</span>)  <span class="co"># Convert to binary</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Train-test split</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient Boosting hyperparameter search</span></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_estimators'</span>: [<span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">300</span>],  <span class="co"># Number of boosting stages</span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>    <span class="st">'learning_rate'</span>: [<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>],  <span class="co"># Learning rate</span></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: [<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>],  <span class="co"># Maximum depth of the trees</span></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_split'</span>: [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>],  <span class="co"># Minimum samples required to split an internal node</span></span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_leaf'</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>],  <span class="co"># Minimum samples required at a leaf node</span></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>    <span class="st">'subsample'</span>: [<span class="fl">0.8</span>, <span class="fl">1.0</span>],  <span class="co"># Fraction of samples used for fitting the individual base learners</span></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_features'</span>: [<span class="st">'sqrt'</span>, <span class="st">'log2'</span>, <span class="va">None</span>]  <span class="co"># Number of features to consider for best split</span></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>gbr <span class="op">=</span> GradientBoostingRegressor(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>gbr,</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>    param_grid<span class="op">=</span>param_grid,</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">'neg_mean_squared_error'</span>,</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span></span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the GridSearchCV</span></span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>grid_search.fit(X_train, y_train)</span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Best parameters</span></span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best Parameters:"</span>, grid_search.best_params_)</span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the best model</span></span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a>best_gbr <span class="op">=</span> grid_search.best_estimator_</span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a>best_gbr.fit(X_train, y_train)</span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict</span></span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> best_gbr.predict(X_test)</span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate regression performance</span></span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a>rmse <span class="op">=</span> np.sqrt(mean_squared_error(y_test, y_pred))</span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a>mae <span class="op">=</span> mean_absolute_error(y_test, y_pred)</span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a>r2 <span class="op">=</span> r2_score(y_test, y_pred)</span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Regression Metrics:"</span>)</span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"RMSE: </span><span class="sc">{</span>rmse<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-74"><a href="#cb6-74" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MAE: </span><span class="sc">{</span>mae<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-75"><a href="#cb6-75" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"R-squared: </span><span class="sc">{</span>r2<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-76"><a href="#cb6-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-77"><a href="#cb6-77" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualization: Actual vs Predicted</span></span>
<span id="cb6-78"><a href="#cb6-78" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb6-79"><a href="#cb6-79" aria-hidden="true" tabindex="-1"></a>plt.scatter(y_test, y_pred, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb6-80"><a href="#cb6-80" aria-hidden="true" tabindex="-1"></a>plt.plot([y_test.<span class="bu">min</span>(), y_test.<span class="bu">max</span>()], [y_test.<span class="bu">min</span>(), y_test.<span class="bu">max</span>()], </span>
<span id="cb6-81"><a href="#cb6-81" aria-hidden="true" tabindex="-1"></a>         color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">"Perfect Fit"</span>)</span>
<span id="cb6-82"><a href="#cb6-82" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Actual View Count"</span>)</span>
<span id="cb6-83"><a href="#cb6-83" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Predicted View Count"</span>)</span>
<span id="cb6-84"><a href="#cb6-84" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Actual vs Predicted View Counts"</span>)</span>
<span id="cb6-85"><a href="#cb6-85" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb6-86"><a href="#cb6-86" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb6-87"><a href="#cb6-87" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 300, 'subsample': 0.8}
Regression Metrics:
RMSE: 0.12945966218516167
MAE: 0.07980190234589664
R-squared: 0.8727429441887932</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-5-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="binary-classification" class="level1">
<h1>Binary classification</h1>
<section id="random-forest-classifier-1" class="level2">
<h2 class="anchored" data-anchor-id="random-forest-classifier-1">Random Forest classifier</h2>
<div id="cell-15" class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> StratifiedKFold, GridSearchCV</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler, LabelEncoder</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Features and target variable</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">"likeCount"</span>, <span class="st">"duration"</span>, <span class="st">"definition"</span>, <span class="st">"topicCategories"</span>]</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[features]  <span class="co"># Feature matrix</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X.fillna(<span class="dv">0</span>)  <span class="co"># Replace NaN values with 0</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">"popularity"</span>]  <span class="co"># Target variable (labels)</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>label_encoder <span class="op">=</span> LabelEncoder()</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> label_encoder.fit_transform(y)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure categorical features are encoded properly</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> pd.get_dummies(X, columns<span class="op">=</span>[<span class="st">"definition"</span>, <span class="st">"topicCategories"</span>], drop_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature scaling (not strictly necessary for Random Forest)</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> scaler.fit_transform(X)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameter search space for Random Forest</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_estimators'</span>: [<span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">300</span>],        <span class="co"># Number of trees</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: [<span class="va">None</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>],       <span class="co"># Maximum depth of trees</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_split'</span>: [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>],       <span class="co"># Minimum samples to split a node</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_leaf'</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>],         <span class="co"># Minimum samples at a leaf node</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_features'</span>: [<span class="st">'sqrt'</span>, <span class="st">'log2'</span>, <span class="va">None</span>],<span class="co"># Features to consider for the best split</span></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>    <span class="st">'class_weight'</span>: [<span class="va">None</span>, <span class="st">'balanced'</span>],    <span class="co"># Class weights for imbalance</span></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>    <span class="st">'bootstrap'</span>: [<span class="va">True</span>, <span class="va">False</span>]             <span class="co"># Whether to use bootstrapping</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validation setup</span></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> StratifiedKFold(n_splits<span class="op">=</span><span class="dv">3</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the Random Forest Classifier</span></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> RandomForestClassifier(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameter tuning with GridSearchCV</span></span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>clf,</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>    param_grid<span class="op">=</span>param_grid,</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">'accuracy'</span>,</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>cv,</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">1</span></span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform the grid search</span></span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>grid_search.fit(X_scaled, y)</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Best hyperparameters</span></span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>best_params <span class="op">=</span> grid_search.best_params_</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best Parameters:"</span>, best_params)</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the best model on the entire dataset</span></span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>best_rf <span class="op">=</span> grid_search.best_estimator_</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize metric storage for cross-validation evaluation</span></span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>accuracies, precisions, recalls, f1_scores, roc_aucs <span class="op">=</span> [], [], [], [], []</span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validation evaluation with best parameters</span></span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_idx, test_idx <span class="kw">in</span> cv.split(X_scaled, y):</span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a>    X_train, X_test <span class="op">=</span> X_scaled[train_idx], X_scaled[test_idx]</span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a>    y_train, y_test <span class="op">=</span> np.array(y)[train_idx], np.array(y)[test_idx]</span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a>    best_rf.fit(X_train, y_train)  <span class="co"># Train the best model</span></span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> best_rf.predict(X_test)  <span class="co"># Predict on test data</span></span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a>    y_proba <span class="op">=</span> best_rf.predict_proba(X_test)[:, <span class="dv">1</span>]  <span class="co"># Predicted probabilities for the positive class</span></span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluate metrics</span></span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true" tabindex="-1"></a>    accuracies.append(accuracy_score(y_test, y_pred))</span>
<span id="cb8-77"><a href="#cb8-77" aria-hidden="true" tabindex="-1"></a>    precisions.append(precision_score(y_test, y_pred, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb8-78"><a href="#cb8-78" aria-hidden="true" tabindex="-1"></a>    recalls.append(recall_score(y_test, y_pred, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true" tabindex="-1"></a>    f1_scores.append(f1_score(y_test, y_pred, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(np.unique(y_test)) <span class="op">&gt;</span> <span class="dv">1</span>:  <span class="co"># Check if both classes exist</span></span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true" tabindex="-1"></a>        roc_aucs.append(roc_auc_score(y_test, y_proba))</span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true" tabindex="-1"></a>        roc_aucs.append(<span class="bu">float</span>(<span class="st">"nan"</span>))</span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true" tabindex="-1"></a><span class="co"># Print evaluation metrics</span></span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Random Forest Classifier - Binary Classification Metrics:"</span>)</span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Accuracy:"</span>, np.mean(accuracies))</span>
<span id="cb8-88"><a href="#cb8-88" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Precision:"</span>, np.mean(precisions))</span>
<span id="cb8-89"><a href="#cb8-89" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Recall:"</span>, np.mean(recalls))</span>
<span id="cb8-90"><a href="#cb8-90" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"F1 Score:"</span>, np.mean(f1_scores))</span>
<span id="cb8-91"><a href="#cb8-91" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"ROC AUC:"</span>, np.nanmean(roc_aucs))</span>
<span id="cb8-92"><a href="#cb8-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-93"><a href="#cb8-93" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion matrix visualization</span></span>
<span id="cb8-94"><a href="#cb8-94" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb8-95"><a href="#cb8-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-96"><a href="#cb8-96" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the confusion matrix with more readable labels</span></span>
<span id="cb8-97"><a href="#cb8-97" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb8-98"><a href="#cb8-98" aria-hidden="true" tabindex="-1"></a>sns.heatmap(conf_matrix, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">"d"</span>, cmap<span class="op">=</span><span class="st">"Blues"</span>, xticklabels<span class="op">=</span>label_encoder.classes_, yticklabels<span class="op">=</span>label_encoder.classes_)</span>
<span id="cb8-99"><a href="#cb8-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-100"><a href="#cb8-100" aria-hidden="true" tabindex="-1"></a><span class="co"># Add titles and labels</span></span>
<span id="cb8-101"><a href="#cb8-101" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Confusion Matrix - Random Forest Classifier"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb8-102"><a href="#cb8-102" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Predicted"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb8-103"><a href="#cb8-103" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Actual"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb8-104"><a href="#cb8-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-105"><a href="#cb8-105" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the plot</span></span>
<span id="cb8-106"><a href="#cb8-106" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb8-107"><a href="#cb8-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-108"><a href="#cb8-108" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Fitting 3 folds for each of 1296 candidates, totalling 3888 fits
Best Parameters: {'bootstrap': True, 'class_weight': None, 'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 300}

Random Forest Classifier - Binary Classification Metrics:
Accuracy: 0.9496530060632625
Precision: 0.9626261725812437
Recall: 0.9800851783786938
F1 Score: 0.9712351897838586
ROC AUC: 0.9642452763319597</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-6-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="logistic-regression-1" class="level2">
<h2 class="anchored" data-anchor-id="logistic-regression-1">Logistic Regression</h2>
<div id="cell-17" class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> StratifiedKFold, GridSearchCV</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler, LabelEncoder</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[[<span class="st">"likeCount"</span>, <span class="st">"duration"</span>, <span class="st">"definition"</span>, <span class="st">"topicCategories"</span>]].copy()</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>X[<span class="st">"likeCount"</span>] <span class="op">=</span> X[<span class="st">"likeCount"</span>].fillna(X[<span class="st">"likeCount"</span>].median())</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>X[<span class="st">"duration"</span>] <span class="op">=</span> X[<span class="st">"duration"</span>].fillna(X[<span class="st">"duration"</span>].median())</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X.fillna(<span class="st">"missing"</span>)  </span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">"popularity"</span>]</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>label_encoder <span class="op">=</span> LabelEncoder()</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> label_encoder.fit_transform(y)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> pd.get_dummies(X, columns<span class="op">=</span>[<span class="st">"definition"</span>, <span class="st">"topicCategories"</span>], drop_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> scaler.fit_transform(X)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> [</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>    {<span class="st">'penalty'</span>: [<span class="st">'l2'</span>], <span class="st">'C'</span>: [<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>], <span class="st">'solver'</span>: [<span class="st">'lbfgs'</span>], <span class="st">'max_iter'</span>: [<span class="dv">5000</span>]},</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>    {<span class="st">'penalty'</span>: [<span class="st">'l1'</span>], <span class="st">'C'</span>: [<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>], <span class="st">'solver'</span>: [<span class="st">'liblinear'</span>], <span class="st">'max_iter'</span>: [<span class="dv">5000</span>]},</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>    {<span class="st">'penalty'</span>: [<span class="st">'elasticnet'</span>], <span class="st">'C'</span>: [<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>], <span class="st">'solver'</span>: [<span class="st">'saga'</span>], <span class="st">'l1_ratio'</span>: [<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.9</span>], <span class="st">'max_iter'</span>: [<span class="dv">5000</span>]}</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> StratifiedKFold(n_splits<span class="op">=</span><span class="dv">5</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> LogisticRegression(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>clf,</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>    param_grid<span class="op">=</span>param_grid,</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">'accuracy'</span>,</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>cv,</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>grid_search.fit(X_scaled, y)</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>best_params <span class="op">=</span> grid_search.best_params_</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best Parameters:"</span>, best_params)</span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a>best_lr <span class="op">=</span> grid_search.best_estimator_</span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>metrics <span class="op">=</span> {<span class="st">'accuracy'</span>: [], <span class="st">'precision'</span>: [], <span class="st">'recall'</span>: [], <span class="st">'f1_score'</span>: [], <span class="st">'roc_auc'</span>: []}</span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_idx, test_idx <span class="kw">in</span> cv.split(X_scaled, y):</span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>    X_train, X_test <span class="op">=</span> X_scaled[train_idx], X_scaled[test_idx]</span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a>    y_train, y_test <span class="op">=</span> y[train_idx], y[test_idx]</span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a>    best_lr.fit(X_train, y_train)</span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> best_lr.predict(X_test)</span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a>    y_proba <span class="op">=</span> best_lr.predict_proba(X_test)[:, <span class="dv">1</span>]</span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a>    metrics[<span class="st">'accuracy'</span>].append(accuracy_score(y_test, y_pred))</span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a>    metrics[<span class="st">'precision'</span>].append(precision_score(y_test, y_pred, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a>    metrics[<span class="st">'recall'</span>].append(recall_score(y_test, y_pred, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a>    metrics[<span class="st">'f1_score'</span>].append(f1_score(y_test, y_pred, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a>    metrics[<span class="st">'roc_auc'</span>].append(roc_auc_score(y_test, y_proba))</span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> metric, scores <span class="kw">in</span> metrics.items():</span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>metric<span class="sc">.</span>capitalize()<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>np<span class="sc">.</span>mean(scores)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a>sns.heatmap(conf_matrix, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">"d"</span>, cmap<span class="op">=</span><span class="st">"Blues"</span>, xticklabels<span class="op">=</span>label_encoder.classes_, yticklabels<span class="op">=</span>label_encoder.classes_)</span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Confusion Matrix - Logistic Regression"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Predicted"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Actual"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-91"><a href="#cb10-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-92"><a href="#cb10-92" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best Parameters: {'C': 0.1, 'max_iter': 5000, 'penalty': 'l1', 'solver': 'liblinear'}
Accuracy: 0.9422550629447182
Precision: 0.9531104160219265
Recall: 0.9817906630406631
F1_score: 0.9672090134595874
Roc_auc: 0.962362779409425</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-7-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="gradient-boosting-classifier-2" class="level2">
<h2 class="anchored" data-anchor-id="gradient-boosting-classifier-2">Gradient Boosting Classifier</h2>
<div id="cell-19" class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingClassifier</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> StratifiedKFold, GridSearchCV</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler, LabelEncoder</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Features and target variable</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">"likeCount"</span>, <span class="st">"duration"</span>, <span class="st">"definition"</span>, <span class="st">"topicCategories"</span>]</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[features]  <span class="co"># Feature matrix</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X.fillna(<span class="dv">0</span>)  <span class="co"># Replace NaN values with 0</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">"popularity"</span>]  <span class="co"># Target variable (binary labels)</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>label_encoder <span class="op">=</span> LabelEncoder()</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> label_encoder.fit_transform(y)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure categorical features are encoded properly</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> pd.get_dummies(X, columns<span class="op">=</span>[<span class="st">"definition"</span>, <span class="st">"topicCategories"</span>], drop_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature scaling</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> scaler.fit_transform(X)</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameter search space for Gradient Boosting Classifier</span></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_estimators'</span>: [<span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">300</span>],         <span class="co"># Number of boosting stages</span></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">'learning_rate'</span>: [<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>],      <span class="co"># Learning rate</span></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: [<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>],                 <span class="co"># Maximum depth of each tree</span></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_split'</span>: [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>],        <span class="co"># Minimum samples required to split an internal node</span></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_leaf'</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>],          <span class="co"># Minimum samples required at a leaf node</span></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>    <span class="st">'subsample'</span>: [<span class="fl">0.8</span>, <span class="fl">1.0</span>],                <span class="co"># Fraction of samples used for fitting individual trees</span></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_features'</span>: [<span class="st">'sqrt'</span>, <span class="st">'log2'</span>, <span class="va">None</span>]  <span class="co"># Number of features considered when looking for the best split</span></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validation setup</span></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> StratifiedKFold(n_splits<span class="op">=</span><span class="dv">5</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameter tuning with GridSearchCV</span></span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> GradientBoostingClassifier(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>clf,</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>    param_grid<span class="op">=</span>param_grid,</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">'accuracy'</span>,</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>cv,</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">1</span></span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform the grid search</span></span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>grid_search.fit(X_scaled, y)</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Best hyperparameters</span></span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a>best_params <span class="op">=</span> grid_search.best_params_</span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best Parameters:"</span>, best_params)</span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the best model on the entire dataset</span></span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>best_gbc <span class="op">=</span> grid_search.best_estimator_</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize metric storage for cross-validation evaluation</span></span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a>accuracies, precisions, recalls, f1_scores, roc_aucs <span class="op">=</span> [], [], [], [], []</span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validation evaluation with best parameters</span></span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_idx, test_idx <span class="kw">in</span> cv.split(X_scaled, y):</span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a>    X_train, X_test <span class="op">=</span> X_scaled[train_idx], X_scaled[test_idx]</span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a>    y_train, y_test <span class="op">=</span> y[train_idx], y[test_idx]</span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a>    best_gbc.fit(X_train, y_train)  <span class="co"># Train the best model</span></span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> best_gbc.predict(X_test)  <span class="co"># Predict on test data</span></span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a>    y_proba <span class="op">=</span> best_gbc.predict_proba(X_test)[:, <span class="dv">1</span>]  <span class="co"># Predicted probabilities</span></span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluate metrics</span></span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a>    accuracies.append(accuracy_score(y_test, y_pred))</span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a>    precisions.append(precision_score(y_test, y_pred, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a>    recalls.append(recall_score(y_test, y_pred, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a>    f1_scores.append(f1_score(y_test, y_pred, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a>    roc_aucs.append(roc_auc_score(y_test, y_proba))</span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a><span class="co"># Print evaluation metrics</span></span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Gradient Boosting Classifier - Binary Classification"</span>)</span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Accuracy:"</span>, np.mean(accuracies))</span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Precision:"</span>, np.mean(precisions))</span>
<span id="cb12-84"><a href="#cb12-84" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Recall:"</span>, np.mean(recalls))</span>
<span id="cb12-85"><a href="#cb12-85" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"F1 Score:"</span>, np.mean(f1_scores))</span>
<span id="cb12-86"><a href="#cb12-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"ROC AUC:"</span>, np.mean(roc_aucs))</span>
<span id="cb12-87"><a href="#cb12-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-88"><a href="#cb12-88" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion matrix visualization</span></span>
<span id="cb12-89"><a href="#cb12-89" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb12-90"><a href="#cb12-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-91"><a href="#cb12-91" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the confusion matrix with more readable labels</span></span>
<span id="cb12-92"><a href="#cb12-92" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb12-93"><a href="#cb12-93" aria-hidden="true" tabindex="-1"></a>sns.heatmap(conf_matrix, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">"d"</span>, cmap<span class="op">=</span><span class="st">"Blues"</span>, xticklabels<span class="op">=</span>label_encoder.classes_, yticklabels<span class="op">=</span>label_encoder.classes_)</span>
<span id="cb12-94"><a href="#cb12-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-95"><a href="#cb12-95" aria-hidden="true" tabindex="-1"></a><span class="co"># Add titles and labels</span></span>
<span id="cb12-96"><a href="#cb12-96" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Confusion Matrix - Gradient Boosting Classifier"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb12-97"><a href="#cb12-97" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Predicted"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb12-98"><a href="#cb12-98" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Actual"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb12-99"><a href="#cb12-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-100"><a href="#cb12-100" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the plot</span></span>
<span id="cb12-101"><a href="#cb12-101" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-102"><a href="#cb12-102" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Fitting 5 folds for each of 1458 candidates, totalling 7290 fits
Best Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300, 'subsample': 0.8}

Gradient Boosting Classifier - Binary Classification
Accuracy: 0.9516316973788237
Precision: 0.9642341340048605
Recall: 0.9806494431494432
F1 Score: 0.9723572674490519
ROC AUC: 0.9695507727270917</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-8-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="multi-class-classification-section-1" class="level1">
<h1>Multi-class classification section</h1>
<section id="gradient-boosting-classifier-3" class="level2">
<h2 class="anchored" data-anchor-id="gradient-boosting-classifier-3">Gradient Boosting Classifier</h2>
<div id="cell-22" class="cell" data-execution_count="76">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingClassifier</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> StratifiedKFold, GridSearchCV</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, log_loss, cohen_kappa_score</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler, LabelEncoder</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Features and target variable</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">"likeCount"</span>, <span class="st">"duration"</span>, <span class="st">"definition"</span>, <span class="st">"topicCategories"</span>]</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[features].fillna(<span class="dv">0</span>)  <span class="co"># Replace NaN values with 0</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">"popularity_multi_class"</span>]  <span class="co"># Target variable (multi-class labels)</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Label encode target variable</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>label_encoder <span class="op">=</span> LabelEncoder()</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> label_encoder.fit_transform(y)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure categorical features are encoded properly</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> pd.get_dummies(X, columns<span class="op">=</span>[<span class="st">"definition"</span>, <span class="st">"topicCategories"</span>], drop_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature scaling</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> scaler.fit_transform(X)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Define parameter grid for hyperparameter tuning</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">'learning_rate'</span>: [<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>],</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_estimators'</span>: [<span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">300</span>],</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: [<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>],</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_split'</span>: [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>],</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_leaf'</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>],</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>    <span class="st">'subsample'</span>: [<span class="fl">0.8</span>, <span class="fl">1.0</span>]</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validation setup</span></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> StratifiedKFold(n_splits<span class="op">=</span><span class="dv">5</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize Gradient Boosting Classifier</span></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> GradientBoostingClassifier(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameter tuning with GridSearchCV</span></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>clf,</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>    param_grid<span class="op">=</span>param_grid,</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">'accuracy'</span>,</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>cv,</span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">1</span></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform grid search</span></span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a>grid_search.fit(X_scaled, y)</span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Get best model and parameters</span></span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a>best_model <span class="op">=</span> grid_search.best_estimator_</span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a>best_params <span class="op">=</span> grid_search.best_params_</span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best Hyperparameters:"</span>, best_params)</span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validation evaluation with best model</span></span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a>accuracies, precisions_macro, recalls_macro, f1_scores_macro <span class="op">=</span> [], [], [], []</span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a>precisions_weighted, recalls_weighted, f1_scores_weighted <span class="op">=</span> [], [], []</span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a>log_losses, kappa_scores <span class="op">=</span> [], []</span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_idx, test_idx <span class="kw">in</span> cv.split(X_scaled, y):</span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a>    X_train, X_test <span class="op">=</span> X_scaled[train_idx], X_scaled[test_idx]</span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a>    y_train, y_test <span class="op">=</span> y[train_idx], y[test_idx]</span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train the best model</span></span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a>    best_model.fit(X_train, y_train)</span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Predictions</span></span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> best_model.predict(X_test)</span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a>    y_proba <span class="op">=</span> best_model.predict_proba(X_test)</span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluate metrics</span></span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true" tabindex="-1"></a>    accuracies.append(accuracy_score(y_test, y_pred))</span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a>    precisions_macro.append(precision_score(y_test, y_pred, average<span class="op">=</span><span class="st">'macro'</span>, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a>    recalls_macro.append(recall_score(y_test, y_pred, average<span class="op">=</span><span class="st">'macro'</span>, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a>    f1_scores_macro.append(f1_score(y_test, y_pred, average<span class="op">=</span><span class="st">'macro'</span>, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true" tabindex="-1"></a>    precisions_weighted.append(precision_score(y_test, y_pred, average<span class="op">=</span><span class="st">'weighted'</span>, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true" tabindex="-1"></a>    recalls_weighted.append(recall_score(y_test, y_pred, average<span class="op">=</span><span class="st">'weighted'</span>, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true" tabindex="-1"></a>    f1_scores_weighted.append(f1_score(y_test, y_pred, average<span class="op">=</span><span class="st">'weighted'</span>, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true" tabindex="-1"></a>    log_losses.append(log_loss(y_test, y_proba))</span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a>    kappa_scores.append(cohen_kappa_score(y_test, y_pred))</span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true" tabindex="-1"></a><span class="co"># Print evaluation metrics</span></span>
<span id="cb14-88"><a href="#cb14-88" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Gradient Boosting Classifier - Multi-Class Classification"</span>)</span>
<span id="cb14-89"><a href="#cb14-89" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"========================================================="</span>)</span>
<span id="cb14-90"><a href="#cb14-90" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>np<span class="sc">.</span>mean(accuracies)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb14-91"><a href="#cb14-91" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Macro Precision: </span><span class="sc">{</span>np<span class="sc">.</span>mean(precisions_macro)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb14-92"><a href="#cb14-92" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Macro Recall: </span><span class="sc">{</span>np<span class="sc">.</span>mean(recalls_macro)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb14-93"><a href="#cb14-93" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Macro F1 Score: </span><span class="sc">{</span>np<span class="sc">.</span>mean(f1_scores_macro)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb14-94"><a href="#cb14-94" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Weighted Precision: </span><span class="sc">{</span>np<span class="sc">.</span>mean(precisions_weighted)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb14-95"><a href="#cb14-95" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Weighted Recall: </span><span class="sc">{</span>np<span class="sc">.</span>mean(recalls_weighted)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb14-96"><a href="#cb14-96" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Weighted F1 Score: </span><span class="sc">{</span>np<span class="sc">.</span>mean(f1_scores_weighted)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb14-97"><a href="#cb14-97" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Log Loss: </span><span class="sc">{</span>np<span class="sc">.</span>mean(log_losses)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb14-98"><a href="#cb14-98" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cohen Kappa Score: </span><span class="sc">{</span>np<span class="sc">.</span>mean(kappa_scores)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb14-99"><a href="#cb14-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-100"><a href="#cb14-100" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion matrix visualization</span></span>
<span id="cb14-101"><a href="#cb14-101" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb14-102"><a href="#cb14-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-103"><a href="#cb14-103" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb14-104"><a href="#cb14-104" aria-hidden="true" tabindex="-1"></a>sns.heatmap(conf_matrix, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">"d"</span>, cmap<span class="op">=</span><span class="st">"Blues"</span>, xticklabels<span class="op">=</span>label_encoder.classes_, yticklabels<span class="op">=</span>label_encoder.classes_)</span>
<span id="cb14-105"><a href="#cb14-105" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Confusion Matrix - Multi-Class Gradient Boosting"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb14-106"><a href="#cb14-106" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Predicted"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb14-107"><a href="#cb14-107" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Actual"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb14-108"><a href="#cb14-108" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb14-109"><a href="#cb14-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-110"><a href="#cb14-110" aria-hidden="true" tabindex="-1"></a><span class="co"># Detailed classification report</span></span>
<span id="cb14-111"><a href="#cb14-111" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Classification Report:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb14-112"><a href="#cb14-112" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred, target_names<span class="op">=</span>label_encoder.classes_))</span>
<span id="cb14-113"><a href="#cb14-113" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Fitting 5 folds for each of 486 candidates, totalling 2430 fits
Best Hyperparameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}
Gradient Boosting Classifier - Multi-Class Classification
=========================================================
Accuracy: 0.8533
Macro Precision: 0.8516
Macro Recall: 0.8360
Macro F1 Score: 0.8427
Weighted Precision: 0.8526
Weighted Recall: 0.8533
Weighted F1 Score: 0.8521
Log Loss: 0.4071
Cohen Kappa Score: 0.7610</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-9-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Classification Report:

              precision    recall  f1-score   support

        high       0.87      0.82      0.84        71
         low       0.85      0.91      0.88       204
      medium       0.80      0.74      0.77       165

    accuracy                           0.83       440
   macro avg       0.84      0.82      0.83       440
weighted avg       0.83      0.83      0.83       440
</code></pre>
</div>
</div>
</section>
<section id="decision-tree-classifier-1" class="level2">
<h2 class="anchored" data-anchor-id="decision-tree-classifier-1">Decision Tree Classifier</h2>
<div id="cell-24" class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> StratifiedKFold, GridSearchCV</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, log_loss, cohen_kappa_score</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler, LabelEncoder</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Features and target variable</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">"likeCount"</span>, <span class="st">"duration"</span>, <span class="st">"definition"</span>, <span class="st">"topicCategories"</span>]</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[features]  <span class="co"># Feature matrix</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X.fillna(<span class="dv">0</span>)  <span class="co"># Replace NaN values with 0</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">"popularity_multi_class"</span>]  <span class="co"># Target variable (multi-class labels)</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>label_encoder <span class="op">=</span> LabelEncoder()</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> label_encoder.fit_transform(y)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure categorical features are encoded properly</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> pd.get_dummies(X, columns<span class="op">=</span>[<span class="st">"definition"</span>, <span class="st">"topicCategories"</span>], drop_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature scaling (not strictly needed for Decision Tree but kept for consistency)</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> scaler.fit_transform(X)</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validation setup</span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> StratifiedKFold(n_splits<span class="op">=</span><span class="dv">5</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameter grid for DecisionTreeClassifier</span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>    <span class="st">"criterion"</span>: [<span class="st">"gini"</span>, <span class="st">"entropy"</span>, <span class="st">"log_loss"</span>],</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>    <span class="st">"max_depth"</span>: [<span class="va">None</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>],</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>    <span class="st">"min_samples_split"</span>: [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>],</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>    <span class="st">"min_samples_leaf"</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>],</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>    <span class="st">"max_features"</span>: [<span class="va">None</span>, <span class="st">"sqrt"</span>, <span class="st">"log2"</span>]</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Decision Tree Classifier</span></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a><span class="co"># GridSearchCV for hyperparameter tuning</span></span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(</span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>clf,</span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>    param_grid<span class="op">=</span>param_grid,</span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">"accuracy"</span>,</span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>cv,</span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">1</span></span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit GridSearchCV</span></span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a>grid_search.fit(X_scaled, y)</span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Best hyperparameters</span></span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a>best_params <span class="op">=</span> grid_search.best_params_</span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best Parameters:"</span>, best_params)</span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Use the best model</span></span>
<span id="cb17-59"><a href="#cb17-59" aria-hidden="true" tabindex="-1"></a>best_clf <span class="op">=</span> grid_search.best_estimator_</span>
<span id="cb17-60"><a href="#cb17-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-61"><a href="#cb17-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize metric storage for evaluation</span></span>
<span id="cb17-62"><a href="#cb17-62" aria-hidden="true" tabindex="-1"></a>accuracies, precisions_macro, recalls_macro, f1_scores_macro <span class="op">=</span> [], [], [], []</span>
<span id="cb17-63"><a href="#cb17-63" aria-hidden="true" tabindex="-1"></a>precisions_weighted, recalls_weighted, f1_scores_weighted <span class="op">=</span> [], [], []</span>
<span id="cb17-64"><a href="#cb17-64" aria-hidden="true" tabindex="-1"></a>log_losses, kappa_scores <span class="op">=</span> [], []</span>
<span id="cb17-65"><a href="#cb17-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-66"><a href="#cb17-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validation loop with the best model</span></span>
<span id="cb17-67"><a href="#cb17-67" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_idx, test_idx <span class="kw">in</span> cv.split(X_scaled, y):</span>
<span id="cb17-68"><a href="#cb17-68" aria-hidden="true" tabindex="-1"></a>    X_train, X_test <span class="op">=</span> X_scaled[train_idx], X_scaled[test_idx]</span>
<span id="cb17-69"><a href="#cb17-69" aria-hidden="true" tabindex="-1"></a>    y_train, y_test <span class="op">=</span> y[train_idx], y[test_idx]</span>
<span id="cb17-70"><a href="#cb17-70" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-71"><a href="#cb17-71" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train the best Decision Tree Classifier</span></span>
<span id="cb17-72"><a href="#cb17-72" aria-hidden="true" tabindex="-1"></a>    best_clf.fit(X_train, y_train)</span>
<span id="cb17-73"><a href="#cb17-73" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-74"><a href="#cb17-74" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Predictions</span></span>
<span id="cb17-75"><a href="#cb17-75" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> best_clf.predict(X_test)</span>
<span id="cb17-76"><a href="#cb17-76" aria-hidden="true" tabindex="-1"></a>    y_proba <span class="op">=</span> best_clf.predict_proba(X_test)  <span class="co"># Probability estimates for all classes</span></span>
<span id="cb17-77"><a href="#cb17-77" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-78"><a href="#cb17-78" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluate metrics</span></span>
<span id="cb17-79"><a href="#cb17-79" aria-hidden="true" tabindex="-1"></a>    accuracies.append(accuracy_score(y_test, y_pred))</span>
<span id="cb17-80"><a href="#cb17-80" aria-hidden="true" tabindex="-1"></a>    precisions_macro.append(precision_score(y_test, y_pred, average<span class="op">=</span><span class="st">'macro'</span>, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb17-81"><a href="#cb17-81" aria-hidden="true" tabindex="-1"></a>    recalls_macro.append(recall_score(y_test, y_pred, average<span class="op">=</span><span class="st">'macro'</span>, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb17-82"><a href="#cb17-82" aria-hidden="true" tabindex="-1"></a>    f1_scores_macro.append(f1_score(y_test, y_pred, average<span class="op">=</span><span class="st">'macro'</span>, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb17-83"><a href="#cb17-83" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-84"><a href="#cb17-84" aria-hidden="true" tabindex="-1"></a>    precisions_weighted.append(precision_score(y_test, y_pred, average<span class="op">=</span><span class="st">'weighted'</span>, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb17-85"><a href="#cb17-85" aria-hidden="true" tabindex="-1"></a>    recalls_weighted.append(recall_score(y_test, y_pred, average<span class="op">=</span><span class="st">'weighted'</span>, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb17-86"><a href="#cb17-86" aria-hidden="true" tabindex="-1"></a>    f1_scores_weighted.append(f1_score(y_test, y_pred, average<span class="op">=</span><span class="st">'weighted'</span>, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb17-87"><a href="#cb17-87" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-88"><a href="#cb17-88" aria-hidden="true" tabindex="-1"></a>    log_losses.append(log_loss(y_test, y_proba))</span>
<span id="cb17-89"><a href="#cb17-89" aria-hidden="true" tabindex="-1"></a>    kappa_scores.append(cohen_kappa_score(y_test, y_pred))</span>
<span id="cb17-90"><a href="#cb17-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-91"><a href="#cb17-91" aria-hidden="true" tabindex="-1"></a><span class="co"># Print evaluation metrics</span></span>
<span id="cb17-92"><a href="#cb17-92" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Decision Tree Classifier with Hyperparameter Tuning - Multi-Class Classification"</span>)</span>
<span id="cb17-93"><a href="#cb17-93" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"==========================================================================="</span>)</span>
<span id="cb17-94"><a href="#cb17-94" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>np<span class="sc">.</span>mean(accuracies)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-95"><a href="#cb17-95" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Macro Precision: </span><span class="sc">{</span>np<span class="sc">.</span>mean(precisions_macro)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-96"><a href="#cb17-96" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Macro Recall: </span><span class="sc">{</span>np<span class="sc">.</span>mean(recalls_macro)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-97"><a href="#cb17-97" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Macro F1 Score: </span><span class="sc">{</span>np<span class="sc">.</span>mean(f1_scores_macro)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-98"><a href="#cb17-98" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Weighted Precision: </span><span class="sc">{</span>np<span class="sc">.</span>mean(precisions_weighted)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-99"><a href="#cb17-99" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Weighted Recall: </span><span class="sc">{</span>np<span class="sc">.</span>mean(recalls_weighted)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-100"><a href="#cb17-100" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Weighted F1 Score: </span><span class="sc">{</span>np<span class="sc">.</span>mean(f1_scores_weighted)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-101"><a href="#cb17-101" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Log Loss: </span><span class="sc">{</span>np<span class="sc">.</span>mean(log_losses)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-102"><a href="#cb17-102" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cohen Kappa Score: </span><span class="sc">{</span>np<span class="sc">.</span>mean(kappa_scores)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-103"><a href="#cb17-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-104"><a href="#cb17-104" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion matrix visualization</span></span>
<span id="cb17-105"><a href="#cb17-105" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb17-106"><a href="#cb17-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-107"><a href="#cb17-107" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the confusion matrix</span></span>
<span id="cb17-108"><a href="#cb17-108" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb17-109"><a href="#cb17-109" aria-hidden="true" tabindex="-1"></a>sns.heatmap(conf_matrix, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">"d"</span>, cmap<span class="op">=</span><span class="st">"Blues"</span>, xticklabels<span class="op">=</span>label_encoder.classes_, yticklabels<span class="op">=</span>label_encoder.classes_)</span>
<span id="cb17-110"><a href="#cb17-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-111"><a href="#cb17-111" aria-hidden="true" tabindex="-1"></a><span class="co"># Add titles and labels</span></span>
<span id="cb17-112"><a href="#cb17-112" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Confusion Matrix - Multi-Class Decision Tree (Tuned)"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb17-113"><a href="#cb17-113" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Predicted"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb17-114"><a href="#cb17-114" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Actual"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb17-115"><a href="#cb17-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-116"><a href="#cb17-116" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the plot</span></span>
<span id="cb17-117"><a href="#cb17-117" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-118"><a href="#cb17-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-119"><a href="#cb17-119" aria-hidden="true" tabindex="-1"></a><span class="co"># Detailed classification report</span></span>
<span id="cb17-120"><a href="#cb17-120" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Classification Report:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb17-121"><a href="#cb17-121" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred, target_names<span class="op">=</span>label_encoder.classes_))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Fitting 5 folds for each of 405 candidates, totalling 2025 fits
Best Parameters: {'criterion': 'gini', 'max_depth': 5, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2}
Decision Tree Classifier with Hyperparameter Tuning - Multi-Class Classification
===========================================================================
Accuracy: 0.8401339929911359
Macro Precision: 0.833451423276734
Macro Recall: 0.8258330198476095
Macro F1 Score: 0.828370209357443
Weighted Precision: 0.8393017350579939
Weighted Recall: 0.8401339929911359
Weighted F1 Score: 0.8386803790157578
Log Loss: 0.7277971131663302
Cohen Kappa Score: 0.7402908330435282</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-10-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Classification Report:

              precision    recall  f1-score   support

        high       0.83      0.80      0.81        71
         low       0.84      0.91      0.87       204
      medium       0.78      0.72      0.75       165

    accuracy                           0.82       440
   macro avg       0.82      0.81      0.81       440
weighted avg       0.82      0.82      0.82       440
</code></pre>
</div>
</div>
</section>
<section id="k-nearest-neighbors-knn-1" class="level2">
<h2 class="anchored" data-anchor-id="k-nearest-neighbors-knn-1">K-Nearest Neighbors (KNN)</h2>
<div id="cell-26" class="cell" data-execution_count="78">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> StratifiedKFold, GridSearchCV</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, log_loss, cohen_kappa_score</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler, LabelEncoder</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Features and target variable</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">"likeCount"</span>, <span class="st">"duration"</span>, <span class="st">"definition"</span>, <span class="st">"topicCategories"</span>]</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[features]  <span class="co"># Feature matrix</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X.fillna(<span class="dv">0</span>)  <span class="co"># Replace NaN values with 0</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">"popularity_multi_class"</span>]  <span class="co"># Target variable (multi-class labels)</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>label_encoder <span class="op">=</span> LabelEncoder()</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> label_encoder.fit_transform(y)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure categorical features are encoded properly</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> pd.get_dummies(X, columns<span class="op">=</span>[<span class="st">"definition"</span>, <span class="st">"topicCategories"</span>], drop_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature scaling (important for KNN because it relies on distance metrics)</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> scaler.fit_transform(X)</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validation setup</span></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> StratifiedKFold(n_splits<span class="op">=</span><span class="dv">5</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameter grid for KNN</span></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_neighbors'</span>: [<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">11</span>],  <span class="co"># Number of neighbors</span></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>    <span class="st">'weights'</span>: [<span class="st">'uniform'</span>, <span class="st">'distance'</span>],  <span class="co"># Weighting scheme</span></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>    <span class="st">'metric'</span>: [<span class="st">'euclidean'</span>, <span class="st">'manhattan'</span>, <span class="st">'minkowski'</span>]  <span class="co"># Distance metrics</span></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a><span class="co"># K-Nearest Neighbors Classifier</span></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> KNeighborsClassifier()</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a><span class="co"># GridSearchCV for hyperparameter tuning</span></span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>    estimator<span class="op">=</span>clf,</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>    param_grid<span class="op">=</span>param_grid,</span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>    scoring<span class="op">=</span><span class="st">"accuracy"</span>,</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>cv,</span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">1</span></span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit GridSearchCV</span></span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>grid_search.fit(X_scaled, y)</span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Best hyperparameters</span></span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>best_params <span class="op">=</span> grid_search.best_params_</span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best Parameters:"</span>, best_params)</span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Use the best model</span></span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a>best_clf <span class="op">=</span> grid_search.best_estimator_</span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize metric storage for evaluation</span></span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a>accuracies, precisions_macro, recalls_macro, f1_scores_macro <span class="op">=</span> [], [], [], []</span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a>precisions_weighted, recalls_weighted, f1_scores_weighted <span class="op">=</span> [], [], []</span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a>log_losses, kappa_scores <span class="op">=</span> [], []</span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validation loop with the best model</span></span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_idx, test_idx <span class="kw">in</span> cv.split(X_scaled, y):</span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a>    X_train, X_test <span class="op">=</span> X_scaled[train_idx], X_scaled[test_idx]</span>
<span id="cb20-67"><a href="#cb20-67" aria-hidden="true" tabindex="-1"></a>    y_train, y_test <span class="op">=</span> y[train_idx], y[test_idx]</span>
<span id="cb20-68"><a href="#cb20-68" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-69"><a href="#cb20-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train the best KNN model</span></span>
<span id="cb20-70"><a href="#cb20-70" aria-hidden="true" tabindex="-1"></a>    best_clf.fit(X_train, y_train)</span>
<span id="cb20-71"><a href="#cb20-71" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-72"><a href="#cb20-72" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Predictions</span></span>
<span id="cb20-73"><a href="#cb20-73" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> best_clf.predict(X_test)</span>
<span id="cb20-74"><a href="#cb20-74" aria-hidden="true" tabindex="-1"></a>    y_proba <span class="op">=</span> best_clf.predict_proba(X_test)  <span class="co"># Probability estimates for all classes</span></span>
<span id="cb20-75"><a href="#cb20-75" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-76"><a href="#cb20-76" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluate metrics</span></span>
<span id="cb20-77"><a href="#cb20-77" aria-hidden="true" tabindex="-1"></a>    accuracies.append(accuracy_score(y_test, y_pred))</span>
<span id="cb20-78"><a href="#cb20-78" aria-hidden="true" tabindex="-1"></a>    precisions_macro.append(precision_score(y_test, y_pred, average<span class="op">=</span><span class="st">'macro'</span>, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb20-79"><a href="#cb20-79" aria-hidden="true" tabindex="-1"></a>    recalls_macro.append(recall_score(y_test, y_pred, average<span class="op">=</span><span class="st">'macro'</span>, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb20-80"><a href="#cb20-80" aria-hidden="true" tabindex="-1"></a>    f1_scores_macro.append(f1_score(y_test, y_pred, average<span class="op">=</span><span class="st">'macro'</span>, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb20-81"><a href="#cb20-81" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-82"><a href="#cb20-82" aria-hidden="true" tabindex="-1"></a>    precisions_weighted.append(precision_score(y_test, y_pred, average<span class="op">=</span><span class="st">'weighted'</span>, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb20-83"><a href="#cb20-83" aria-hidden="true" tabindex="-1"></a>    recalls_weighted.append(recall_score(y_test, y_pred, average<span class="op">=</span><span class="st">'weighted'</span>, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb20-84"><a href="#cb20-84" aria-hidden="true" tabindex="-1"></a>    f1_scores_weighted.append(f1_score(y_test, y_pred, average<span class="op">=</span><span class="st">'weighted'</span>, zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb20-85"><a href="#cb20-85" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-86"><a href="#cb20-86" aria-hidden="true" tabindex="-1"></a>    log_losses.append(log_loss(y_test, y_proba))</span>
<span id="cb20-87"><a href="#cb20-87" aria-hidden="true" tabindex="-1"></a>    kappa_scores.append(cohen_kappa_score(y_test, y_pred))</span>
<span id="cb20-88"><a href="#cb20-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-89"><a href="#cb20-89" aria-hidden="true" tabindex="-1"></a><span class="co"># Print evaluation metrics</span></span>
<span id="cb20-90"><a href="#cb20-90" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"K-Nearest Neighbors (KNN) with Hyperparameter Tuning - Multi-Class Classification"</span>)</span>
<span id="cb20-91"><a href="#cb20-91" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"==========================================================================="</span>)</span>
<span id="cb20-92"><a href="#cb20-92" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>np<span class="sc">.</span>mean(accuracies)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-93"><a href="#cb20-93" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Macro Precision: </span><span class="sc">{</span>np<span class="sc">.</span>mean(precisions_macro)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-94"><a href="#cb20-94" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Macro Recall: </span><span class="sc">{</span>np<span class="sc">.</span>mean(recalls_macro)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-95"><a href="#cb20-95" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Macro F1 Score: </span><span class="sc">{</span>np<span class="sc">.</span>mean(f1_scores_macro)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-96"><a href="#cb20-96" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Weighted Precision: </span><span class="sc">{</span>np<span class="sc">.</span>mean(precisions_weighted)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-97"><a href="#cb20-97" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Weighted Recall: </span><span class="sc">{</span>np<span class="sc">.</span>mean(recalls_weighted)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-98"><a href="#cb20-98" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Weighted F1 Score: </span><span class="sc">{</span>np<span class="sc">.</span>mean(f1_scores_weighted)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-99"><a href="#cb20-99" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Log Loss: </span><span class="sc">{</span>np<span class="sc">.</span>mean(log_losses)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-100"><a href="#cb20-100" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cohen Kappa Score: </span><span class="sc">{</span>np<span class="sc">.</span>mean(kappa_scores)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-101"><a href="#cb20-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-102"><a href="#cb20-102" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion matrix visualization</span></span>
<span id="cb20-103"><a href="#cb20-103" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_test, y_pred)</span>
<span id="cb20-104"><a href="#cb20-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-105"><a href="#cb20-105" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the confusion matrix</span></span>
<span id="cb20-106"><a href="#cb20-106" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb20-107"><a href="#cb20-107" aria-hidden="true" tabindex="-1"></a>sns.heatmap(conf_matrix, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">"d"</span>, cmap<span class="op">=</span><span class="st">"Blues"</span>, xticklabels<span class="op">=</span>label_encoder.classes_, yticklabels<span class="op">=</span>label_encoder.classes_)</span>
<span id="cb20-108"><a href="#cb20-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-109"><a href="#cb20-109" aria-hidden="true" tabindex="-1"></a><span class="co"># Add titles and labels</span></span>
<span id="cb20-110"><a href="#cb20-110" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Confusion Matrix - Multi-Class KNN (Tuned)"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb20-111"><a href="#cb20-111" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Predicted"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb20-112"><a href="#cb20-112" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Actual"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb20-113"><a href="#cb20-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-114"><a href="#cb20-114" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the plot</span></span>
<span id="cb20-115"><a href="#cb20-115" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-116"><a href="#cb20-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-117"><a href="#cb20-117" aria-hidden="true" tabindex="-1"></a><span class="co"># Detailed classification report</span></span>
<span id="cb20-118"><a href="#cb20-118" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Classification Report:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb20-119"><a href="#cb20-119" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred, target_names<span class="op">=</span>label_encoder.classes_))</span>
<span id="cb20-120"><a href="#cb20-120" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Fitting 5 folds for each of 30 candidates, totalling 150 fits
Best Parameters: {'metric': 'manhattan', 'n_neighbors': 9, 'weights': 'distance'}
K-Nearest Neighbors (KNN) with Hyperparameter Tuning - Multi-Class Classification
===========================================================================
Accuracy: 0.7724695938981653
Macro Precision: 0.7794176006169204
Macro Recall: 0.7433875665336453
Macro F1 Score: 0.7568255245158504
Weighted Precision: 0.7721904010101952
Weighted Recall: 0.7724695938981653
Weighted F1 Score: 0.7687121833806551
Log Loss: 1.4498988304474825
Cohen Kappa Score: 0.6249967306705042</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-11-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Classification Report:

              precision    recall  f1-score   support

        high       0.86      0.61      0.71        71
         low       0.75      0.87      0.81       204
      medium       0.69      0.65      0.67       165

    accuracy                           0.74       440
   macro avg       0.77      0.71      0.73       440
weighted avg       0.75      0.74      0.74       440
</code></pre>
</div>
</div>
</section>
</section>
<section id="summary-and-interpretation-of-results" class="level1">
<h1>Summary and Interpretation of Results</h1>
<hr>
<section id="regression-section-2" class="level2">
<h2 class="anchored" data-anchor-id="regression-section-2">Regression Section</h2>
<section id="random-forest-regressor-2" class="level3">
<h3 class="anchored" data-anchor-id="random-forest-regressor-2">Random Forest Regressor</h3>
<ol type="1">
<li><strong>Best Hyperparameters</strong>:
<ul>
<li>bootstrap: True<br>
</li>
<li>max_depth: 20<br>
</li>
<li>max_features: ‘sqrt’<br>
</li>
<li>min_samples_leaf: 3<br>
</li>
<li>min_samples_split: 5<br>
</li>
<li>n_estimators: 445</li>
</ul></li>
<li><strong>Performance Metrics</strong>:
<ul>
<li>RMSE: 0.2495<br>
</li>
<li>MAE: 0.1099<br>
</li>
<li>R²: 0.7713</li>
</ul></li>
</ol>
<section id="model-performance-summary" class="level4">
<h4 class="anchored" data-anchor-id="model-performance-summary">Model Performance Summary:</h4>
<p>The Random Forest Regressor achieved optimal performance with the following hyperparameters: <code>bootstrap=True</code>, <code>max_depth=20</code>, <code>max_features='sqrt'</code>, <code>min_samples_leaf=3</code>, <code>min_samples_split=5</code>, and <code>n_estimators=445</code>. The RMSE of 0.2495 and MAE of 0.1099 are relatively low, showing that the model’s predictions are close to the actual view counts, with only minor errors. The R² value of 0.7713 is also high. This demonstrates strong predictive capability, but there is room for improvement, as 22.87% of the variance remains unexplained.</p>
</section>
<section id="interpretationtechnical-implications" class="level4">
<h4 class="anchored" data-anchor-id="interpretationtechnical-implications">Interpretation/Technical implications:</h4>
<p>As shown in the figure, most points cluster near the origin, indicating that the model performs well for lower view counts. This aligns with expectations, as the number of highly popular videos is relatively small. However, as the view count increases, the deviation from the line becomes more significant, suggesting that the model struggles to predict higher values. This is likely due to data sparsity caused by the limited number of highly popular videos. Overall, while the model performs effectively for the majority of the data, its performance on extreme view counts could be improved through better feature engineering or outlier handling.</p>
<hr>
</section>
</section>
<section id="linear-regression" class="level3">
<h3 class="anchored" data-anchor-id="linear-regression">Linear Regression</h3>
<ol type="1">
<li><strong>Best Hyperparameters</strong>:
<ul>
<li>alpha: 10<br>
</li>
<li>solver: ‘auto’</li>
</ul></li>
<li><strong>Performance Metrics</strong>:
<ul>
<li>RMSE: 0.2057<br>
</li>
<li>MAE: 0.1241<br>
</li>
<li>R²: 0.8033</li>
</ul></li>
</ol>
<section id="model-performance-summary-1" class="level4">
<h4 class="anchored" data-anchor-id="model-performance-summary-1">Model Performance Summary:</h4>
<p>The Linear Regression model achieved optimal performance with the following hyperparameters: alpha=10 and solver=‘auto’. The RMSE of 0.2057 and MAE of 0.1241 are relatively low, indicating that the model’s predictions closely match the actual view counts with minimal errors. The R² value of 0.8033 is relatively high, demonstrating that the model explains 80.33% of the variance in view counts. However, there is still room for improvement, as 19.67% of the variance remains unexplained, which could be addressed with additional features or refined model tuning.</p>
</section>
<section id="interpretationtechnical-implications-1" class="level4">
<h4 class="anchored" data-anchor-id="interpretationtechnical-implications-1">Interpretation/Technical implications:</h4>
<p>As shown in the figure, most points align closely with the red dashed “Perfect Fit” line for lower view counts, indicating that the model performs well in this range. This outcome aligns with expectations, as lower view counts are more common in the dataset. However, as view counts increase, the deviation from the line becomes more pronounced, highlighting the model’s struggle to accurately predict higher values. This issue is likely due to data sparsity, as the dataset contains a limited number of highly popular videos. Overall, while the model performs effectively for the majority of the data, its accuracy for extreme view counts could be improved through better feature engineering, the addition of relevant predictive features, or addressing data imbalance.</p>
<hr>
</section>
</section>
<section id="gradient-boosting-regressor-2" class="level3">
<h3 class="anchored" data-anchor-id="gradient-boosting-regressor-2">Gradient Boosting Regressor</h3>
<ol type="1">
<li><strong>Best Hyperparameters</strong>:
<ul>
<li>learning_rate: 0.01<br>
</li>
<li>max_depth: 5<br>
</li>
<li>max_features: None<br>
</li>
<li>min_samples_leaf: 4<br>
</li>
<li>min_samples_split: 2<br>
</li>
<li>n_estimators: 300<br>
</li>
<li>subsample: 0.8</li>
</ul></li>
<li><strong>Performance Metrics</strong>:
<ul>
<li>RMSE: 0.1295<br>
</li>
<li>MAE: 0.0798<br>
</li>
<li>R²: 0.8727</li>
</ul></li>
</ol>
<section id="model-performance-summary-2" class="level4">
<h4 class="anchored" data-anchor-id="model-performance-summary-2">Model Performance Summary:</h4>
<p>The Gradient Boosting Regressor achieved optimal performance with the following hyperparameters: <code>learning_rate=0.01</code>, <code>max_depth=5</code>, <code>max_features=None</code>, <code>min_samples_leaf=4</code>, <code>min_samples_split=2</code>, <code>n_estimators=300</code>, and <code>subsample=0.8</code>. The model demonstrated strong predictive accuracy with an <strong>RMSE</strong> of 0.1295 and <strong>MAE</strong> of 0.0798, indicating small prediction errors. The <strong>R²</strong> value of 0.8727 signifies that the model explains approximately 87.27% of the variance in view counts, reflecting high reliability and effectiveness in predicting view counts. However, there is still room for improvement to address the unexplained variance (12.73%). Overall, the model balances precision and generalization effectively across the dataset.</p>
</section>
<section id="interpretationtechnical-implications-2" class="level4">
<h4 class="anchored" data-anchor-id="interpretationtechnical-implications-2">Interpretation/Technical implications:</h4>
<p>Most data points cluster near the origin, indicating strong model performance for low view counts, where the predictions align closely with the perfect fit line. In the moderate view count range (0.25 to 0.75), the points show increased dispersion, suggesting slightly reduced accuracy. For high view counts (above 0.75), the scatter becomes sparser with significant deviations from the line, highlighting the model’s difficulty in predicting high view counts accurately. This is likely due to data sparsity in the high view count range and potential limitations in the selected features. Overall, the model performs well for lower and moderate intervals but struggles with higher intervals, where improvements in feature engineering or data balance could enhance performance.</p>
<hr>
</section>
</section>
<section id="discussion-for-regression-section" class="level3">
<h3 class="anchored" data-anchor-id="discussion-for-regression-section">Discussion for Regression Section</h3>
<p><strong>Result Interpretation</strong>:</p>
<p>This analysis highlights the performance of three regression models — Random Forest Regressor, Linear Regression, and Gradient Boosting Regressor — in predicting view counts. Among all models, the selected features (<code>likeCount</code>, <code>duration</code>, <code>definition</code>, <code>topicCategories</code>, <code>popularity</code>) proved effective in explaining the variance in view counts. The <strong>Gradient Boosting Regressor</strong> demonstrated the strongest performance, achieving an R² of 0.8727, which indicates its superior ability to explain the variance in view counts. The <strong>Linear Regression</strong> followed with an R² of 0.8033, showcasing strong predictive capabilities but limited by its linear assumptions. Lastly, the <strong>Random Forest Regressor</strong> achieved an R² of 0.7713, reflecting good performance overall, but less accurate compared to the other models.</p>
<p><strong>Model Performance Comparison</strong>:</p>
<ul>
<li><p><strong>Gradient Boosting Regressor</strong> achieved the best performance, as indicated by its lowest RMSE (0.1295) and MAE (0.0798), alongside the highest R² value (0.8727). This suggests that the model handles both variance and bias effectively, leveraging its sequential learning process to iteratively minimize errors.</p></li>
<li><p><strong>Linear Regression</strong>, with an RMSE of 0.2057 and MAE of 0.1241, performed well for a simple model. However, its R² of 0.8033 indicates that it struggles to capture non-linear relationships, particularly for high view count predictions.</p></li>
<li><p><strong>Random Forest Regressor</strong> delivered a solid performance with an RMSE of 0.2495 and MAE of 0.1099 but lagged in explaining variance, as evidenced by its R² of 0.7713. Its performance was limited in the higher view count range, where data sparsity likely reduced its effectiveness.</p></li>
</ul>
<p><strong>Insights Gained</strong>:<br>
The analysis revealed that the selected features (<code>likeCount</code>, <code>duration</code>, <code>definition</code>, <code>topicCategories</code>, <code>popularity</code>) demonstrate strong predictive power, particularly for lower and moderate view counts, though additional features may be needed to improve predictions for extreme values. A key challenge across all models was data sparsity in the high view count range, highlighting the need for better handling of sparse data through techniques like feature engineering, outlier management, or data augmentation. Among the models, the Gradient Boosting Regressor consistently outperformed others, demonstrating its ability to capture complex patterns in the data and effectively balancing bias and variance, making it the most reliable choice for view count prediction.</p>
<hr>
</section>
</section>
<section id="binary-classification-section-1" class="level2">
<h2 class="anchored" data-anchor-id="binary-classification-section-1">Binary Classification Section</h2>
<section id="random-forest-classifier-2" class="level3">
<h3 class="anchored" data-anchor-id="random-forest-classifier-2">Random Forest Classifier</h3>
<ol type="1">
<li><strong>Best Hyperparameters</strong>:
<ul>
<li>bootstrap: True<br>
</li>
<li>class_weight: None<br>
</li>
<li>max_depth: 30<br>
</li>
<li>max_features: ‘sqrt’<br>
</li>
<li>min_samples_leaf: 1<br>
</li>
<li>min_samples_split: 10<br>
</li>
<li>n_estimators: 300</li>
</ul></li>
<li><strong>Performance Metrics</strong>:
<ul>
<li>Accuracy: 0.9496<br>
</li>
<li>Precision: 0.9626<br>
</li>
<li>Recall: 0.9800<br>
</li>
<li>F1 Score: 0.9712<br>
</li>
<li>ROC AUC: 0.9644</li>
</ul></li>
</ol>
<section id="model-performance-summary-3" class="level4">
<h4 class="anchored" data-anchor-id="model-performance-summary-3">Model Performance Summary:</h4>
<p>The Random Forest Classifier demonstrated excellent performance with high accuracy (94.96%) and F1 score (97.12%), indicating a reliable and well-balanced model. The precision score (96.26%) reflects strong specificity, correctly identifying most “high” popularity videos, while the recall (98.00%) highlights its ability to capture nearly all actual “high” popularity cases. A ROC AUC of 96.44% confirms the model’s strong capability in distinguishing between “high” and “low” popularity videos, making its overall performance robust and effective.</p>
</section>
<section id="interpretationtechnical-implications-3" class="level4">
<h4 class="anchored" data-anchor-id="interpretationtechnical-implications-3">Interpretation/Technical Implications:</h4>
<p>The confusion matrix shows that the model performs well overall, correctly predicting 572 “low” popularity videos and accurately identifying 66 “high” popularity videos. However, there are 23 false negatives (actual “high” predicted as “low”) and 14 false positives (actual “low” predicted as “high”). The relatively higher number of false negatives may be due to the smaller sample size of “high” popularity videos in the dataset. This suggests that while the model excels in predicting “low” popularity videos, it has room for improvement in handling “high” popularity predictions, potentially through addressing class imbalance.</p>
<hr>
</section>
</section>
<section id="logistic-regression-2" class="level3">
<h3 class="anchored" data-anchor-id="logistic-regression-2">Logistic Regression</h3>
<ol type="1">
<li><strong>Best Hyperparameters</strong>:
<ul>
<li>C: 0.1<br>
</li>
<li>max_iter: 5000<br>
</li>
<li>penalty: ‘l1’<br>
</li>
<li>solver: ‘liblinear’</li>
</ul></li>
<li><strong>Performance Metrics</strong>:
<ul>
<li>Accuracy: 0.9423<br>
</li>
<li>Precision: 0.9531<br>
</li>
<li>Recall: 0.9818<br>
</li>
<li>F1 Score: 0.9672<br>
</li>
<li>ROC AUC: 0.9623</li>
</ul></li>
</ol>
<section id="model-performance-summary-4" class="level4">
<h4 class="anchored" data-anchor-id="model-performance-summary-4">Model Performance Summary:</h4>
<p>The Logistic Regression model exhibited strong performance with high accuracy (94.23%) and F1 score (96.72%), demonstrating its effectiveness as a balanced and reliable classifier. The precision score (95.31%) indicates strong specificity, correctly identifying most “high” popularity videos, while the recall (98.18%) shows the model’s ability to capture nearly all actual “high” popularity cases. A ROC AUC of 96.23% confirms its robust capability in distinguishing between “high” and “low” popularity videos.</p>
</section>
<section id="interpretationtechnical-implications-4" class="level4">
<h4 class="anchored" data-anchor-id="interpretationtechnical-implications-4">Interpretation/Technical Implications:</h4>
<p>The confusion matrix shows that the model correctly predicted 346 “low” popularity videos and 37 “high” popularity videos. However, it also resulted in 16 false negatives (actual “high” predicted as “low”) and 6 false positives (actual “low” predicted as “high”). The relatively higher rate of false negatives indicates that the model is slightly conservative when predicting “high” popularity videos, likely due to the smaller sample size of “high” popularity cases in the dataset. While the model’s overall performance is strong, its ability to predict “high” popularity videos could be improved, potentially by lowering the threshold for classifying videos as “high” popularity.</p>
<hr>
</section>
</section>
<section id="gradient-boosting-classifier-4" class="level3">
<h3 class="anchored" data-anchor-id="gradient-boosting-classifier-4">Gradient Boosting Classifier</h3>
<ol type="1">
<li><strong>Best Hyperparameters</strong>:
<ul>
<li>learning_rate: 0.01<br>
</li>
<li>max_depth: 5<br>
</li>
<li>max_features: None<br>
</li>
<li>min_samples_leaf: 1<br>
</li>
<li>min_samples_split: 2<br>
</li>
<li>n_estimators: 300<br>
</li>
<li>subsample: 0.8</li>
</ul></li>
<li><strong>Performance Metrics</strong>:
<ul>
<li>Accuracy: 0.9516<br>
</li>
<li>Precision: 0.9642<br>
</li>
<li>Recall: 0.9806<br>
</li>
<li>F1 Score: 0.9724<br>
</li>
<li>ROC AUC: 0.9696</li>
</ul></li>
</ol>
<section id="model-performance-summary-5" class="level4">
<h4 class="anchored" data-anchor-id="model-performance-summary-5">Model Performance Summary:</h4>
<p>The Gradient Boosting Classifier demonstrated excellent performance, achieving high accuracy and F1 scores, indicating a well-balanced and effective model. The precision of 96.42% reflects strong specificity, accurately identifying most “high” popularity cases, while the recall of 98.06% highlights its ability to capture nearly all true “high” popularity videos. The ROC AUC score of 96.96% confirms the model’s robust discriminatory power between “high” and “low” popularity classes.</p>
</section>
<section id="interpretationtechnical-implications-5" class="level4">
<h4 class="anchored" data-anchor-id="interpretationtechnical-implications-5">Interpretation/Technical Implications:</h4>
<p>The confusion matrix shows that the model correctly predicted 345 “low” popularity videos and 40 “high” popularity videos. However, it also produced 13 false negatives (actual “high” predicted as “low”) and 7 false positives (actual “low” predicted as “high”). These results suggest that the model performs better at identifying “low” popularity cases but remains effective overall in both classes. Due to the smaller sample size of “high” popularity cases in the dataset, the model’s ability to predict “high” popularity videos could be improved, potentially by lowering the threshold for classifying videos as “high” popularity.</p>
<hr>
</section>
</section>
<section id="discussion-for-binary-classification-section" class="level3">
<h3 class="anchored" data-anchor-id="discussion-for-binary-classification-section">Discussion for Binary Classification Section</h3>
<p><strong>Result Interpretation</strong>:<br>
The analysis highlights the performance of three binary classification models — Random Forest Classifier, Logistic Regression, and Gradient Boosting Classifier — in predicting video popularity. All three models demonstrated strong overall performance, with accuracy, precision, recall, F1 score, and ROC AUC metrics consistently high. Among them, the <strong>Gradient Boosting Classifier</strong> achieved the best overall performance, with an F1 score of 97.24% and a ROC AUC of 96.96%, indicating its superior ability to balance precision and recall while effectively distinguishing between “high” and “low” popularity classes. The <strong>Random Forest Classifier</strong> and <strong>Logistic Regression</strong> also delivered excellent results, with only minor differences in their ability to handle false positives and false negatives.</p>
<p><strong>Model Performance Comparison</strong>:</p>
<ul>
<li><p><strong>Gradient Boosting Classifier</strong> emerged as the top-performing model, with the highest ROC AUC (96.96%) and F1 score (97.24%). It showed the strongest balance between precision and recall, making it effective in identifying both “high” and “low” popularity videos. However, it produced slightly more false negatives compared to the Random Forest Classifier.</p></li>
<li><p><strong>Random Forest Classifier</strong> performed slightly below Gradient Boosting, with an F1 score of 97.12% and ROC AUC of 96.44%. It excelled in identifying “low” popularity videos but was slightly more conservative in predicting “high” popularity cases, resulting in 23 false negatives.</p></li>
<li><p><strong>Logistic Regression</strong> demonstrated robust performance with an F1 score of 96.72% and ROC AUC of 96.23%. While it maintained a strong balance between precision and recall, its slightly higher false-negative rate compared to the other models indicates a conservative bias when predicting “high” popularity videos.</p></li>
</ul>
<p><strong>Insights Gained</strong>:<br>
The analysis shows that all three models effectively utilized the selected features (“likeCount,” “duration,” “definition,” and “topicCategories”) to distinguish between “high” and “low” popularity videos. All three models performed better in predicting “low” popularity videos compared to “high” popularity ones. However, class imbalance remains a challenge, as evidenced by the relatively higher misclassification rates, particularly for the “high” popularity category. Fine-tuning the classification threshold could improve the ability to correctly identify “high” popularity videos without significantly increasing the false positive rate. Overall, the Gradient Boosting Classifier achieved a strong balance between precision, recall, and robustness when handling imbalanced data.</p>
<hr>
</section>
</section>
<section id="multi-class-classification-section-2" class="level2">
<h2 class="anchored" data-anchor-id="multi-class-classification-section-2">Multi-class Classification Section</h2>
<section id="gradient-boosting-classifier-5" class="level3">
<h3 class="anchored" data-anchor-id="gradient-boosting-classifier-5">Gradient Boosting Classifier</h3>
<ol type="1">
<li><strong>Best Hyperparameters</strong>:
<ul>
<li>learning_rate: 0.01<br>
</li>
<li>max_depth: 5<br>
</li>
<li>min_samples_leaf: 1<br>
</li>
<li>min_samples_split: 5<br>
</li>
<li>n_estimators: 200<br>
</li>
<li>subsample: 0.8</li>
</ul></li>
<li><strong>Performance Metrics</strong>:
<ul>
<li>Accuracy: 0.853<br>
</li>
<li>Macro Precision: 0.852<br>
</li>
<li>Macro Recall: 0.836<br>
</li>
<li>Macro F1 Score: 0.843<br>
</li>
<li>Weighted Precision: 0.853<br>
</li>
<li>Weighted Recall: 0.853<br>
</li>
<li>Weighted F1 Score: 0.852<br>
</li>
<li>Log Loss: 0.407<br>
</li>
<li>Cohen Kappa Score: 0.761</li>
</ul></li>
</ol>
<section id="model-performance-summary-6" class="level4">
<h4 class="anchored" data-anchor-id="model-performance-summary-6">Model Performance Summary:</h4>
<p>The Gradient Boosting Classifier demonstrated strong overall performance, with an accuracy of 85.3% and a weighted F1 score of 85.2%. These values indicate that the model performs reliably across all classes, striking a balance between precision (85.3%) and recall (85.3%). The macro precision (85.2%) and macro F1 score (84.3%) further reinforce its effectiveness in multi-class classification scenarios. Additionally, the low log loss value (0.407) suggests that the model provides well-calibrated probability estimates, which is a critical requirement for applications where probabilistic outputs are needed. The Cohen Kappa score of 0.761 indicates substantial agreement between the model’s predictions and the actual class labels.</p>
</section>
<section id="interpretationtechnical-implications-6" class="level4">
<h4 class="anchored" data-anchor-id="interpretationtechnical-implications-6">Interpretation/Technical Implications:</h4>
<p>The model excelled in predicting the “low” class, correctly classifying 186 instances with minimal errors, which reflects its strong ability to identify this category. For the “high” class, 58 instances were correctly classified, but 13 were misclassified as “medium,” indicating some confusion between these two classes. For the “medium” class, the model identified 122 instances correctly, but 34 were misclassified as “low,” and 9 as “high,” which highlights the inherent difficulty in distinguishing the medium class due to overlapping features. While the model’s overall performance is good, the challenges in correctly predicting the “medium” class suggest opportunities for improvement. Better feature engineering, such as creating more discriminative features or addressing potential data imbalance, could enhance the model’s ability to separate the classes effectively. Despite these challenges, the model’s consistent performance across precision, recall, and F1 score metrics makes it a reliable choice for this classification task.</p>
<hr>
</section>
</section>
<section id="decision-tree-classifier-2" class="level3">
<h3 class="anchored" data-anchor-id="decision-tree-classifier-2">Decision Tree Classifier</h3>
<ol type="1">
<li><strong>Best Hyperparameters</strong>:
<ul>
<li>criterion: gini<br>
</li>
<li>max_depth: 5<br>
</li>
<li>max_features: None<br>
</li>
<li>min_samples_leaf: 1<br>
</li>
<li>min_samples_split: 2</li>
</ul></li>
<li><strong>Performance Metrics</strong>:
<ul>
<li>Accuracy: 0.84<br>
</li>
<li>Macro Precision: 0.83<br>
</li>
<li>Macro Recall: 0.82<br>
</li>
<li>Macro F1 Score: 0.83<br>
</li>
<li>Weighted Precision: 0.84<br>
</li>
<li>Weighted Recall: 0.84<br>
</li>
<li>Weighted F1 Score: 0.84<br>
</li>
<li>Log Loss: 0.73<br>
</li>
<li>Cohen Kappa Score: 0.74</li>
</ul></li>
</ol>
<section id="model-performance-summary-7" class="level4">
<h4 class="anchored" data-anchor-id="model-performance-summary-7">Model Performance Summary:</h4>
<p>The Decision Tree Classifier demonstrated solid overall performance, achieving an accuracy of 84.0% and a weighted F1 score of 83.9%, indicating reliable classification across all classes. Its macro precision (83.3%) and macro F1 score (82.8%) reflect balanced performance across the “high,” “medium,” and “low” classes, which is critical for multi-class tasks. The low log loss value of 0.73 highlights the model’s ability to provide reasonably calibrated probability estimates, essential for applications requiring confidence-based decision-making. Additionally, the Cohen Kappa score of 0.74 indicates substantial agreement between the model’s predictions and actual labels. These metrics collectively demonstrate the model’s effectiveness in multi-class classification scenarios, though its misclassification of some “medium” instances as “low” or “high” points to potential improvements through enhanced feature engineering or parameter tuning.</p>
</section>
<section id="interpretationtechnical-implications-7" class="level4">
<h4 class="anchored" data-anchor-id="interpretationtechnical-implications-7">Interpretation/Technical Implications:</h4>
<p>The confusion matrix reveals that the model excels in predicting the “low” class, with 185 correctly classified instances and only 19 misclassified as “medium.” The “high” class is also predicted well, with 57 correct classifications, but 14 instances were misclassified as “medium.” The “medium” class presented the greatest challenge, with 118 correctly predicted instances, but 35 were misclassified as “low” and 12 as “high.” This indicates that the “medium” class is more prone to misclassification, likely due to overlapping feature distributions with the other two classes. The classification report shows that the precision, recall, and F1 scores for the “high” and “low” classes are relatively strong, exceeding 80%, whereas the “medium” class lags slightly behind with an F1 score of 75%. This suggests that while the model performs well overall, additional work is needed to improve the distinction between “medium” and the other two classes. Enhanced feature engineering, such as creating features that better capture the nuances of the “medium” class, or using ensemble methods, may improve performance.</p>
<hr>
</section>
</section>
<section id="k-nearest-neighbors-knn-2" class="level3">
<h3 class="anchored" data-anchor-id="k-nearest-neighbors-knn-2">K-Nearest Neighbors (KNN)</h3>
<ol type="1">
<li><strong>Best Hyperparameters</strong>:
<ul>
<li>metric: manhattan<br>
</li>
<li>n_neighbors: 9<br>
</li>
<li>weights: distance</li>
</ul></li>
<li><strong>Performance Metrics</strong>:
<ul>
<li>Accuracy: 0.772<br>
</li>
<li>Macro Precision: 0.779<br>
</li>
<li>Macro Recall: 0.743<br>
</li>
<li>Macro F1 Score: 0.757<br>
</li>
<li>Weighted Precision: 0.772<br>
</li>
<li>Weighted Recall: 0.772<br>
</li>
<li>Weighted F1 Score: 0.769<br>
</li>
<li>Log Loss: 1.450<br>
</li>
<li>Cohen Kappa Score: 0.625</li>
</ul></li>
</ol>
<section id="model-performance-summary-8" class="level4">
<h4 class="anchored" data-anchor-id="model-performance-summary-8">Model Performance Summary:</h4>
<p>The K-Nearest Neighbors (KNN) classifier achieved moderate performance, with an accuracy of 77.2% and a weighted F1 score of 76.9%, indicating that it provides reasonable predictions across all classes. The macro F1 score of 75.7% and macro precision of 77.9% highlight its balanced performance but show some room for improvement in distinguishing among the “high,” “medium,” and “low” classes. The log loss value of 1.450, while relatively high compared to other models, suggests less confidence in its probabilistic estimates. The Cohen Kappa score of 0.625 indicates a moderate level of agreement between the model’s predictions and the actual class labels.</p>
</section>
<section id="interpretationtechnical-implications-8" class="level4">
<h4 class="anchored" data-anchor-id="interpretationtechnical-implications-8">Interpretation/Technical Implications:</h4>
<p>The confusion matrix reveals that the KNN model performed well in predicting the “low” class, correctly classifying 177 instances, but struggled with the “high” and “medium” classes. For the “high” class, the model correctly identified 43 instances but misclassified 22 as “medium,” suggesting some overlap in the feature space between these two classes. Similarly, for the “medium” class, it identified 107 instances correctly but misclassified 52 as “low” and 6 as “high.” These misclassifications indicate that KNN struggles with feature boundaries, particularly for intermediate classes. The reliance on the Manhattan distance metric and distance-based weighting helped improve performance, but further feature engineering or dimensionality reduction may enhance its discriminative power. Despite its limitations, KNN provides a good baseline for multi-class classification and is effective when computational efficiency and interpretability are priorities.</p>
<hr>
</section>
</section>
<section id="discussion-for-multi-class-classification-section" class="level3">
<h3 class="anchored" data-anchor-id="discussion-for-multi-class-classification-section">Discussion for Multi-class Classification Section</h3>
<p><strong>Result Interpretation</strong>:<br>
The analysis evaluates the performance of three multi-class classification models — Gradient Boosting Classifier, Decision Tree Classifier, and K-Nearest Neighbors (KNN). Among the models, <strong>Gradient Boosting Classifier</strong> demonstrated the best overall performance, achieving the highest accuracy (85.3%) and weighted F1 score (85.2%). The <strong>Decision Tree Classifier</strong> followed closely with an accuracy of 84.0% and a weighted F1 score of 83.9%, showing solid classification performance. The <strong>KNN Classifier</strong>, while effective for “low” class predictions, lagged with an accuracy of 77.2% and a weighted F1 score of 76.9%, reflecting its struggles with overlapping feature spaces.</p>
<p><strong>Model Performance Comparison</strong>:</p>
<ol type="1">
<li><p><strong>Gradient Boosting Classifier</strong> excelled in balancing precision, recall, and F1 scores across all classes. Its low log loss (0.407) and high Cohen Kappa score (0.761) reflect its robust performance and reliable probabilistic outputs.</p></li>
<li><p><strong>Decision Tree Classifier</strong> offered competitive results, particularly for the “low” class, but struggled with the “medium” class due to feature overlap. Despite these challenges, its low log loss (0.73) and substantial Cohen Kappa score (0.74) highlight its reliability.</p></li>
<li><p><strong>KNN Classifier</strong> showed moderate performance, with strong predictions for the “low” class but difficulties in distinguishing the “high” and “medium” classes. Its high log loss (1.450) and lower Cohen Kappa score (0.625) indicate less robust performance.</p></li>
</ol>
<p><strong>Insights Gained</strong>:<br>
The analysis highlights that all three models effectively use the selected features (“likeCount,” “duration,” “definition,” and “topicCategories”) to distinguish between “low,” “medium,” and “high” popularity videos. However, the “medium” class consistently posed challenges, with high misclassification rates across all models, likely due to feature overlap. Future work should focus on enhancing feature separability through advanced feature engineering or dimensionality reduction. Additionally, addressing class imbalance and optimizing hyperparameters further could enhance performance, particularly for the “medium” and “high” classes. Overall, the Gradient Boosting Classifier is the most reliable model for this task, combining high accuracy with well-calibrated probabilistic predictions.</p>
<hr>
</section>
</section>
</section>
<section id="key-features-in-youtube-video-popularity" class="level1">
<h1>Key Features in YouTube Video Popularity</h1>
<p>In the Multi-class Classification and Binary Classification sections, the Gradient Boosting Classifier emerged as the best-performing model. Similarly, in the Regression section, the Gradient Boosting Regressor proved to be the most effective. Therefore, we will use this model to identify the most important features influencing YouTube video popularity.</p>
<div id="cell-29" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingRegressor</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Load and preprocess data</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">"likeCount"</span>, <span class="st">"duration"</span>, <span class="st">"definition"</span>, <span class="st">"topicCategories"</span>, <span class="st">"popularity"</span>]</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[features]</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">"viewCount"</span>]</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove outliers using percentile</span></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>lower_bound <span class="op">=</span> y.quantile(<span class="fl">0.01</span>)</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>upper_bound <span class="op">=</span> y.quantile(<span class="fl">0.99</span>)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data[(y <span class="op">&gt;=</span> lower_bound) <span class="op">&amp;</span> (y <span class="op">&lt;=</span> upper_bound)]</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[features]</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">"viewCount"</span>]</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Handle missing values and ensure proper data types</span></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X.fillna(<span class="dv">0</span>)</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>X.loc[:, <span class="st">"topicCategories"</span>] <span class="op">=</span> X[<span class="st">"topicCategories"</span>].<span class="bu">map</span>(X[<span class="st">"topicCategories"</span>].value_counts()).fillna(<span class="dv">0</span>).astype(<span class="bu">int</span>)</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>X.loc[:, <span class="st">"definition"</span>] <span class="op">=</span> (X[<span class="st">"definition"</span>] <span class="op">==</span> <span class="st">"hd"</span>).astype(<span class="bu">int</span>)</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>X[<span class="st">"popularity"</span>] <span class="op">=</span> X[<span class="st">"popularity"</span>].fillna(<span class="st">"low"</span>)  <span class="co"># Fill missing values</span></span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>X[<span class="st">"popularity"</span>] <span class="op">=</span> (X[<span class="st">"popularity"</span>] <span class="op">==</span> <span class="st">"high"</span>).astype(<span class="bu">int</span>)  <span class="co"># Convert to binary</span></span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Train-test split</span></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the Gradient Boosting Regressor</span></span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>gbr <span class="op">=</span> GradientBoostingRegressor(</span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">200</span>,  <span class="co"># Chosen as a reasonable default</span></span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a>gbr.fit(X_train, y_train)</span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract and display feature importance</span></span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a>feature_importances <span class="op">=</span> gbr.feature_importances_</span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a>feature_names <span class="op">=</span> X.columns</span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame for feature importance</span></span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a>importance_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Feature'</span>: feature_names,</span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Importance'</span>: feature_importances</span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a>}).sort_values(by<span class="op">=</span><span class="st">'Importance'</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a>importance_df <span class="op">=</span> importance_df[importance_df[<span class="st">'Feature'</span>] <span class="op">!=</span> <span class="st">'popularity'</span>]</span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a>top_features <span class="op">=</span> importance_df.head(<span class="dv">3</span>)</span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-51"><a href="#cb23-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Feature Importances:"</span>)</span>
<span id="cb23-52"><a href="#cb23-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(top_features)</span>
<span id="cb23-53"><a href="#cb23-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-54"><a href="#cb23-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualization: Feature Importance</span></span>
<span id="cb23-55"><a href="#cb23-55" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb23-56"><a href="#cb23-56" aria-hidden="true" tabindex="-1"></a>plt.barh(top_features[<span class="st">'Feature'</span>], top_features[<span class="st">'Importance'</span>], color<span class="op">=</span><span class="st">'skyblue'</span>)</span>
<span id="cb23-57"><a href="#cb23-57" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Feature Importance"</span>)</span>
<span id="cb23-58"><a href="#cb23-58" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Features"</span>)</span>
<span id="cb23-59"><a href="#cb23-59" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Feature Importance"</span>)</span>
<span id="cb23-60"><a href="#cb23-60" aria-hidden="true" tabindex="-1"></a>plt.gca().invert_yaxis()  <span class="co"># Most important feature at the top</span></span>
<span id="cb23-61"><a href="#cb23-61" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb23-62"><a href="#cb23-62" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Feature Importances:
           Feature  Importance
0        likeCount    0.200364
1         duration    0.041265
3  topicCategories    0.022252</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="main_files/figure-html/cell-12-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Among the features, <strong>likeCount</strong> is the most influential, with an importance score of approximately 0.2004, indicating a strong correlation with viewership. <strong>duration</strong> has a moderate influence (0.0413), suggesting that video length contributes to predictions but is less critical than engagement metrics like likes. <strong>topicCategories</strong>, with the lowest importance (0.0222), plays a minimal role in the model, implying that the specific video topics have less predictive power for view counts. These insights suggest that engagement metrics such as likes are the strongest predictors, while other factors like duration and topic categories have a more limited impact.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-gpt4o" class="csl-entry" role="listitem">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Refined by ChatGPT, version-4o, OpenAI, 2024, chat.openai.com.</div>
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>
[
  {
    "objectID": "instructions/overview.html",
    "href": "instructions/overview.html",
    "title": "Project instruction:",
    "section": "",
    "text": "Author(s): Dr. H and Gerard Pendleton Thurston the 4th\nNote: You can delete this folder and remove it from the project website once you have read and understood the instructions. It shouldn’t be part of your final submission.\nAudio instructions:\nIf you want, you can listen to the instructions:\n\n\n\n Source: Text-to-speech conversion done with Amazon Polly on AWS \nNote: These audio instructions should not be included in your final submission or repository, once you are done wiht them, please delete the files and remove them from the website."
  },
  {
    "objectID": "instructions/overview.html#python-package-optional",
    "href": "instructions/overview.html#python-package-optional",
    "title": "Project instruction:",
    "section": "Python package (optional)",
    "text": "Python package (optional)\nThis is an optional component of the project, if you’d like, you can create a dedicated Python package for your project. The source folder for this package should be included in the root of your repository, and it can be imported into your processing scripts used in the various technical-details sections. If you create a package, it should be well-documented, with an additional tab on the navigation bar for the package documentation.\nWhile a package would typically have its own GitHub repository, for this project, please include it within the same repository.\nThe skeleton for the package is not provided in the repo, but you can recycle what you created in past assignments."
  },
  {
    "objectID": "instructions/overview.html#select-a-broad-topic-area",
    "href": "instructions/overview.html#select-a-broad-topic-area",
    "title": "Project instruction:",
    "section": "Select a broad topic area",
    "text": "Select a broad topic area\nStart by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/overview.html#narrow-your-focus",
    "href": "instructions/overview.html#narrow-your-focus",
    "title": "Project instruction:",
    "section": "Narrow your focus",
    "text": "Narrow your focus\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/overview.html#data-science-questions",
    "href": "instructions/overview.html#data-science-questions",
    "title": "Project instruction:",
    "section": "Data Science Questions",
    "text": "Data Science Questions\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/overview.html#repository-setup",
    "href": "instructions/overview.html#repository-setup",
    "title": "Project instruction:",
    "section": "Repository Setup",
    "text": "Repository Setup\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/overview.html#expectations-for-github-usage",
    "href": "instructions/overview.html#expectations-for-github-usage",
    "title": "Project instruction:",
    "section": "Expectations for GitHub Usage",
    "text": "Expectations for GitHub Usage\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n1. Use a Logical Repository Structure\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n2. Commit Regularly\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n3. Data Storage\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n4. Syncing with GU Domains\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n5. Code Documentation\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "href": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "title": "Project instruction:",
    "section": "Collaboration in Groups (If Applicable)",
    "text": "Collaboration in Groups (If Applicable)\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/overview.html#website-development",
    "href": "instructions/overview.html#website-development",
    "title": "Project instruction:",
    "section": "Website Development",
    "text": "Website Development\nIt is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/overview.html#website-hosting",
    "href": "instructions/overview.html#website-hosting",
    "title": "Project instruction:",
    "section": "Website Hosting",
    "text": "Website Hosting\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/overview.html#the-two-audiences",
    "href": "instructions/overview.html#the-two-audiences",
    "title": "Project instruction:",
    "section": "The two audiences",
    "text": "The two audiences\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/overview.html#get-started-early",
    "href": "instructions/overview.html#get-started-early",
    "title": "Project instruction:",
    "section": "Get started early",
    "text": "Get started early\nRemember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/overview.html#graduate-level-work",
    "href": "instructions/overview.html#graduate-level-work",
    "title": "Project instruction:",
    "section": "Graduate level work",
    "text": "Graduate level work\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "Project instruction:",
    "section": "The intersection of skills and domain knowledge",
    "text": "The intersection of skills and domain knowledge\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/overview.html#what-is-impact",
    "href": "instructions/overview.html#what-is-impact",
    "title": "Project instruction:",
    "section": "What is “impact”?",
    "text": "What is “impact”?\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/overview.html#what-makes-a-good-research-project",
    "href": "instructions/overview.html#what-makes-a-good-research-project",
    "title": "Project instruction:",
    "section": "What makes a good research project?",
    "text": "What makes a good research project?\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/overview.html#time-management",
    "href": "instructions/overview.html#time-management",
    "title": "Project instruction:",
    "section": "Time management",
    "text": "Time management\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/overview.html#visualization-guidelines",
    "href": "instructions/overview.html#visualization-guidelines",
    "title": "Project instruction:",
    "section": "Visualization guidelines",
    "text": "Visualization guidelines\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/overview.html#coding",
    "href": "instructions/overview.html#coding",
    "title": "Project instruction:",
    "section": "Coding",
    "text": "Coding\n\nPractice First\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\nPrototype and develop on a small data set\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/overview.html#debugging",
    "href": "instructions/overview.html#debugging",
    "title": "Project instruction:",
    "section": "Debugging",
    "text": "Debugging\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/overview.html#file-types",
    "href": "instructions/overview.html#file-types",
    "title": "Project instruction:",
    "section": "File Types",
    "text": "File Types\nYou can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/overview.html#quarto-includes",
    "href": "instructions/overview.html#quarto-includes",
    "title": "Project instruction:",
    "section": "Quarto Includes",
    "text": "Quarto Includes\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\nWhy Use Quarto Includes?\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/overview.html#citation",
    "href": "instructions/overview.html#citation",
    "title": "Project instruction:",
    "section": "Citation",
    "text": "Citation\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/overview.html#acceptable-use-cases",
    "href": "instructions/overview.html#acceptable-use-cases",
    "title": "Project instruction:",
    "section": "Acceptable use cases",
    "text": "Acceptable use cases\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/overview.html#unacceptable-use-cases",
    "href": "instructions/overview.html#unacceptable-use-cases",
    "title": "Project instruction:",
    "section": "Unacceptable use cases",
    "text": "Unacceptable use cases\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\nEffect on grade\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\nPlagiarism investigation\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "technical-details/llm-usage-log.html",
    "href": "technical-details/llm-usage-log.html",
    "title": "LLM usage log",
    "section": "",
    "text": "This page can serve as a “catch-all” for LLM use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\nLLM tools were used in the following way for the tasks below"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#brainstorming",
    "href": "technical-details/llm-usage-log.html#brainstorming",
    "title": "LLM usage log",
    "section": "Brainstorming",
    "text": "Brainstorming\n\nTo create the initial idea, LLM tools1 were used to brainstorm ideas and provide feedback and refine the project plan."
  },
  {
    "objectID": "technical-details/llm-usage-log.html#writing",
    "href": "technical-details/llm-usage-log.html#writing",
    "title": "LLM usage log",
    "section": "Writing:",
    "text": "Writing:\n\nReformating text from bulleted lists into process:\n\nWe asked ChatGPT1 to format our process log\n\nProofreading\nGrammar mistake fixation in Literature Review\nReference formatting to Bibtext"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#code",
    "href": "technical-details/llm-usage-log.html#code",
    "title": "LLM usage log",
    "section": "Code:",
    "text": "Code:\n\nCode commenting and explanatory documentation\ndoc string summarization for some of the functions."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html",
    "href": "technical-details/unsupervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "href": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/eda/instructions.html",
    "href": "technical-details/eda/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/eda/instructions.html#suggested-page-structure",
    "href": "technical-details/eda/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/eda/instructions.html#what-to-address",
    "href": "technical-details/eda/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/data-collection/methods.html",
    "href": "technical-details/data-collection/methods.html",
    "title": "Methods",
    "section": "",
    "text": "Methods\nThis section provides an overview of the methods used in this project, including key techniques, tools, and processes for data collection, processing, and analysis. Below, we outline the approach taken to achieve the project’s objectives.\n\nYouTube Data API Integration\nWe used the build function from the YouTube Data API to gather video metadata using a unique API key. This served as the foundation for querying and collecting data directly from YouTube. We included out API key in the comment, but please replace with your own API key over there.\n\n\nCustom Functions for Data Collection\n\nretrieve_randomized_videos:\n\nThis function was designed to randomly select videos from English-speaking countries, such as the United States (US), the United Kingdom (GB), and Australia (AU).\n\nA predefined list of video categories/topics was used to randomly choose specific types of videos to include.\n\nTo focus on factors driving high view counts, the chart parameter in the youtube.videos().list() function was set to \"mostPopular\".\n\nThe function returns videos that match the specified conditions, including metadata such as content details, statistics, and snippets.\n\nretrieve_200_videos:\n\nThis function builds on retrieve_randomized_videos to retrieve up to 200 videos in total.\n\nIf the data retrieved contains a nextPageToken, the function iteratively retrieves additional videos until the limit is reached or no further pages are available.\n\nget_video_details:\n\nThis function processes the JSON data returned by the YouTube API to extract key statistics and metadata.\n\nExamples of extracted data include channelTitle (from the snippet section) and viewCount (from the statistics section).\n\n\n\n\nData Summary\nAt the end of this data collection workflow, we compiled a dataset containing 13,598 rows and 15 columns, and it contains string data, numerical data, Boolean values and dates, but we won’t use the dates columns for furthere analysis. This raw dataset (saved in data/raw-data) is now ready for further preprocessing and analysis in the data cleaning stage."
  },
  {
    "objectID": "technical-details/data-collection/closing.html",
    "href": "technical-details/data-collection/closing.html",
    "title": "Summary",
    "section": "",
    "text": "we finally gatered youtube video data set with 13,598 rows and 15 columns in ‘video_id’, ‘channelTitle’, ‘title’, ‘tags’, ‘publishedAt’, ‘viewCount’, ‘likeCount’, ‘dislikeCount’, ‘favoriteCount’, ‘commentCount’, ‘duration’, ‘definition’, ‘caption’, ‘topicCategories’, ‘popularity’. We saved the dataset into csv file for future usage. In which, the dataset has regression target such as ‘viewCount’, ‘likeCount’; binary classification target such as ‘definition’, ‘caption’,‘popularity’, and multiclass-classification target such as ‘topicCategories’.\n\n\n\nDuring the data collection phase, several technical challenges were encountered. These included:\n\nAPI Limitations: The YouTube Data API’s rate limits required careful management of requests to ensure the collection process was both efficient and thorough, or we will receive HTTP error. Therefore, we include try except clasuses and ask the functions to sleep for seconds, and this step ensures the pipeline of retrieving data from API would not break in the middle.\nData Diversity: Although we attempting to create a diverse dataset, balancing content across regions, categories, and popularity levels posed a challenge. Random sampling sometimes led to imbalanced subsets that required additional filtering and adjustments.\nIncomplete Metadata: Some videos lacked complete metadata fields, such as view counts or engagement statistics, leading to gaps in the dataset. These cases required exclusion or imputation strategies.\nFuture Enhancement: As regression target, binary classification target, multiclass-classification target are required to be included, we will tune and classify the raw data in the data cleaning section to ensure the target labels exist.\n\n\n\n\nWhen comparing the collected dataset to industry benchmarks and intuitive expectations:\n\nThe collected dataset is representative of English-speaking regions, enabling targeted analysis while adhering to linguistic and cultural consistency.\nThe features included in the dataset are generally being mentioned in social media analysis papers1.\n\n\n\n\nThe data collection phase successfully generated a dataset of 13,598 rows and 15 columns, encapsulating a wide range of video metadata, including content details, engagement metrics, and categories. This dataset serves as a strong foundation for subsequent cleaning, processing, and analysis phases.\nTo improve this process further:\n\nAPI Optimization: Employ parallel requests or caching mechanisms to handle rate limits more efficiently and speed up the collection process.\nData Enrichment: Enrich the dataset with additional features, such as user demographics or sentiment analysis of comments, for richer insights.\nScalability: Expand data collection to include non-English-speaking regions or underrepresented video categories to support more generalized findings.\n\nFuture work should focus on utilizing this dataset to uncover trends in viewer engagement, analyze YouTube’s recommendation system, and provide actionable insights for content creators and marketers. These next steps will not only refine the dataset further but also help address the challenges and insights discovered during this phase."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#challenges",
    "href": "technical-details/data-collection/closing.html#challenges",
    "title": "Summary",
    "section": "",
    "text": "During the data collection phase, several technical challenges were encountered. These included:\n\nAPI Limitations: The YouTube Data API’s rate limits required careful management of requests to ensure the collection process was both efficient and thorough, or we will receive HTTP error. Therefore, we include try except clasuses and ask the functions to sleep for seconds, and this step ensures the pipeline of retrieving data from API would not break in the middle.\nData Diversity: Although we attempting to create a diverse dataset, balancing content across regions, categories, and popularity levels posed a challenge. Random sampling sometimes led to imbalanced subsets that required additional filtering and adjustments.\nIncomplete Metadata: Some videos lacked complete metadata fields, such as view counts or engagement statistics, leading to gaps in the dataset. These cases required exclusion or imputation strategies.\nFuture Enhancement: As regression target, binary classification target, multiclass-classification target are required to be included, we will tune and classify the raw data in the data cleaning section to ensure the target labels exist."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#benchmarks",
    "href": "technical-details/data-collection/closing.html#benchmarks",
    "title": "Summary",
    "section": "",
    "text": "When comparing the collected dataset to industry benchmarks and intuitive expectations:\n\nThe collected dataset is representative of English-speaking regions, enabling targeted analysis while adhering to linguistic and cultural consistency.\nThe features included in the dataset are generally being mentioned in social media analysis papers1."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "title": "Summary",
    "section": "",
    "text": "The data collection phase successfully generated a dataset of 13,598 rows and 15 columns, encapsulating a wide range of video metadata, including content details, engagement metrics, and categories. This dataset serves as a strong foundation for subsequent cleaning, processing, and analysis phases.\nTo improve this process further:\n\nAPI Optimization: Employ parallel requests or caching mechanisms to handle rate limits more efficiently and speed up the collection process.\nData Enrichment: Enrich the dataset with additional features, such as user demographics or sentiment analysis of comments, for richer insights.\nScalability: Expand data collection to include non-English-speaking regions or underrepresented video categories to support more generalized findings.\n\nFuture work should focus on utilizing this dataset to uncover trends in viewer engagement, analyze YouTube’s recommendation system, and provide actionable insights for content creators and marketers. These next steps will not only refine the dataset further but also help address the challenges and insights discovered during this phase."
  },
  {
    "objectID": "report/report.html",
    "href": "report/report.html",
    "title": "Final Report",
    "section": "",
    "text": "This report investigates the key factors that influence the popularity of YouTube videos, providing actionable insights for content creators to optimize their strategies. By analyzing video metrics such as content topics, video duration, engagement rates, and audience behavior, the findings aim to help YouTubers refine their content strategies to increase view counts and, ultimately, maximize monetary rewards through higher audience engagement and ad revenue. The insights derived from this analysis can also assist marketers and advertisers in identifying effective promotional content.\n\n\n\nThe objective of this project is to identify and analyze the key drivers of YouTube video popularity, focusing on:\n\nGathering and selecting useful variables related to YouTube videos.\nInvestigating the correlations between video variables based on visualizations.\nUnderstanding how content topics, video duration, and other metrics influence engagement and popularity.\nHelping content creators and marketers optimize their strategies to enhance video performance and audience reach.\n\n\n\n\n\n\n\n\n\nVideo Duration by Popularity\n\n\n\n\n\nVideo Duration by Topic\n\n\nWe classified popular videos with more than 6 million views and found that their views are concentrated in shorter durations (typically under 10 minutes). In contrast, less popular videos tend to vary widely in length.\n\n\n\nVideos on trending topics in the entertainment and gaming categories tend to have higher engagement rates. This is a key finding derived from our clustering analysis.\n\n\n\nObjective: Identifying patterns and structures in YouTube video data using clustering and dimensionality reduction.\n\n\n\nPCA (Principal Component Analysis) was used to reduce data dimensionality but struggled with non-linear relationships.\n\nt-SNE (t-Distributed Stochastic Neighbor Embedding) performed better in capturing complex structures, revealing clearer video clusters than PCA.\n\nBest Perplexity for t-SNE: 50, leading to the best visualization of clustered data.\n\nBest Number of PCA Components: 47, capturing ~10% variance, indicating high data complexity.\n\n\n\n\n\nK-Means clustering revealed two optimal clusters, grouping videos based on engagement levels.\n\nDBSCAN identified outlier videos, potentially viral content.\n\nAgglomerative Clustering provided more granular subcategories, differentiating niche and trending videos.\n\nBIRCH Clustering handled the large dataset efficiently, highlighting groups of videos based on shared attributes.\n\n\n\n\n\nVideo engagement and viewership trends can be categorized into distinct clusters.\n\nNonlinear relationships exist, suggesting that traditional dimensionality reduction (PCA) is insufficient.\n\nClustering models can identify viral content and niche categories, which may help content creators tailor strategies.\n\n\n\n\n\nObjective: Predicting YouTube video popularity and view counts using regression and classification models.\n\n\n\nRandom Forest, Gradient Boosting, and Ridge Regression were used to predict view counts.\n\nGradient Boosting performed well with:\n\nRMSE: 0.249\n\nR²: 0.771\n\n\nRandom Forest had better overall predictive power, capturing nonlinear interactions.\n\nFeature Importance:\n\nLike count, duration, and topic category were the most influential predictors.\n\n\n\n\n\n\nVideos were classified as high or low popularity (binary), and further categorized into high, medium, and low popularity (multi-class).\n\nBest Model: Gradient Boosting Classifier\n\nBinary Classification Metrics:\n\nAccuracy: 95.1%\n\nF1 Score: 97.2%\n\n\nMulti-Class Classification Metrics:\n\nAccuracy: 85.3%\n\nMacro F1 Score: 84.2%\n\n\n\n\n\n\n\n\n\n\nVideo View Like Comment Count Correlation\n\n\nView counts, like counts are highly correlated with each other. Subscribers, who actively choose to interact with video creators, are more likely to engage in behaviors such as liking, commenting, or saving videos. This suggests that fostering active audience engagement can help convert viewers into subscribers.\n\n\n\nThe three key features influencing YouTube video popularity are likeCount, duration, and topicCategories. LikeCount reflects audience engagement, duration represents the video’s length, and topicCategories capture the type of content, all contributing to what makes a video popular. \n\n\n\n\nContent Creators: Focus on aligning videos with trending topics, particularly in high-engagement categories like entertainment and gaming, to boost viewership. Additionally, actively encouraging likes and comments during videos can strengthen audience connections and foster long-term subscriber growth.\nAdvertisers: Target advertising efforts on high-engagement videos within popular categories to maximize visibility and ROI. Partnering with creators who consistently produce engaging content can amplify the reach and effectiveness of promotional campaigns.\n\n\n\n\n\nContent Creators:\n\n\nEncourage Engagement: Include clear and consistent call-to-actions (CTAs) within videos, encouraging viewers to like, comment, and subscribe. For example, creators can verbally remind viewers to “hit the like button if you’re enjoying this content” at strategic points in the video.\nEngaging Intros: Capture attention within the first 30 seconds by highlighting the video’s value and encouraging immediate engagement.\nIncorporate Visual Prompts: Use on-screen graphics, animations, or pop-ups during the video to subtly remind viewers to engage without interrupting the flow of content.\nLeveraging Trends: Videos that align with trending topics, especially in popular categories like entertainment and gaming, tend to attract higher audience engagement and view counts.\n\n\nAdvertisers:\n\n\nCollaborations with Engaged Creators: Partner with creators who actively foster audience interaction and have high like and comment rates.\nTarget High-Engagement Content: Focus ad placements on videos that incorporate engagement-driving strategies to ensure higher visibility and effectiveness.\n\n\n\n\nThe analysis highlights the key factors influencing YouTube video popularity, including audience engagement (likes and comments), video duration, and alignment with trending topics. These findings emphasize the importance of producing concise, engaging content and leveraging popular categories like entertainment and gaming to maximize viewership and interaction. To capitalize on these insights, content creators should focus on fostering audience engagement through consistent call-to-actions and strategic content planning. Marketers and advertisers can enhance campaign effectiveness by collaborating with high-performing creators and targeting videos with proven engagement metrics. By addressing these opportunities, creators and advertisers can drive sustained audience growth, improve engagement, and achieve greater profitability.\n\n\n\nIn our analysis, we initially classified YouTube videos into two categories based on social benchmarks and the median view count in our dataset: highly popular and low-popular videos, with the threshold set at 6 million views. The videos span a variety of topics, with some overlap in content. For instance, videos categorized as “entertainment” may also be labeled as “gaming” or “video games” at times. In the unsupervised learning section, we clustered the dimensionally reduced video data into groups using different models, including K-Means, DBSCAN, and Agglomerative Clustering. The clustering results suggest that videos can be categorized into distinct types, such as high-engagement content, niche topics, and viral videos, each reflecting specific audience behaviors. While K-Means and BIRCH identified around six clusters, Agglomerative Clustering uncovered more granular subcategories, and DBSCAN highlighted outliers, which may correspond to viral or exceptional videos. These findings provide valuable insights into how YouTube videos resonate with different audiences and the factors influencing video popularity.\nIn our analysis of video duration and popularity, we found that most highly popular videos tend to be relatively short, typically under 5000 seconds (less than one hour). Our investigation revealed that entertainment videos make up a large proportion of popular content, with their durations generally ranging from 0 to 30 minutes. When examining music videos, we observed significant variation in audience preferences based on music genre and the purpose of listening. For instance, long music videos are preferred as background noise for studying, while shorter music videos, such as those from Korean singer ROSÉ, attract a high level of engagement from viewers who enjoy short, focused content.\nWe found that the number of comments on a video is weakly correlated with view counts, like counts, and even video duration. This suggests that, while a video may be widely viewed or liked, audiences may not always feel inclined to comment. This reluctance could stem from concerns about revealing personal information or a preference for passively consuming content without engaging. However, platforms like YouTube and other video-sharing services commonly use metrics such as “Likes,” “Favorites,” and “Comments” as key indicators of interaction between creators and audiences. To benefit the video creators, our findings suggest that these platforms should consider incorporating additional variables to better evaluate the success of video content and refine their recommendation algorithms accordingly.\nAdvertisers should prioritize creators with high like counts, engaging entertainment-focused content, and shorter video durations. It is recommended to collaborate with viral creators on TikTok, as the platform’s content aligns well with these characteristics, ensuring maximum advertising effectiveness.\nIt is possible to predict the popularity trends of specific video categories to help creators gain a competitive advantage. Our analysis reveals that most viral videos are concentrated in categories such as entertainment and gaming, which consistently show high audience engagement and view counts. By monitoring real-time trends, keyword usage, and engagement patterns within these categories, creators can align their content with emerging topics and audience preferences. Leveraging predictive models and clustering analysis can further identify subcategories or niche trends, enabling creators to produce timely, relevant videos that capitalize on shifting viewer interests and maximize their reach.\nThe three key features influencing YouTube video popularity are likeCount, duration, and topicCategories, each providing critical insights for content success. LikeCount reflects audience engagement and satisfaction, with higher likes boosting visibility through YouTube’s recommendation algorithm, making it essential for creators to include clear call-to-actions (CTAs) encouraging likes. Duration, particularly shorter videos under 3 minutes, aligns with audience attention spans and performs well for entertainment or viral content, though longer videos can succeed in niches like tutorials or educational topics. Finally, topicCategories such as entertainment and gaming attract broader audiences and higher engagement, making them prime opportunities for creators to align content with trending topics and audience interests. By strategically leveraging these features—maximizing engagement, optimizing video length, and targeting popular content categories—creators can enhance visibility, audience retention, and overall video performance.1"
  },
  {
    "objectID": "report/report.html#guidelines-for-creating-a-good-narrative",
    "href": "report/report.html#guidelines-for-creating-a-good-narrative",
    "title": "Final Report",
    "section": "",
    "text": "Clear Purpose: Define the core message or objective early on.\nKnow Your Audience: Adjust language, tone, and detail based on the audience’s understanding.\nStrong Opening: Start with a hook that sets context and stakes.\nLogical Flow: Structure with a clear beginning, middle, and end.\nKey Insights: Highlight the most important points; avoid unnecessary details or jargon.\nData Support: Use data to enhance the narrative without overwhelming.\nVisuals: Incorporate charts to simplify ideas and engage the audience.\nActionable Takeaways: Conclude with recommendations or next steps.\nAuthenticity: Use storytelling to make the content engaging and relatable.\nRevise: Edit for clarity and impact, removing unnecessary content."
  },
  {
    "objectID": "report/report.html#report-content",
    "href": "report/report.html#report-content",
    "title": "Final Report",
    "section": "",
    "text": "These are just examples, you can use any structure that is suitable for your project.\n\n\n\nIntroduction: Provide an accessible overview and explain the motivation and importance of the research. Example: “This study explores how climate change affects local ecosystems, vital for wildlife conservation.”\nObjective: Clearly define the research goal and relate it to real-world challenges. Example: “We aim to analyze the effects of air pollution on public health in urban areas.”\nKey Findings: Present insights without technical terms, focusing on the impact. Example: “Air pollution increases the risk of respiratory diseases by 20%.”\nMethodology Overview: Briefly explain relevant methods. Example: “We analyzed air quality data from 50 cities and surveyed 10,000 residents.”\nVisualizations: Use simple graphs and infographics to convey findings. Example: A map showing pollution levels and a bar chart of health risks.\nSocietal Implications: Highlight the broader impact. Example: “This study highlights the need for better air quality policies.”\nCall to Action: Offer recommendations based on findings. Example: “We recommend city planners invest in green spaces.”\nConclusion: Recap the main findings and societal impact. Example: “Understanding pollution’s health impacts will help create healthier cities.”\n\n\n\n\n\nExecutive Summary: Summarize key findings and their relevance to business goals. Example: “This report predicts customer churn and offers strategies to reduce churn by 15%.”\nObjective: Define the problem or question addressed. Example: “This project aims to identify the drivers of customer churn.”\nKey Insights: Present the most important, actionable insights. Example: “Customers who interact with support twice within 30 days are 25% less likely to churn.”\nVisualizations: Use clear graphs to convey key insights. Example: A bar chart showing churn likelihood based on engagement.\nBusiness Implications: Explain how findings impact business outcomes and offer specific recommendations. Example: “Focus retention efforts on low-engagement customers.”\nRecommendations: Provide clear, actionable steps with projections. Example: “Implement an automated retention campaign, reducing churn by 10%.”\nConclusion: Summarize findings and suggest next steps. Example: “Addressing churn drivers can reduce loss and improve profitability.”\nAppendix (Optional): Additional charts or explanations for further insights."
  },
  {
    "objectID": "report/report.html#final-tips",
    "href": "report/report.html#final-tips",
    "title": "Final Report",
    "section": "",
    "text": "Simplicity: Avoid jargon; focus on business-relevant insights.\nVisual Focus: Prioritize charts and graphs over dense text.\nEmphasize Impact: Always link data insights to business outcomes."
  },
  {
    "objectID": "instructions/topic-selection.html",
    "href": "instructions/topic-selection.html",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena\n\n\n\n\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation.\n\n\n\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/topic-selection.html#select-a-broad-topic-area",
    "href": "instructions/topic-selection.html#select-a-broad-topic-area",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/topic-selection.html#narrow-your-focus",
    "href": "instructions/topic-selection.html#narrow-your-focus",
    "title": "Topic selection",
    "section": "",
    "text": "Narrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/topic-selection.html#data-science-questions",
    "href": "instructions/topic-selection.html#data-science-questions",
    "title": "Topic selection",
    "section": "",
    "text": "The data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/llm-usage.html",
    "href": "instructions/llm-usage.html",
    "title": "LLM usage",
    "section": "",
    "text": "We believe that the adoption of LLM tools is inevitable and will be an important skill for success in your future career. Therefore, appropriate and acceptable use of LLM tools is encouraged for this project. Use them to accelerate your workflow and learning, but not as a replacement for critical thinking and understanding. Carefully review and process their output, use them judiciously, and avoid bloating your text with LLM-generated content. Overusing these tools often degrades the quality of your work rather than enhancing it.\nRemember the following guidelines:\n\nUse common sense: If you feel like you’re doing something questionable, you probably are. A good test is to ask yourself, “Would I openly tell the professor or classmates what I’m doing right now?” If the answer is no, you’re probably doing something you shouldn’t.\nCite your LLM use cases: Always cite when and how you’ve used LLM tools. This is a requirement for the project.\nIs your use helping you grow professionally?: If your use of LLM tools is making you a more competent, efficient, and knowledgeable professional, you’re probably using them in an appropriate manner. If you’re using them as a shortcut to avoid work and gain free time, you’re using them incorrectly.\n\n\n\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1\n\n\n\n\n\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code.\n\n\n\n\n\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/llm-usage.html#citation",
    "href": "instructions/llm-usage.html#citation",
    "title": "LLM usage",
    "section": "",
    "text": "ALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/llm-usage.html#acceptable-use-cases",
    "href": "instructions/llm-usage.html#acceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "Note: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/llm-usage.html#unacceptable-use-cases",
    "href": "instructions/llm-usage.html#unacceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "DO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/expectations.html",
    "href": "instructions/expectations.html",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare.\n\n\n\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable.\n\n\n\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields.\n\n\n\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field.\n\n\n\n\n\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important.\n\n\n\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point.\n\n\n\n\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual.\n\n\n\n\n\n\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project .\n\n\n\n\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/expectations.html#get-started-early",
    "href": "instructions/expectations.html#get-started-early",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/expectations.html#graduate-level-work",
    "href": "instructions/expectations.html#graduate-level-work",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "This is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/expectations.html#what-is-impact",
    "href": "instructions/expectations.html#what-is-impact",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Impact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/expectations.html#what-makes-a-good-research-project",
    "href": "instructions/expectations.html#what-makes-a-good-research-project",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Professional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/expectations.html#time-management",
    "href": "instructions/expectations.html#time-management",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/expectations.html#visualization-guidelines",
    "href": "instructions/expectations.html#visualization-guidelines",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Visualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/expectations.html#coding",
    "href": "instructions/expectations.html#coding",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "While not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/expectations.html#debugging",
    "href": "instructions/expectations.html#debugging",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Always remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html",
    "href": "technical-details/supervised-learning/main.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "In this analysis, we aim to predict video view counts and classify video popularity using supervised learning models. Our predictions and classifications are based on five key features highly correlated with viewership: “like count,” “duration,” “definition,” “topic categories,” and “popularity.”\nThe report is divided into four sections: regression, binary classification, multi-class classification and the key importance features\nIn the regression section, we employ supervised learning models to predict view counts. Model performance is evaluated using metrics such as RMSE, MAE, and R². Additionally, we visualize the predicted view counts against the actual view counts to assess the models’ predictive accuracy and reliability.\nIn the binary classification section, we utilize supervised learning models to classify videos as “high” or “low” popularity based on their view counts. Videos with over 6 million views are categorized as “high popularity,” while those below 6 million are considered “low popularity.” Metrics such as accuracy, precision, recall, F1 score, and ROC AUC are used to evaluate the models’ performance.\nIn the multi-class classification section, supervised learning models are applied to classify videos into “high,” “medium,” or “low” popularity categories. Videos with over 6 million views are classified as “high popularity,” those with view counts between 1 million and 6 million as “medium popularity,” and those with fewer than 1 million views as “low popularity.” The models are assessed using metrics such as macro and weighted precision, recall, F1 score, log loss, and Cohen Kappa score to evaluate their ability to effectively handle multi-class classification tasks.\nIn the final section, we identify the most important features influencing video popularity. Using the best-performing model, we analyze and visualize feature importance, providing insights into the factors that most significantly impact viewership and popularity."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/main.html#suggested-page-structure",
    "title": "Supervised Learning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#what-to-address",
    "href": "technical-details/supervised-learning/main.html#what-to-address",
    "title": "Supervised Learning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#data-preprocessing",
    "href": "technical-details/supervised-learning/main.html#data-preprocessing",
    "title": "Supervised Learning",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-selection",
    "href": "technical-details/supervised-learning/main.html#model-selection",
    "title": "Supervised Learning",
    "section": "",
    "text": "Model Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/main.html#training-and-testing-strategy",
    "title": "Supervised Learning",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/main.html#model-evaluation-metrics",
    "title": "Supervised Learning",
    "section": "",
    "text": "Binary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#results",
    "href": "technical-details/supervised-learning/main.html#results",
    "title": "Supervised Learning",
    "section": "",
    "text": "Model Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#discussion",
    "href": "technical-details/supervised-learning/main.html#discussion",
    "title": "Supervised Learning",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/data-collection/main.html",
    "href": "technical-details/data-collection/main.html",
    "title": "Data Collection",
    "section": "",
    "text": "This section provides a high-level summary for technical staff, outlining the key tasks and processes undertaken in this project. It establishes the context, motivation, and objectives of the work.\n\nGoals:\nThe primary goal of this document is to efficiently collect data from the YouTube API to build a robust dataset for addressing future data science research questions. This involves designing a reliable workflow for data acquisition, using Youtube Data API-v3 calls to gather video metadata, and curating a dataset of sufficient size and quality.\nMotivation:\nThe motivation behind this data collection effort is to source original and reliable data directly from the YouTube API. To ensure the dataset is representative and meaningful, we aim to diversify the collected data by selecting subsets of videos from different countries while limiting topics to reduce noise and ambiguity. This approach allows us to create a focused, high-quality dataset that reflects diverse content while maintaining relevance to our research goals.\nObjectives:\nThe specific objectives for data collection are as follows:\n\nRandomly retrieve video metadata using the YouTube Data API.\n\nSelect useful video features, such as viewing statistics, to ensure the data supports comprehensive analysis.\n\nInvestigate YouTube’s recommendation mechanism by retrieving the linked “next page” videos associated with each video.\n\nFocus on videos from English-speaking countries (e.g., the United States and the United Kingdom) to maintain consistency in language and cultural context.\n\n\nThis overview provides technical staff with the necessary context to understand the importance of the work and guides them in exploring the details presented in subsequent sections."
  },
  {
    "objectID": "technical-details/data-collection/main.html#challenges",
    "href": "technical-details/data-collection/main.html#challenges",
    "title": "Data Collection",
    "section": "Challenges",
    "text": "Challenges\nDuring the data collection phase, several technical challenges were encountered. These included:\n\nAPI Limitations: The YouTube Data API’s rate limits required careful management of requests to ensure the collection process was both efficient and thorough, or we will receive HTTP error. Therefore, we include try except clasuses and ask the functions to sleep for seconds, and this step ensures the pipeline of retrieving data from API would not break in the middle.\nData Diversity: Although we attempting to create a diverse dataset, balancing content across regions, categories, and popularity levels posed a challenge. Random sampling sometimes led to imbalanced subsets that required additional filtering and adjustments.\nIncomplete Metadata: Some videos lacked complete metadata fields, such as view counts or engagement statistics, leading to gaps in the dataset. These cases required exclusion or imputation strategies.\nFuture Enhancement: As regression target, binary classification target, multiclass-classification target are required to be included, we will tune and classify the raw data in the data cleaning section to ensure the target labels exist."
  },
  {
    "objectID": "technical-details/data-collection/main.html#benchmarks",
    "href": "technical-details/data-collection/main.html#benchmarks",
    "title": "Data Collection",
    "section": "Benchmarks",
    "text": "Benchmarks\nWhen comparing the collected dataset to industry benchmarks and intuitive expectations:\n\nThe collected dataset is representative of English-speaking regions, enabling targeted analysis while adhering to linguistic and cultural consistency.\nThe features included in the dataset are generally being mentioned in social media analysis papers1."
  },
  {
    "objectID": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "title": "Data Collection",
    "section": "Conclusion and Future Steps",
    "text": "Conclusion and Future Steps\nThe data collection phase successfully generated a dataset of 13,598 rows and 15 columns, encapsulating a wide range of video metadata, including content details, engagement metrics, and categories. This dataset serves as a strong foundation for subsequent cleaning, processing, and analysis phases.\nTo improve this process further:\n\nAPI Optimization: Employ parallel requests or caching mechanisms to handle rate limits more efficiently and speed up the collection process.\nData Enrichment: Enrich the dataset with additional features, such as user demographics or sentiment analysis of comments, for richer insights.\nScalability: Expand data collection to include non-English-speaking regions or underrepresented video categories to support more generalized findings.\n\nFuture work should focus on utilizing this dataset to uncover trends in viewer engagement, analyze YouTube’s recommendation system, and provide actionable insights for content creators and marketers. These next steps will not only refine the dataset further but also help address the challenges and insights discovered during this phase."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html",
    "href": "technical-details/data-cleaning/main.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "This section provides a high-level summary for technical staff, outlining the key tasks and processes undertaken in this project. It establishes the context, motivation, and objectives of the work.\n\nGoals:\nThis document focuses on cleaning and preparing a dataset generated from YouTube API data, ensuring it is accurate, consistent, and suitable for analysis. It includes handling missing values, resolving inconsistencies, and transforming raw features for effective use in supervised-learning and unsupervised-learning tasks.\nMotivation:\nThe primary motivation behind this project is to leverage data collected from the YouTube API to better understand and predict video performance, popularity, and trends. By analyzing and refining the dataset, the goal is to develop reliable models for regression and classification tasks, providing actionable insights into what drives video success on the platform. This effort aims to bridge the gap between raw data and meaningful predictions\nObjectives:\n\nHandle Missing and Erroneous Data\nRemove punctuation and unnecessary symbols from text fields\nExtract meaningful topics from the topicCategories field for better analysis\nConvert duration into seconds for numerical consistency\nNormalize numerical fields\nClassify videos into multi-class (low, medium, high)and binary (low, high) popularity categories based on viewCount\nRemove duplicate entries\nSave Processed Data"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#suggested-page-structure",
    "href": "technical-details/data-cleaning/main.html#suggested-page-structure",
    "title": "Data Cleaning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#general-comments",
    "href": "technical-details/data-cleaning/main.html#general-comments",
    "title": "Data Cleaning",
    "section": "",
    "text": "Iterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#what-to-address",
    "href": "technical-details/data-cleaning/main.html#what-to-address",
    "title": "Data Cleaning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/eda/main.html",
    "href": "technical-details/eda/main.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The primary objective of this Exploratory Data Analysis (EDA) is to explore and identify the key factors that influence YouTube audience engagement. Through a comprehensive analysis, we aim to uncover patterns and trends that drive audience behavior on the platform."
  },
  {
    "objectID": "technical-details/eda/main.html#suggested-page-structure",
    "href": "technical-details/eda/main.html#suggested-page-structure",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/eda/main.html#what-to-address",
    "href": "technical-details/eda/main.html#what-to-address",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/eda/main.html#import-cleaned-data",
    "href": "technical-details/eda/main.html#import-cleaned-data",
    "title": "Exploratory Data Analysis",
    "section": "Import cleaned data",
    "text": "Import cleaned data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import chi2_contingency\nimport scipy.stats as stats\n\n\ndata = pd.read_csv(\"../../data/processed-data/cleaned_data.csv\")\ndata = data.drop(columns=[\"Unnamed: 0\"]) if \"Unnamed: 0\" in data.columns else data # remove the unnamed column\n\n\n# Display basic information about the dataset\nprint(\"Dataset Info:\")\nprint(data.info())\n\nprint(\"\\nFirst Few Rows:\")\ndisplay(data.head())\n\nprint(\"\\nBasic Statistics:\")\ndisplay(data.describe())\n\n# Check for missing values\nprint(\"\\nMissing Values:\")\nprint(data.isnull().sum())\n\nDataset Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2208 entries, 0 to 2207\nData columns (total 16 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   video_id                2208 non-null   object \n 1   channelTitle            2208 non-null   object \n 2   title                   2208 non-null   object \n 3   tags                    1156 non-null   object \n 4   publishedAt             2208 non-null   object \n 5   viewCount               2208 non-null   float64\n 6   likeCount               2153 non-null   float64\n 7   dislikeCount            0 non-null      float64\n 8   favoriteCount           2208 non-null   int64  \n 9   commentCount            2184 non-null   float64\n 10  duration                2208 non-null   int64  \n 11  definition              2208 non-null   object \n 12  caption                 2208 non-null   bool   \n 13  topicCategories         2202 non-null   object \n 14  popularity              2208 non-null   object \n 15  popularity_multi_class  2208 non-null   object \ndtypes: bool(1), float64(4), int64(2), object(9)\nmemory usage: 261.0+ KB\nNone\n\nFirst Few Rows:\n\n\n\n\n\n\n\n\n\nvideo_id\nchannelTitle\ntitle\ntags\npublishedAt\nviewCount\nlikeCount\ndislikeCount\nfavoriteCount\ncommentCount\nduration\ndefinition\ncaption\ntopicCategories\npopularity\npopularity_multi_class\n\n\n\n\n0\nb6FTKe-u8XI\nRichard Restatement\nCan I get scout badgetherookie viralvideo fory...\nNaN\n2024-11-20T22:42:00Z\n0.049597\n0.139295\nNaN\n0\n0.009236\n59\nhd\nFalse\nEntertainment Film\nhigh\nhigh\n\n\n1\nW_19sjpQttw\nSuccesful Celebrity\nBill Nye The Science Guy Shows Kai Cenat A Sci...\nkai cenat kai cenat imkaicenat amp kai cenat l...\n2024-11-17T17:01:47Z\n0.082768\n0.193623\nNaN\n0\n0.033605\n26\nhd\nFalse\nEntertainment Film Television_program\nhigh\nhigh\n\n\n2\nudVz8GN_DP0\nKieran Ta\nYoure not gonna hit me again are you supernatu...\nNaN\n2024-11-18T03:13:34Z\n0.045601\n0.105196\nNaN\n0\n0.007719\n58\nhd\nFalse\nEntertainment Film Television_program\nhigh\nhigh\n\n\n3\niOecn1gSBZY\nM2M Heart\nOpen the cash with moneyThis is funniest robbe...\nNaN\n2024-11-16T13:00:53Z\n0.070301\n0.179141\nNaN\n0\n0.013535\n59\nhd\nFalse\nEntertainment Film\nhigh\nhigh\n\n\n4\nu1IuChCXplQ\n9-1-1 house\nHe realized that they did not stop death\nNaN\n2024-11-20T19:52:08Z\n0.032618\n0.069746\nNaN\n0\n0.007707\n60\nhd\nFalse\nEntertainment Film\nhigh\nhigh\n\n\n\n\n\n\n\n\nBasic Statistics:\n\n\n\n\n\n\n\n\n\nviewCount\nlikeCount\ndislikeCount\nfavoriteCount\ncommentCount\nduration\n\n\n\n\ncount\n2208.000000\n2153.000000\n0.0\n2208.0\n2184.000000\n2208.000000\n\n\nmean\n0.017055\n0.026817\nNaN\n0.0\n0.014774\n929.403080\n\n\nstd\n0.041650\n0.057310\nNaN\n0.0\n0.044780\n2790.428613\n\n\nmin\n0.000000\n0.000000\nNaN\n0.0\n0.000000\n10.000000\n\n\n25%\n0.001437\n0.001700\nNaN\n0.0\n0.002301\n34.750000\n\n\n50%\n0.005317\n0.006941\nNaN\n0.0\n0.005831\n60.000000\n\n\n75%\n0.017203\n0.028407\nNaN\n0.0\n0.014094\n921.500000\n\n\nmax\n1.000000\n1.000000\nNaN\n0.0\n0.999957\n42901.000000\n\n\n\n\n\n\n\n\nMissing Values:\nvideo_id                     0\nchannelTitle                 0\ntitle                        0\ntags                      1052\npublishedAt                  0\nviewCount                    0\nlikeCount                   55\ndislikeCount              2208\nfavoriteCount                0\ncommentCount                24\nduration                     0\ndefinition                   0\ncaption                      0\ntopicCategories              6\npopularity                   0\npopularity_multi_class       0\ndtype: int64"
  },
  {
    "objectID": "technical-details/eda/main.html#eda",
    "href": "technical-details/eda/main.html#eda",
    "title": "Exploratory Data Analysis",
    "section": "EDA",
    "text": "EDA\n\nBivariate Analysis\n\nNumerical Variables:\n\nProvide summary statistics\nAnalyze distribution of two features : Histogrm of Caption Inclusion by Popularity\n\n\n\ngrouped_data = data.groupby(['caption', 'popularity']).size().unstack(fill_value=0)\n\n# Plot stacked bar chart\ngrouped_data.plot(\n    kind='bar',\n    stacked=True,\n    figsize=(8, 5),\n    colormap='viridis',  # Choose a colormap for encoding\n    alpha=0.7\n)\n\n# Chart details\nplt.title(\"Stacked Bar Chart of Caption Values by Popularity\")\nplt.xlabel(\"Caption Values\")\nplt.ylabel(\"Frequency\")\nplt.xticks(rotation=45)\nplt.legend(title='Popularity')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\nplt.show()\n\n\n\n\n\n\n\n\nFrom this plot, we can see that most of youtube videos do not include captions for audiences. This plot is a stacked bar chart that represents the frequency of caption values categorized by popularity (“high” and “low”, high popularity means the video has more than 6000000 views). The bars are segmented by the popularity levels, with distinct colors assigned to each category (“high” in purple and “low” in yellow). This chart effectively highlights the distribution of popularity levels across different caption values, and it suggest that embedding caption or not would not lead to increasing popularity of videos.\n\nNumerical Variables:\n\nAnalyze distribution of two features : Scatter Plot of Video Duration by Popularity\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Scatter plot for duration vs popularity\nplt.figure(figsize=(8, 5))\nsns.stripplot(x='popularity', y='duration', data=data, jitter=True, alpha=0.6, palette='Set2')\nplt.title(\"Scatter Plot of Video Duration by Popularity\")\nplt.xlabel(\"Popularity\")\nplt.ylabel(\"Video Duration (seconds)\")\nplt.show()\n\n/var/folders/0k/58z97my108j9p_yv9d2xvzxr0000gn/T/ipykernel_25476/1976354669.py:6: FutureWarning: Passing `palette` without assigning `hue` is deprecated.\n  sns.stripplot(x='popularity', y='duration', data=data, jitter=True, alpha=0.6, palette='Set2')\n/opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nThis scatter plot visualizes the relationship between video duration (in seconds) and popularity levels (“high” and “low”). Videos with “low” popularity show a wider range of durations, and even with very large outliers representing very long videos. In contrast, “high” popularity videos tend to cluster at shorter durations, suggesting that creating and watching shorter videos is a social trend. This trend could indicate that audiences prefer concise content, or it might reflect platform-specific optimization for shorter videos.\n\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions: Topic categories pie chart\n\n\ndata['topicCategories'] = data['topicCategories'].fillna('')  # Handle NaN values\nall_categories = data['topicCategories'].str.split(' ').explode()  # Split and flatten\n\n# Count frequency of each category\ncategory_counts = all_categories.value_counts()\n\ntop_20_categories = category_counts.head(15)\n    \nplt.figure(figsize=(10, 10))\nplt.pie(\n    top_20_categories.values, \n    labels=None, \n    autopct='%1.1f%%', \n    startangle=90, \n    colors=plt.cm.Pastel1.colors\n)\n\nlabels = [f\"{label} ({value})\" for label, value in zip(top_20_categories.index, top_20_categories.values)]\nplt.legend(\n    labels,\n    loc=\"center left\",\n    bbox_to_anchor=(1.05, 0.5),  # Adjust position of the legend\n    title=\"Top Categories\"\n)\n\nplt.title(\"Pie Chart: Top 15 Individual Topic Categories\")\nplt.show()\n\n\n\n\n\n\n\n\nMost of the videos we randomly selected from YouTube globally fall under the topics of Entertainment, with Video Game and Video Game Culture videos ranking as the top two categories. Among the top 15 topics selected for our analysis, we found that five of them are related to gaming. This suggests that there is a significant audience interest in game-related content on YouTube, highlighting the popularity of entertainment and gaming among viewers.\n\npopular_data = data[data[\"popularity\"]==\"high\"]\npopular_data['topicCategories'] = popular_data['topicCategories'].fillna('')  # Handle NaN values\nall_categories = popular_data['topicCategories'].str.split(' ').explode()  # Split and flatten\n\n# Count frequency of each category\ncategory_counts = all_categories.value_counts()\n\ntop_20_categories = category_counts.head(15)\n    \nplt.figure(figsize=(8,10))\nplt.pie(\n    top_20_categories.values, \n    labels=None, \n    autopct='%1.1f%%', \n    startangle=180, \n    colors=plt.cm.Pastel1.colors\n)\n\nlabels = [f\"{label} ({value})\" for label, value in zip(top_20_categories.index, top_20_categories.values)]\nplt.legend(\n    labels,\n    loc=\"center left\",\n    bbox_to_anchor=(1.05, 0.5),  # Adjust position of the legend\n    title=\"Top Categories\"\n)\n\nplt.title(\"Pie Chart: Top 15 Individual Topic Categories\")\nplt.show()\n\n/var/folders/0k/58z97my108j9p_yv9d2xvzxr0000gn/T/ipykernel_25476/1985519554.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  popular_data['topicCategories'] = popular_data['topicCategories'].fillna('')  # Handle NaN values\n\n\n\n\n\n\n\n\n\nThis pie chart visualize the distribution of the top 15 individual topic categories from the dataset, highlighting their respective proportions. The “Entertainment” category dominates with 30%, followed by “Film” (15%), “Sport” (13%), and “Vehicle” (10%). The remaining categories, including “Lifestyle (Sociology)” (8%), “Television Program” (3.6%), and others, represent smaller shares of the dataset. This visualization effectively emphasizes the prevalence of entertainment-related content while showing the diversity of other topics.\n\n# Filter high-popularity videos and calculate duration stats per topic\npivot_melted = (\n    data[data[\"popularity\"] == \"high\"]\n    .groupby('topicCategories')['duration']\n    .agg(['min', 'max', 'count'])\n    .reset_index()\n    .sort_values(by='count', ascending=False)\n    .rename(columns={'topicCategories': 'Topic', 'min': 'Min Duration', 'max': 'Max Duration', 'count': 'Video Count'})\n)\n\n# Split topics into individual words and explode them into rows\npivot_melted = (\n    pivot_melted\n    .assign(Melted_Topic=pivot_melted['Topic'].str.split(' '))\n    .explode('Melted_Topic')\n    .drop(columns='Topic')\n    .rename(columns={'Melted_Topic': 'Topic'})\n)\n\n# Combine duplicate topics and aggregate statistics\npivot_melted = (\n    pivot_melted\n    .groupby('Topic', as_index=False)\n    .agg({\n        'Min Duration': 'min',\n        'Max Duration': 'max',\n        'Video Count': 'sum'\n    })\n    .sort_values(by='Video Count', ascending=False)\n)\n\n# Display the final result\ndisplay(pivot_melted.head(15))\n\n\n\n\n\n\n\n\nTopic\nMin Duration\nMax Duration\nVideo Count\n\n\n\n\n8\nEntertainment\n10\n175\n179\n\n\n9\nFilm\n14\n175\n90\n\n\n35\nSport\n10\n1452\n75\n\n\n39\nVehicle\n10\n61\n61\n\n\n18\nLifestyle_(sociology)\n10\n1521\n49\n\n\n38\nTelevision_program\n17\n42896\n21\n\n\n37\nTechnology\n14\n1521\n21\n\n\n15\nHumour\n10\n175\n19\n\n\n28\nPop_music\n13\n302\n14\n\n\n2\nAssociation_football\n10\n840\n14\n\n\n5\nBoxing\n10\n59\n11\n\n\n22\nMusic\n61\n302\n11\n\n\n4\nBasketball\n14\n1452\n7\n\n\n40\nVideo_game_culture\n17\n151\n7\n\n\n33\nSociety\n30\n42896\n6\n\n\n\n\n\n\n\nThe output of this code provides a summarized table of YouTube video topics with high popularity.\n\nTopic Names: Distinct keywords extracted from the original topic categories (e.g., “Entertainment,” “Film”).\nDuration Range: Minimum and maximum video durations for each topic.\nVideo Count: The total number of videos in each topic.\n\nThe results are sorted by video count in descending order, highlighting the most frequent topics and their duration patterns. This helps identify dominant video themes and the typical duration ranges associated with popular videos.\n\n\nMultivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using Pearson Correlation matrix and Spearman Correlation matrix\nAnalyze relationships between numerical variables using a correlation matrix: Correlation Heatmap plot\n\n\nprint(\"----------------------\")\nprint(\"SPEARMAN CORRELATION MATRIX:\")\nprint(\"----------------------\")\ndisplay(data.corr(method='spearman',numeric_only=True).fillna(0))\n\n----------------------\nSPEARMAN CORRELATION MATRIX:\n----------------------\n\n\n\n\n\n\n\n\n\nviewCount\nlikeCount\ndislikeCount\nfavoriteCount\ncommentCount\nduration\ncaption\n\n\n\n\nviewCount\n1.000000\n0.933141\n0.0\n0.0\n0.337583\n-0.571253\n-0.081384\n\n\nlikeCount\n0.933141\n1.000000\n0.0\n0.0\n0.335939\n-0.569777\n-0.121677\n\n\ndislikeCount\n0.000000\n0.000000\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n\n\nfavoriteCount\n0.000000\n0.000000\n0.0\n0.0\n0.000000\n0.000000\n0.000000\n\n\ncommentCount\n0.337583\n0.335939\n0.0\n0.0\n1.000000\n0.090784\n0.131055\n\n\nduration\n-0.571253\n-0.569777\n0.0\n0.0\n0.090784\n1.000000\n0.168698\n\n\ncaption\n-0.081384\n-0.121677\n0.0\n0.0\n0.131055\n0.168698\n1.000000\n\n\n\n\n\n\n\nFrom this top-level visualization of correlation between features, we can see that view counts are highly correlated with like counts; view counts or like counts are not closely correlated with comment counts; view counts and like counts are moderately negatively correlated with duration. The trends suggest that YouTube audiences in general prefer shorter videos than longer ones, and they may not comment on vidoes even they view or like the videos.\n\nnumerical_features = data[['viewCount', 'likeCount', 'commentCount']]\ncorrelation_matrix = numerical_features.corr()\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True)\nplt.title('Correlation Heatmap Across Numerical Features', fontsize=16)\nplt.show()\n\n\n\n\n\n\n\n\nThe heatmap suggest there is a higher correlation between like counts and view counts than the correlation between view counts and comment counts.\n\n\nCross-tabulations:\n\nFor categorical variables, use crosstabs to explore relationships: cross tabulation of video definition(high definition / standard definition) and caption embedding\n\n\n\ncrosstab_result = pd.crosstab(data['definition'], data['caption'])\nprint(crosstab_result)\nplt.figure(figsize=(8, 6))\nsns.heatmap(crosstab_result, annot=True, cmap='Blues', fmt=\"d\")\nplt.title(\"Cross-Tabulation Heatmap\")\nplt.ylabel(\"Definition\")\nplt.xlabel(\"Caption\")\nplt.show()\n\ncaption     False  True \ndefinition              \nhd           2074    130\nsd              3      1\n\n\n\n\n\n\n\n\n\nThe cross-tabulation plot shows most of the videos are in high definition quality and without caption.\n\n\nFeature Pairings:\n\nAnalyze relationships between key variables: Scatter plot of View Counts and Duration\n\n\nplt.figure(figsize=(10, 6))\nplt.scatter(data['viewCount'], data['duration'], color='blue', alpha=0.7)\n\nplt.title('Scatter Plot of View Count vs. Video Duration', fontsize=16)\nplt.xlabel('View Count', fontsize=12)\nplt.ylabel('Video Duration(s)', fontsize=12)\nplt.grid(True, linestyle='--', alpha=0.7)\n\nplt.show()\n\n\n\n\n\n\n\n\nFrom the plot, we can observe a negative relationship between video duration and view count. Specifically, videos with shorter durations tend to have higher view counts. This suggests that audiences prefer shorter videos in the current media landscape, possibly due to the ease of consumption and the trend towards quick, engaging content. The high view counts for shorter videos may also reflect a growing trend of continuous viewing, where users are more likely to watch a series of short videos in a single sitting. This trend aligns with the popularity of short-form video platforms (e.g., TikTok, YouTube Shorts), where users tend to engage with bite-sized content in a loop, contributing to higher overall engagement.\n\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis::\n\nAnalyze and discuss the distribution of variables.: Distribution of view counts and like counts\n\n\n\nviewcounts_df = data[['viewCount']]\nviewcounts_df = viewcounts_df[viewcounts_df['viewCount'] != 0]\n\nplt.hist(viewcounts_df[\"viewCount\"], bins=50)\nplt.title('Histogram of View Count')\nplt.xlabel('View Count (millions)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\n\nlikecounts_df = data[['likeCount']]\n\nplt.hist(likecounts_df[\"likeCount\"], bins=50)\nplt.title('Histogram of Like Count')\n\nplt.xlabel('Like Count (millions)')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: The histogram of like counts and view counts are highly right skewed and include very large outliers according to the distributions.\n\nskewness = data['viewCount'].skew()\nkurtosis = data['viewCount'].kurtosis()\n\nprint(f\"Skewness of view counts: {skewness}\")\nprint(f\"Kurtosis of view counts: {kurtosis}\")\n\nSkewness of view counts: 10.672893845120713\nKurtosis of view counts: 186.37037135267474\n\n\n\nSkewness: The skewness value indicates a strong positive skew. This means the distribution of view counts is heavily right-skewed, with a long tail on the right side. Most of the videos have lower view counts, but there are a few videos with extremely high view counts, pulling the average upward.\nKurtosis: The kurtosis value is extremely high, suggesting that the distribution has heavy tails and a sharp peak. This indicates that there are many extreme outliers (i.e., videos with exceptionally high view counts) in your data. These outliers significantly affect the overall distribution, making it more “peaked” compared to a normal distribution, which has a kurtosis of 3.\n\n\nskewness = data['likeCount'].skew()\nkurtosis = data['likeCount'].kurtosis()\n\nprint(f\"Skewness of like counts: {skewness}\")\nprint(f\"Kurtosis of like counts: {kurtosis}\")\n\nSkewness of like counts: 6.4863537512329135\nKurtosis of like counts: 68.364500479544\n\n\n\nBoth view counts and like counts exhibit positive skewness, meaning there are a few items with very high values. However, view counts are more extremely skewed than like counts, indicating that there are more videos with significantly higher views compared to likes. This suggests that view counts have a wider disparity, with certain videos getting far more views than likes.\nBoth features exhibit high kurtosis, indicating heavy tails and outliers. However, the view count distribution is much more peaked and has more extreme outliers than like counts. This means that while both view counts and like counts have some extreme values, the view counts have far more extreme outliers with a sharper peak, whereas the like counts have somewhat fewer but still significant outliers.\n\nIn summary, the kurtosis value and skewness value suggest that although some videso gained high view counts, people just slided through the videos without hitting the like button.\n\n\nStatistical Insights:\nHypotheses for Chi-square tests:\n\nNull Hypothesis (H₀): There is no significant association between the viewCount and likeCount.\nAlternative Hypothesis (H₁): There is a significant association between the viewCount and likeCount.\n\n\n# Create a contingency table\ncontingency_table = pd.crosstab(data['viewCount'], data['likeCount'])\n\n# Perform chi-square test\nchi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n\nprint(f\"P-value: {p_value}\")\n\nP-value: 0.2409455758561337\n\n\nAs the result shows, we can see that the p-value is greater than 0.05, which means we fail to reject the null hypothesis that there is no significant association between viewCount and likeCount. Therefore, viewCount and likeCount have a significant association.\nHypotheses for Independent T-test: - Null Hypothesis (H₀): There is no significant difference between the mean like count of high definition videos and standard definition videos. - Alternative Hypothesis (H₁): There is a significant difference between the mean like count of high definition videos and standard definition videos.\n\nlike_count_hd = data['likeCount'][data[\"definition\"] == \"hd\"].dropna() # like counts of videos in high definition\nlike_count_sd = data['commentCount'][data[\"definition\"] == \"sd\"].dropna()  # like counts of videos in standard definition\n\n# Perform an independent T-test (equal variance by default)\nt_statistic, p_value = stats.ttest_ind(like_count_hd, like_count_sd)\n\n# Display the result\nprint(f\"T-statistic: {t_statistic}\")\nprint(f\"P-value: {p_value}\")\n\n# Interpreting the result\nalpha = 0.05  # Common significance level\nif p_value &lt; alpha:\n    print(\"Reject the null hypothesis: There is a significant difference between the means.\")\nelse:\n    print(\"Fail to reject the null hypothesis: There is no significant difference between the means.\")\n\nT-statistic: -4.371805625489004\nP-value: 1.2909463665668727e-05\nReject the null hypothesis: There is a significant difference between the means.\n\n\nAs the result shows, we can see that the p-value is less than 0.05, which means we should reject the null hypothesis, and there is significant association between the mean like count of high definition videos and standard definition videos. And audiences have preference on definition of videos."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html",
    "href": "technical-details/unsupervised-learning/main.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "In this analysis, we aim to identify anomalous trends hidden in the dataset and apply different clustering and dimensionality reduction techniques to our cleaned YouTube video dataset, which includes both textual(topics of video, tages of video, or topic of video) and numerical data(view counts of video, like counts of video, etc.). We calculated the average engagement rate per video by subscribers on YouTube is calculated as the total engagement (likes, comments, and dislikes) divided by the number of videos the profile published1. The result is then divided by the number of subscribers, and all multiplied by 100. The goal is to gain insights into the underlying structure of the dataset by reducing its high dimensionality and identifying meaningful patterns. We will apply methods such as PCA, t-SNE, K-Means, DBSCAN, and Agglomerative Clustering to understand how each technique performs in capturing the data’s inherent structure. By tuning the hyperparameters and evaluating model performance, we seek to uncover hidden relationships in the data and visualize the results for easier interpretation. This analysis will help us assess the suitability of each method for this particular dataset, guiding future decisions for clustering and dimensionality reduction in more complex datasets."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#part-1-dimensionality-reduction",
    "href": "technical-details/unsupervised-learning/main.html#part-1-dimensionality-reduction",
    "title": "Unsupervised Learning",
    "section": "Part 1: Dimensionality Reduction",
    "text": "Part 1: Dimensionality Reduction\n\nPCA (Principal Component Analysis):\nt-SNE (t-distributed Stochastic Neighbor Embedding):\nEvaluation and Comparison:\n\n\ndef dimensionality_reduction(X, method=\"pca\"):\n    \"\"\"Dimensionality reduction with PCA or t-SNE, includes hyperparameter tuning.\n\n    Raises:\n        ValueError: if providing unsupported dimension reduction method name\n\n    Returns:\n        _type_: data has dimension reduced\n    \"\"\"\n    if method.lower() == \"pca\":\n        best_n_component = 2\n        best_var = 0\n        print(\"Tuning PCA number of components...\")\n\n        for n in range(2, 50, 5):\n            pca = PCA(n_components=n)\n            X_pca = pca.fit_transform(X)\n            exp_var = np.cumsum(pca.explained_variance_ratio_)\n            print(f\"Number of componenets: {n}, Explained Variance Ratio: {exp_var[-1]}\")\n            if exp_var[-1] &gt; best_var:  # Compare the last cumulative variance\n                best_n_component = n\n                best_var = exp_var[-1]  # Update best variance\n\n        pca = PCA(n_components=best_n_component)\n        X_pca = pca.fit_transform(X)\n        print(f\"Best number of components: {best_n_component}, Explained Variance Ratio: {best_var}\")\n        return X_pca\n\n    elif method.lower() == \"tsne\":\n        best_perplexity = None\n        best_kl_divergence = float('inf')\n        tuned_tsne = None\n\n        print(\"Tuning t-SNE perplexity...\")\n        for perplexity in [5, 10, 30, 50]:\n            tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n            X_transformed = tsne.fit_transform(X)\n            kl_divergence = tsne.kl_divergence_\n            print(f\"Perplexity: {perplexity}, KL Divergence: {kl_divergence}\")\n            if kl_divergence &lt; best_kl_divergence:\n                best_kl_divergence = kl_divergence\n                best_perplexity = perplexity\n                tuned_tsne = X_transformed\n\n        print(f\"Best Perplexity: {best_perplexity}, Best KL Divergence: {best_kl_divergence}\")\n        return tuned_tsne\n\n    else:\n        raise ValueError(\"Unsupported dimensionality reduction method.\")\n\n\nx_raw_pca = dimensionality_reduction(combined_features, method=\"PCA\")\nx_raw_tsne = dimensionality_reduction(combined_features, method=\"TSNE\")\n\nTuning PCA number of components...\nNumber of componenets: 2, Explained Variance Ratio: 0.007621356137330164\nNumber of componenets: 7, Explained Variance Ratio: 0.023568496533213433\nNumber of componenets: 12, Explained Variance Ratio: 0.03691464799856191\nNumber of componenets: 17, Explained Variance Ratio: 0.04850314466749101\nNumber of componenets: 22, Explained Variance Ratio: 0.059166877699340335\nNumber of componenets: 27, Explained Variance Ratio: 0.06940027107590181\nNumber of componenets: 32, Explained Variance Ratio: 0.07933031287387941\nNumber of componenets: 37, Explained Variance Ratio: 0.08902957786542476\nNumber of componenets: 42, Explained Variance Ratio: 0.0984258666124817\nNumber of componenets: 47, Explained Variance Ratio: 0.10750359448628671\nBest number of components: 47, Explained Variance Ratio: 0.10750359448628671\nTuning t-SNE perplexity...\nPerplexity: 5, KL Divergence: 5.56992769241333\nPerplexity: 10, KL Divergence: 4.418832302093506\nPerplexity: 30, KL Divergence: 3.3578200340270996\nPerplexity: 50, KL Divergence: 2.8662688732147217\nBest Perplexity: 50, Best KL Divergence: 2.8662688732147217\n\n\n\n# Create a figure with two subplots side by side\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))  # 1 row, 2 columns\n\n# First scatter plot\nax1.scatter(x_raw_pca[:,0], x_raw_pca[:,1], color='blue')  \nax1.set(xlabel='Dimension1', ylabel='Dimension2')\nax1.grid()\nax1.set_title('PCA')\n\n# Second scatter plot\nax2.scatter(x_raw_tsne[:,0], x_raw_tsne[:,1], color='green')  \nax2.set(xlabel='Dimension1', ylabel='Dimension2')\nax2.grid()\nax2.set_title('TSNE')\n\n# Adjust layout to avoid overlap\nplt.tight_layout()\n\n# Show the plots\nplt.show()\n\n\n\n\n\n\n\n\nBy placing the scatter plots of PCA and t-SNE side by side, we can see that the PCA scatter plot only shows three clusters visiblely. The reason for this is the linear nature of PCA, which assumes that the data’s primary patterns can be captured by linear combinations of the features.\nThis finding aligns with our assumption: if the data contains nonlinear relationships, PCA may not fully represent its structure, leading to a less informative visualization. Additionally, feature scaling could affect the outcome. In contrast, t-SNE excels at capturing nonlinear relationships, which is why its visualization shows a more meaningful, circular pattern. The flexibility of t-SNE allows it to reveal complex clustering structures that PCA might miss."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#part-2-clustering-methods",
    "href": "technical-details/unsupervised-learning/main.html#part-2-clustering-methods",
    "title": "Unsupervised Learning",
    "section": "Part 2: Clustering Methods",
    "text": "Part 2: Clustering Methods\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset.\n\nClustering Methods\n\nApply K-Means, DBSCAN, and Hierarchical clustering to dataset.\n\nResults Section\nConclusion\n\n\ndef plot_2D(X,color_vector, modelName):\n    \"\"\"Draw 2D scatter plots for data and predicted labels\n\n    Args:\n        X (array): data\n        color_vector (array): predicted labels\n        modelName (str): title on the plot\n    \"\"\"\n    fig, ax = plt.subplots()\n    ax.scatter(X[:,0], X[:,1],c=color_vector, cmap='viridis',alpha=0.5)  \n    ax.set(xlabel='Dimension1', ylabel='Dimension2',\n    title=f'{modelName}')\n    ax.grid()\n    plt.show()\n\n\ndef calculate_optimal_clustering(X, algo=\"kmeans\", nmax=20):\n    \"\"\"\n    Calculate the optimal clustering parameter and labels based on silhouette scores.\n\n    Parameters:\n        X (array-like): Input data for clustering.\n        algo (str): Clustering algorithm (\"birch\", \"ag\", \"dbscan\", \"kmeans\").\n        nmax (int): Maximum range for hyperparameter tuning.\n\n    Returns:\n        tuple: Optimal parameter, labels, and silhouette scores for each parameter.\n    \"\"\"\n    # Ensure `X` is contiguous\n    # X = X.toarray()\n    \n    # Initialize variables\n    params = []\n    sil_scores = []\n    sil_max = -10\n    opt_param = None\n    opt_labels = None\n\n    for p in range(2, nmax + 1):\n        if algo.lower() == \"birch\":\n            model = skcluster.Birch(n_clusters=p)\n            pred_labels = model.fit_predict(X)\n\n        elif algo.lower() == \"ag\":\n            model = skcluster.AgglomerativeClustering(n_clusters=p)\n            pred_labels = model.fit_predict(X)\n\n        elif algo.lower() == \"dbscan\":\n            eps = p * 0.25  # Adjust `eps` for DBSCAN\n            model = skcluster.DBSCAN(eps=eps, min_samples=2) \n            pred_labels = model.fit_predict(X)\n\n        elif algo.lower() == \"kmeans\":\n            model = skcluster.KMeans(n_clusters=p, random_state=42)\n            pred_labels = model.fit_predict(X)\n\n        else:\n            raise ValueError(\"Unsupported clustering algorithm.\")\n        \n        # Compute silhouette score\n        try: \n            sil_score = silhouette_score(X, pred_labels)\n            sil_scores.append(sil_score)\n            params.append(p)\n        except:\n            continue\n\n        if sil_score &gt;= sil_max:\n            sil_max = sil_score\n            opt_param = p\n            opt_labels = pred_labels\n\n    if not sil_scores:\n        raise ValueError(\"No valid silhouette scores found. Ensure your data and parameters are suitable for clustering.\")\n    \n    return opt_param, opt_labels, params, sil_scores\n\n\ndef plot_silhouette_scores(params, sil_scores, algo):\n    \"\"\"\n    Plot silhouette scores against clustering parameters.\n\n    Parameters:\n        params (list): List of clustering parameters.\n        sil_scores (list): Corresponding silhouette scores for each parameter.\n        algo (str): Clustering algorithm name (for title).\n\n    Returns:\n    a line plot of Silhouette Score in different parameter values\n    \"\"\"\n    plt.figure(figsize=(8, 6))\n    plt.plot(params, sil_scores, marker='o', label=\"Silhouette Score\")\n    plt.xlim(1, max(params) + 1)\n    plt.xlabel(\"Parameter\")\n    plt.ylabel(\"Silhouette Score\")\n    plt.title(f\"Silhouette Scores for {algo.lower()} Clustering\")\n    plt.legend()\n    plt.grid()\n    plt.show()\n\n\npca_X = dimensionality_reduction(combined_features, method=\"pca\")\nopt_param, opt_labels, params, sil_scores = calculate_optimal_clustering(pca_X, algo=\"kmeans\")\nplot_silhouette_scores(params, sil_scores, algo=\"kmeans\")\nprint(\"Optimal Parameter:\", opt_param)\nprint(\"Number of Optimal Labels: \", len(opt_labels))\nplot_2D(pca_X, opt_labels, \"Kmeans\")\n\nTuning PCA number of components...\nNumber of componenets: 2, Explained Variance Ratio: 0.007622876451205808\nNumber of componenets: 7, Explained Variance Ratio: 0.02356908405705091\nNumber of componenets: 12, Explained Variance Ratio: 0.036920049979048984\nNumber of componenets: 17, Explained Variance Ratio: 0.04855698535454517\nNumber of componenets: 22, Explained Variance Ratio: 0.05915508579096912\nNumber of componenets: 27, Explained Variance Ratio: 0.06930950728830565\nNumber of componenets: 32, Explained Variance Ratio: 0.07930831947846977\nNumber of componenets: 37, Explained Variance Ratio: 0.08905551489624212\nNumber of componenets: 42, Explained Variance Ratio: 0.09837550758343419\nNumber of componenets: 47, Explained Variance Ratio: 0.10756753880097233\nBest number of components: 47, Explained Variance Ratio: 0.10756753880097233\n\n\n\n\n\n\n\n\n\nOptimal Parameter: 2\nNumber of Optimal Labels:  2130\n\n\n\n\n\n\n\n\n\nWe tuned the number of components to fit PCA based on explained variance ratio and select 47 as the best parameter, which indicates it captures 10% of the variance in the dataset. Despite this relatively low explained variance, the K-Means clustering model identified 2 optimal clusters based on silhouette scores, with the PCA-transformed data. This means that K-Means effectively groups the 2130 data points into 2 clusters, although the PCA reduction retains only a portion of the total variance. The results suggest that PCA may not fully capture the complexity of the dataset.\n\ntsne_X = dimensionality_reduction(combined_features, method=\"tsne\")\nopt_param, opt_labels, params, sil_scores = calculate_optimal_clustering(tsne_X, algo=\"kmeans\")\nplot_silhouette_scores(params, sil_scores, algo=\"kmeans\")\nprint(\"Optimal Parameter:\", opt_param)\nprint(\"Number of Optimal Labels: \", len(opt_labels))\nplot_2D(tsne_X, opt_labels, \"Kmeans\")\n\nTuning t-SNE perplexity...\nPerplexity: 5, KL Divergence: 5.56992769241333\nPerplexity: 10, KL Divergence: 4.418832302093506\nPerplexity: 30, KL Divergence: 3.3578200340270996\nPerplexity: 50, KL Divergence: 2.8662688732147217\nBest Perplexity: 50, Best KL Divergence: 2.8662688732147217\n\n\n\n\n\n\n\n\n\nOptimal Parameter: 7\nNumber of Optimal Labels:  2130\n\n\n\n\n\n\n\n\n\nWe tuned the value of perplexity to fit TSNE based on KL divergence score and received 50 as the best value of perplexity, which has the lowest difference between the pairwise similarities in the original high-dimensional space and the pairwise similarities in the lower-dimensional space after transformation. The Kmeans clustering model identified 7 optimal clusters based on silhouette scores, with the TSNE-transformed data. This means that K-Means effectively groups the 2130 data points into 7 clusters. The distribution of data points on the second scatter plot suggests that TSNE successfully captures the complexity of the dataset.\n\nopt_param, opt_labels, params, sil_scores = calculate_optimal_clustering(tsne_X, algo=\"dbscan\")\nplot_silhouette_scores(params, sil_scores, algo=\"dbscan\")\nprint(\"Optimal Parameter:\", opt_param)\nprint(\"Number of Optimal Labels: \", len(opt_labels))\nplot_2D(tsne_X, opt_labels, \"DBSCAN\")\n\n\n\n\n\n\n\n\nOptimal Parameter: 2\nNumber of Optimal Labels:  2130\n\n\n\n\n\n\n\n\n\nIn the section above, we applied t-SNE with a perplexity of 50 to reduce the dimensionality of the same dataset. Using the t-SNE-transformed data, the DBSCAN clustering model identified 2 optimal clusters based on silhouette scores. This means DBSCAN grouped the 2130 data points into 2 clusters.\nHowever, the scatter plot suggests that DBSCAN visually separates the data into three groups based on color. The separation appears less effective compared to the results achieved with K-Means, which provided clearer and more distinct clusters.\n\nopt_param, opt_labels, params, sil_scores = calculate_optimal_clustering(tsne_X, algo=\"AG\")\nplot_silhouette_scores(params, sil_scores, algo=\"AG\")\nprint(\"Optimal Parameter:\", opt_param)\nprint(\"Number of Optimal Labels: \", len(opt_labels))\nplot_2D(tsne_X, opt_labels, \"Agglomerative Clustering\")\n\n\n\n\n\n\n\n\nOptimal Parameter: 6\nNumber of Optimal Labels:  2130\n\n\n\n\n\n\n\n\n\nIn the section above, we applied t-SNE with a perplexity of 50 to reduce the dimensionality of the dataset. Using the t-SNE-transformed data, the Agglomerative Clustering model identified 6 optimal clusters based on silhouette scores. This result suggests that Agglomerative Clustering effectively groups the 2130 data points into 6 clusters, capturing some patterns within the dataset. The scatter plot further highlights how t-SNE successfully captures the complexity and nonlinear structure of the data, enabling meaningful clustering.\n\nopt_param, opt_labels, params, sil_scores = calculate_optimal_clustering(tsne_X, algo=\"BIRCH\")\nplot_silhouette_scores(params, sil_scores, algo=\"BIRCH\")\nprint(\"Optimal Parameter:\", opt_param)\nprint(\"Number of Optimal Labels: \", len(opt_labels))\nplot_2D(tsne_X, opt_labels, \"BIRCH\")\n\n\n\n\n\n\n\n\nOptimal Parameter: 8\nNumber of Optimal Labels:  2130\n\n\n\n\n\n\n\n\n\nIn the section above, we applied t-SNE with a perplexity of 50 to reduce the dimensionality of the dataset. Using the t-SNE-transformed data, the BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) model identified 8 optimal clusters based on silhouette scores. This result indicates that BIRCH effectively groups the 2130 data points into 8 clusters, capturing key patterns within the dataset.\nThe scatter plot illustrates that t-SNE successfully captures the dataset’s complexity and nonlinear relationships, enabling meaningful clustering. When compared to the clustering results from K-Means, the outcomes are notably similar. This similarity can be attributed to BIRCH’s approach of recursively compressing the data into a CF (Clustering Feature) tree and performing clustering on the summarized clusters, which can align well with the results of K-Means under certain data structures."
  },
  {
    "objectID": "instructions/github-usage.html",
    "href": "instructions/github-usage.html",
    "title": "GitHub",
    "section": "",
    "text": "Your project will be fully transparent, with all source code hosted on GitHub. This platform will serve as the main repository for your project code, documentation, and website. Proper organization and regular updates are key for effective collaboration and project management.\n\nIMPORTANT: Proficiency in GitHub for collaboration is a valuable addition to your resume. Being able to join a team and immediately contribute by solving problems and adding value is a highly sought-after skill. Now is the time to develop this expertise—embrace Git fully, become proficient, and graduate with a critical skill for your future career.\n\n\n\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure.\n\n\n\n\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies.\n\n\n\n\n\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/github-usage.html#repository-setup",
    "href": "instructions/github-usage.html#repository-setup",
    "title": "GitHub",
    "section": "",
    "text": "You MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/github-usage.html#expectations-for-github-usage",
    "href": "instructions/github-usage.html#expectations-for-github-usage",
    "title": "GitHub",
    "section": "",
    "text": "Your grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "href": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "title": "GitHub",
    "section": "",
    "text": "If you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/quarto-tips.html",
    "href": "instructions/quarto-tips.html",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton.\n\n\n\n\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/quarto-tips.html#file-types",
    "href": "instructions/quarto-tips.html#file-types",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/quarto-tips.html#quarto-includes",
    "href": "instructions/quarto-tips.html#quarto-includes",
    "title": "Quarto Tips",
    "section": "",
    "text": "Quarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/website-structure.html",
    "href": "instructions/website-structure.html",
    "title": "Website project structure",
    "section": "",
    "text": "Please at miniumum include the following pages in your website:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\nSupervised-learning\nLLM-usage\nProgress-log\n\n\nPlease adhere closely to this structure, for consistency accross projects.\nSub-sections can be handles as markdown headers in the respective pages.\nYou can add more pages,and if you want, you can merge EDA and unsupervised learning into one page, since the are similar. Or make these section headers in the dropdown menu, for further sub-sections creation.\nFor example:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\n\nClustering\nDimensionality Reduction\n\nSupervised-learning\n\nFeature selection\n\nregression\nclassification\n\nClassification\n\nBinary classification\nMulti-class classification\n\nRegression\n\nLLM-usage\nProgress-log\n\n\nImportant: Exactly what you put on these pages will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\nA skeleton of the recommended version of the website is provided in the github classroom repository.\n./\n├── README.md\n├── _quarto.yml\n├── assets\n│   ├── gu-logo.png\n│   ├── nature.csl\n│   └── references.bib\n├── build.sh\n├── data\n│   ├── processed-data\n│   │   └── countries_population.csv\n│   └── raw-data\n│       └── countries_population.csv\n├── index.qmd\n├── instructions\n│   ├── expectations.qmd\n│   ├── github-usage.qmd\n│   ├── llm-usage.qmd\n│   ├── overview.qmd\n│   ├── quarto-tips.qmd\n│   ├── topic-selection.qmd\n│   └── website-structure.qmd\n├── report\n│   └── report.qmd\n└── technical-details\n    ├── data-cleaning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── data-collection\n    │   ├── closing.qmd\n    │   ├── instructions.qmd\n    │   ├── main.ipynb\n    │   ├── methods.qmd\n    │   └── overview.qmd\n    ├── eda\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── llm-usage-log.qmd\n    ├── progress-log.qmd\n    ├── supervised-learning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    └── unsupervised-learning\n        ├── instructions.qmd\n        └── main.ipynb\nAlways strive to incorporate the following:\n\nStructure: Use clear headings and subheadings to break down each section of your EDA.\nClarity: Provide concise explanations for all tables and visualizations, ensuring they are easy to interpret.\nCode Links: Link to relevant code (e.g., GitHub) or embed code snippets for transparency and reproducibility.\nReproducibility: Make your EDA reproducible by providing access to the dataset, scripts, and tools you used.\nVisualization: Use visualizations to convey key insights\n\n\n\nIt is required that you build your website with Quarto.\n\n\n\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO.\n\n\n\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/website-structure.html#website-development",
    "href": "instructions/website-structure.html#website-development",
    "title": "Website project structure",
    "section": "",
    "text": "It is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/website-structure.html#website-hosting",
    "href": "instructions/website-structure.html#website-hosting",
    "title": "Website project structure",
    "section": "",
    "text": "You MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/website-structure.html#the-two-audiences",
    "href": "instructions/website-structure.html#the-two-audiences",
    "title": "Website project structure",
    "section": "",
    "text": "Knowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html",
    "href": "technical-details/data-cleaning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you on this page will be specific you your project and data. Some things might “make more sense” on other pages, depending on your workflow, for example, you might feel that normalization and scaling should be included in a later section, dealing with machine learning, rather than here, that is totally fine. Organize your project in the way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\n\nIterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "href": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#general-comments",
    "href": "technical-details/data-cleaning/instructions.html#general-comments",
    "title": "Instructions",
    "section": "",
    "text": "Iterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#what-to-address",
    "href": "technical-details/data-cleaning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html",
    "href": "technical-details/data-collection/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling.\n\n\n\nBegin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work.\n\n\n\n\nDuring the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder\n\n\n\n\n\nYour data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "href": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#what-to-address",
    "href": "technical-details/data-collection/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#start-collecting-data",
    "href": "technical-details/data-collection/instructions.html#start-collecting-data",
    "title": "Instructions",
    "section": "",
    "text": "Begin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "href": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "title": "Instructions",
    "section": "",
    "text": "During the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#requirements",
    "href": "technical-details/data-collection/instructions.html#requirements",
    "title": "Instructions",
    "section": "",
    "text": "Your data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/overview.html",
    "href": "technical-details/data-collection/overview.html",
    "title": "Overview",
    "section": "",
    "text": "Overview\nThis section provides a high-level summary for technical staff, outlining the key tasks and processes undertaken in this project. It establishes the context, motivation, and objectives of the work.\n\nGoals:\nThe primary goal of this document is to efficiently collect data from the YouTube API to build a robust dataset for addressing future data science research questions. This involves designing a reliable workflow for data acquisition, using Youtube Data API-v3 calls to gather video metadata, and curating a dataset of sufficient size and quality.\nMotivation:\nThe motivation behind this data collection effort is to source original and reliable data directly from the YouTube API. To ensure the dataset is representative and meaningful, we aim to diversify the collected data by selecting subsets of videos from different countries while limiting topics to reduce noise and ambiguity. This approach allows us to create a focused, high-quality dataset that reflects diverse content while maintaining relevance to our research goals.\nObjectives:\nThe specific objectives for data collection are as follows:\n\nRandomly retrieve video metadata using the YouTube Data API.\n\nSelect useful video features, such as viewing statistics, to ensure the data supports comprehensive analysis.\n\nInvestigate YouTube’s recommendation mechanism by retrieving the linked “next page” videos associated with each video.\n\nFocus on videos from English-speaking countries (e.g., the United States and the United Kingdom) to maintain consistency in language and cultural context.\n\n\nThis overview provides technical staff with the necessary context to understand the importance of the work and guides them in exploring the details presented in subsequent sections."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html",
    "href": "technical-details/supervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results.\n\n\n\n\nNormalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling.\n\n\n\n\n\nModel Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used\n\n\n\n\n\nSplit Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset.\n\n\n\n\n\nBinary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc.\n\n\n\n\n\nModel Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots).\n\n\n\n\n\nResult Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#what-to-address",
    "href": "technical-details/supervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "href": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "title": "Instructions",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-selection",
    "href": "technical-details/supervised-learning/instructions.html#model-selection",
    "title": "Instructions",
    "section": "",
    "text": "Model Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "title": "Instructions",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "title": "Instructions",
    "section": "",
    "text": "Binary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#results",
    "href": "technical-details/supervised-learning/instructions.html#results",
    "title": "Instructions",
    "section": "",
    "text": "Model Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#discussion",
    "href": "technical-details/supervised-learning/instructions.html#discussion",
    "title": "Instructions",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Unveiling the Secrets of Audience Engagement: Decoding the Factors Behind YouTube Video Success",
    "section": "",
    "text": "Yuxi Shen is a current Master Student in Data Science and Analytics program at Georgetown University. She received her Bachelor’s degree in Northeastern University at Boston, MA majored in Data Science and Psychology in May 2024. She wants to deepen her expertise in machine learning and NLP and gain more hands-on programming experience through this Master program. Her particular interests lie in the applications of Data Science and Data Analytics within the finance and healthcare sectors, including mental health. In her opinion, data can significantly influence decision-making and improve outcomes for individuals and organizations. This project is a group project for my DSAN 5000 Data Science and Analytics course at Georgetown University.\nContact Info: ys1079@georgetown.edu\nConnect with me on LinkedIn\nAcademic Interests\n\nData Analytics in the sectors of Finance and Healthcare\nMachine Learning Algorithms\nNatural Language Processing Models\n\nEducation\n\n2024 - Expected 2026: Master of Science in Data Science and Analytics at Georgetown University\n2019 - 2024: Bachelor of Science in Data Science and Psychology at Northeastern University\n\n\n\n\nThis project explores the dynamics of YouTube video popularity by analyzing metrics such as views, likes, comments, engagement rates, and video duration with data gathered from YouTube API. Understanding what drives a video’s success is significant for content creators and marketers aiming to optimize their strategies for audience engagement and also for decision makers to understand the direction of public opinions. Building on prior work in video analytics and social media data mining, this study investigates patterns and trends within YouTube data to uncover factors contributing to video virality.\nWe outline the key research questions driving this project:\n\nWhat are the distinct types of YouTube videos based on their content (e.g., topics) or engagement metrics? How effectively can videos be clustered using their features, and what do these clusters represent in terms of audience behavior or content patterns?\nWhat role does video duration play in engagement and overall success? Specifically, do audiences show a preference for certain topics depending on whether the videos are long or short?\nWhat key metrics should video creators prioritize when assessing their performance, and how can platforms like YouTube improve their tools to better evaluate the success of channels and content creators?\nWhat characteristics should advertisers prioritize working with creators to ensure the best results from their ads?\nIs it possible to predict the popularity trends of specific video categories to help creators gain a competitive edge?\nWhich features have the strongest impact on video popularity, and how can we interpret their influence to guide content creation strategies?\n\n\n\n\nVideo Information:\n\nLike Count: 8.1 million likes\nView Count: 6 million views\nComment Count: 8489 comments\nDuration: 15 seconds\n\nThis video shows an exceptionally high like-to-view ratio, as the number of likes (8.1 million) surpasses the total views (6 million). This unusual engagement suggests that the video strongly resonated with viewers, potentially due to its short duration (15 seconds), which aligns well with audience preferences for quick, entertaining content. The short format increases the likelihood of replays, boosting engagement metrics like likes and comments. Additionally, the relatively high comment count (8,489) indicates that the video encouraged viewer interaction, further contributing to its success.\n  \n\n\n\nSocial Media Data Analysis, YouTube API, Information Retrieval, Supervised Learning, Unsupervised Learning, Video Recommendations, Machine Learning Algorithm, Video Content Analysis, Attention Mechanisms, Viewer Engagement, YouTuber Personality, Multimodal Analysis, Audience Attraction, Social Networks\n\n\n\nIn the Literature review section, we found research papers to enrich our vision in the video analysis and social media analysis area:\n\nFazeli et al.1 introduce the Visual-Verbal Video Analysis (VVVA) method, a systematic six-step approach combining Multimodal and Visual Grounded Theory. This framework categorizes visual, verbal, and textual elements, offering a comprehensive methodology for video data analysis in social sciences and medicine. While our focus is on video statistics and text attributes, this study inspires us to integrate multi-format data into future projects.\nZhou et al.2 investigate video recommendation mechanisms on platforms like YouTube and Tencent, highlighting Direct and Word-of-Mouth (WOM) recommendations. By analyzing historical view count traces using epidemic models, they estimate the user reach of these mechanisms. This study provides insights for optimizing recommendation strategies, relevant for creators seeking to enhance visibility.\nSedmak and Svetina3 reveal that adolescents’ attraction to YouTubers is driven by “identification,” shaped by YouTubers’ personal traits, their relationships with followers, and the medium’s accessibility. The findings emphasize that relatability and mutual recognition are key to audience connection. These insights could inform strategies to better understand audience attention dynamics.\nA study4 highlights how YouTubers’ self-disclosure, relatability, and charisma foster pseudo-social interactions, akin to parasocial relationships. These bonds influence prolonged engagement, brand attitudes, and purchasing behavior. This research encourages integrating YouTuber traits into our analysis to better understand how personal factors drive engagement.\nYang et al.5 explore how video length, production quality, presenter attributes, and audience demographics impact engagement with science content on YouTube. The study emphasizes balancing accessibility with informative content to retain viewers. These findings inspire us to consider video design elements and audience behavior in future analyses.\nZhang et al.6 focus on contextual factors, such as environment and social dynamics, that shape audience engagement with vloggers. Authentic and relatable contexts are pivotal in fostering audience connection. This study highlights the potential of expanding our research to include situational and behavioral data to better understand engagement dynamics.\n\n\n\n\nCase Study 1: Analyzing Video Engagement Patterns Across Content Categories\nBy leveraging data collected from the YouTube API, we examined engagement metrics (views, likes, comments, etc.) across various video categories, such as entertainment, education, and fitness. For example:\n\nKey Insight: Entertainment videos often have high engagement rates in terms of likes and comments, despite shorter view durations, indicating that audiences actively interact with content they find valuable and concise. Additionally, we found that entertainment videos frequently overlap with short film cuts, suggesting that people living in modern, fast-paced lives have developed new ways to consume movies as entertainment.\nImpact: These findings can help educational YouTubers optimize their content for higher audience interaction while maintaining brevity.\n\nCase Study 2: Exploring Clusters of Similar Videos\nThrough unsupervised maching learning clustering models, we grouped videos based on engagement metrics and textual data from titles and descriptions. For instance:\n\nClusters Found:\n\nCluster 1: High-engagement, short videos in trending topics.\nCluster 2: Long, in-depth videos with steady views over time.\n\nImpact: Clustering revealed distinct audience preferences and engagement patterns, helping content creators and marketers identify strategies for targeted video creation.\n\nCase Study 3: Investigating the Role of Video Duration in Engagement\nWe analyzed whether video duration correlates with audience engagement across different content types.\n\nKey Insight: Video duration shows a strong correlation with video popularity, as demonstrated by the plots generated during the Exploratory Data Analysis phase. Less popular videos (with fewer than 6,000,000 views) have a wide range of video lengths, whereas popular videos (with more than 6,000,000 views) are concentrated around shorter durations.\nImpact: These insights inform creators about the optimal video length based on their target audience and content category.\n\nCase Study 4: Predicting Popularity with Supervised Learning\nUsing machine learning classification techniques, we developed a model to predict video popularity based on features such as like count, video duration, video definition and topic categories.\n\nKey Insight: The model demonstrated high accuracy in predicting videos with low view counts, indicating its reliability in identifying less popular content, while also providing valuable predictions for high view count videos.\nImpact: The model’s ability to accurately identify low-popularity videos and potential breakout content enables content creators and platforms to optimize resource allocation, refine recommendations, and tailor strategies to enhance engagement and audience reach.\n\n\n\n\nWe aim to uncover insights behind YouTube video engagement and audience behavior by creating visualizations and outlining case studies. We are eager to explore how video metrics and machine learning are reshaping content strategies. Follow the progress of this project on GitHub. Contributions are welcome! If you’d like to suggest improvements, report issues, or contribute to the project, feel free to submit a pull request or open an issue.\n\n\n\n\n\ngit clone https://github.com/dsan-5000/project-YuxiShen1079.git\ncd &lt;root-of-this-project &gt; \nTo run the project and navigate between sections, use:\nquarto preview _site\n\n\n\nXun_Lei_portfolio_project\nYuxi_Shen_portfolio_project"
  },
  {
    "objectID": "index.html#landing-page",
    "href": "index.html#landing-page",
    "title": "Unveiling the Secrets of Audience Engagement: Decoding the Factors Behind YouTube Video Success",
    "section": "",
    "text": "Yuxi Shen is a current Master Student in Data Science and Analytics program at Georgetown University. She received her Bachelor’s degree in Northeastern University at Boston, MA majored in Data Science and Psychology in May 2024. She wants to deepen her expertise in machine learning and NLP and gain more hands-on programming experience through this Master program. Her particular interests lie in the applications of Data Science and Data Analytics within the finance and healthcare sectors, including mental health. In her opinion, data can significantly influence decision-making and improve outcomes for individuals and organizations. This project is a group project for my DSAN 5000 Data Science and Analytics course at Georgetown University.\nContact Info: ys1079@georgetown.edu\nConnect with me on LinkedIn\nAcademic Interests\n\nData Analytics in the sectors of Finance and Healthcare\nMachine Learning Algorithms\nNatural Language Processing Models\n\nEducation\n\n2024 - Expected 2026: Master of Science in Data Science and Analytics at Georgetown University\n2019 - 2024: Bachelor of Science in Data Science and Psychology at Northeastern University\n\n\n\n\nThis project explores the dynamics of YouTube video popularity by analyzing metrics such as views, likes, comments, engagement rates, and video duration with data gathered from YouTube API. Understanding what drives a video’s success is significant for content creators and marketers aiming to optimize their strategies for audience engagement and also for decision makers to understand the direction of public opinions. Building on prior work in video analytics and social media data mining, this study investigates patterns and trends within YouTube data to uncover factors contributing to video virality.\nWe outline the key research questions driving this project:\n\nWhat are the distinct types of YouTube videos based on their content (e.g., topics) or engagement metrics? How effectively can videos be clustered using their features, and what do these clusters represent in terms of audience behavior or content patterns?\nWhat role does video duration play in engagement and overall success? Specifically, do audiences show a preference for certain topics depending on whether the videos are long or short?\nWhat key metrics should video creators prioritize when assessing their performance, and how can platforms like YouTube improve their tools to better evaluate the success of channels and content creators?\nWhat characteristics should advertisers prioritize working with creators to ensure the best results from their ads?\nIs it possible to predict the popularity trends of specific video categories to help creators gain a competitive edge?\nWhich features have the strongest impact on video popularity, and how can we interpret their influence to guide content creation strategies?\n\n\n\n\nVideo Information:\n\nLike Count: 8.1 million likes\nView Count: 6 million views\nComment Count: 8489 comments\nDuration: 15 seconds\n\nThis video shows an exceptionally high like-to-view ratio, as the number of likes (8.1 million) surpasses the total views (6 million). This unusual engagement suggests that the video strongly resonated with viewers, potentially due to its short duration (15 seconds), which aligns well with audience preferences for quick, entertaining content. The short format increases the likelihood of replays, boosting engagement metrics like likes and comments. Additionally, the relatively high comment count (8,489) indicates that the video encouraged viewer interaction, further contributing to its success.\n  \n\n\n\nSocial Media Data Analysis, YouTube API, Information Retrieval, Supervised Learning, Unsupervised Learning, Video Recommendations, Machine Learning Algorithm, Video Content Analysis, Attention Mechanisms, Viewer Engagement, YouTuber Personality, Multimodal Analysis, Audience Attraction, Social Networks\n\n\n\nIn the Literature review section, we found research papers to enrich our vision in the video analysis and social media analysis area:\n\nFazeli et al.1 introduce the Visual-Verbal Video Analysis (VVVA) method, a systematic six-step approach combining Multimodal and Visual Grounded Theory. This framework categorizes visual, verbal, and textual elements, offering a comprehensive methodology for video data analysis in social sciences and medicine. While our focus is on video statistics and text attributes, this study inspires us to integrate multi-format data into future projects.\nZhou et al.2 investigate video recommendation mechanisms on platforms like YouTube and Tencent, highlighting Direct and Word-of-Mouth (WOM) recommendations. By analyzing historical view count traces using epidemic models, they estimate the user reach of these mechanisms. This study provides insights for optimizing recommendation strategies, relevant for creators seeking to enhance visibility.\nSedmak and Svetina3 reveal that adolescents’ attraction to YouTubers is driven by “identification,” shaped by YouTubers’ personal traits, their relationships with followers, and the medium’s accessibility. The findings emphasize that relatability and mutual recognition are key to audience connection. These insights could inform strategies to better understand audience attention dynamics.\nA study4 highlights how YouTubers’ self-disclosure, relatability, and charisma foster pseudo-social interactions, akin to parasocial relationships. These bonds influence prolonged engagement, brand attitudes, and purchasing behavior. This research encourages integrating YouTuber traits into our analysis to better understand how personal factors drive engagement.\nYang et al.5 explore how video length, production quality, presenter attributes, and audience demographics impact engagement with science content on YouTube. The study emphasizes balancing accessibility with informative content to retain viewers. These findings inspire us to consider video design elements and audience behavior in future analyses.\nZhang et al.6 focus on contextual factors, such as environment and social dynamics, that shape audience engagement with vloggers. Authentic and relatable contexts are pivotal in fostering audience connection. This study highlights the potential of expanding our research to include situational and behavioral data to better understand engagement dynamics.\n\n\n\n\nCase Study 1: Analyzing Video Engagement Patterns Across Content Categories\nBy leveraging data collected from the YouTube API, we examined engagement metrics (views, likes, comments, etc.) across various video categories, such as entertainment, education, and fitness. For example:\n\nKey Insight: Entertainment videos often have high engagement rates in terms of likes and comments, despite shorter view durations, indicating that audiences actively interact with content they find valuable and concise. Additionally, we found that entertainment videos frequently overlap with short film cuts, suggesting that people living in modern, fast-paced lives have developed new ways to consume movies as entertainment.\nImpact: These findings can help educational YouTubers optimize their content for higher audience interaction while maintaining brevity.\n\nCase Study 2: Exploring Clusters of Similar Videos\nThrough unsupervised maching learning clustering models, we grouped videos based on engagement metrics and textual data from titles and descriptions. For instance:\n\nClusters Found:\n\nCluster 1: High-engagement, short videos in trending topics.\nCluster 2: Long, in-depth videos with steady views over time.\n\nImpact: Clustering revealed distinct audience preferences and engagement patterns, helping content creators and marketers identify strategies for targeted video creation.\n\nCase Study 3: Investigating the Role of Video Duration in Engagement\nWe analyzed whether video duration correlates with audience engagement across different content types.\n\nKey Insight: Video duration shows a strong correlation with video popularity, as demonstrated by the plots generated during the Exploratory Data Analysis phase. Less popular videos (with fewer than 6,000,000 views) have a wide range of video lengths, whereas popular videos (with more than 6,000,000 views) are concentrated around shorter durations.\nImpact: These insights inform creators about the optimal video length based on their target audience and content category.\n\nCase Study 4: Predicting Popularity with Supervised Learning\nUsing machine learning classification techniques, we developed a model to predict video popularity based on features such as like count, video duration, video definition and topic categories.\n\nKey Insight: The model demonstrated high accuracy in predicting videos with low view counts, indicating its reliability in identifying less popular content, while also providing valuable predictions for high view count videos.\nImpact: The model’s ability to accurately identify low-popularity videos and potential breakout content enables content creators and platforms to optimize resource allocation, refine recommendations, and tailor strategies to enhance engagement and audience reach.\n\n\n\n\nWe aim to uncover insights behind YouTube video engagement and audience behavior by creating visualizations and outlining case studies. We are eager to explore how video metrics and machine learning are reshaping content strategies. Follow the progress of this project on GitHub. Contributions are welcome! If you’d like to suggest improvements, report issues, or contribute to the project, feel free to submit a pull request or open an issue.\n\n\n\n\n\ngit clone https://github.com/dsan-5000/project-YuxiShen1079.git\ncd &lt;root-of-this-project &gt; \nTo run the project and navigate between sections, use:\nquarto preview _site\n\n\n\nXun_Lei_portfolio_project\nYuxi_Shen_portfolio_project"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Unveiling the Secrets of Audience Engagement: Decoding the Factors Behind YouTube Video Success",
    "section": "Getting Started",
    "text": "Getting Started\nTo begin the project, first read the instruction document (click here). This document is also accessible from the navigation bar.\nOnce you’ve completed that, you can proceed with the instructions found throughout the website."
  },
  {
    "objectID": "index.html#what-to-include-on-this-page",
    "href": "index.html#what-to-include-on-this-page",
    "title": "Unveiling the Secrets of Audience Engagement: Decoding the Factors Behind YouTube Video Success",
    "section": "What to Include on This Page",
    "text": "What to Include on This Page\nThis is the landing page for your project. Content from this page can be reused in sections of your final report.\n\nCreate an “About You” Page\n\nDevelop your “About You” page. You can reuse content from previous assignments.\nYou can include the content here or on a separate page.\n\nIt’s recommended to create one “About You” page for all DSAN projects, with links to your various class projects.\n\n\n\n\nCreate a Landing Page for Your Project\n\nSummarize your topic, its significance, related work, and the questions you plan to explore.\nDraft an introduction with at least 5 research questions. These may evolve as your project progresses, since data science is an iterative process.\nInclude your data science questions on this page.\n\n\n\nLiterature review\nOnce you decide on a topic, you should ALWAYS START WITH A LITERATURE REVIEW, this is particularly important for academic projects.\nThe literature review is the most important part of most projects.\nIt allows you to;\n\n\nDetermine what is already known and what has already been tried, so that you don't re-invent the wheel.\n\n\nIt makes you more of a subject matter expert, allowing you to ask the right questions, target impactful projects, and communicate with other professionals.\n\n\n\nDoing a project that you think will change the world, only to find at the end that a very similar version of your project was already done in the 1980’s, isn’t a great use of time.\n\nIn this section, please do a literature review and cite at least 3 academic publications per group member, and include internal academic citations.\nOptional: Consider using LLM tools to 10X your literature review, e.g. instead of focusing on 3 papers, aim for 30 or more\nBy following these steps, you can 10X the efficiency of your literature review process, gaining more insights while minimizing the time spent on manual reading.\n\nExpand Your Literature Search\nAim to gather a larger pool of papers. Use academic databases (Google Scholar, arXiv, etc.) to find relevant studies and ensure a broad scope.\nSkim the Abstracts\nRead the abstracts of each paper to quickly understand their focus and identify those most relevant to your topic. Prioritize these papers for deeper analysis.\nUse LLM Tools for Summarization\nUpload the selected papers to an LLM tool capable of text summarization. Have it condense the main points of each paper into a concise, manageable summary (e.g., condense hundreds of pages into a 10-page summary). Carefully review this summary to absorb the key insights.\nLeverage Interactive LLM Tools\nUse tools like NotebookLM or other AI-based text digesters to ask specific, targeted questions about the papers:\n\nExample questions:\n\n“In the papers uploaded, did any of them explore XYZ?”\n“Which paper is most closely related to the following project idea: [explain idea]?” These tools will help you quickly extract relevant information without re-reading entire papers."
  },
  {
    "objectID": "index.html#additional-ideas-for-things-to-include",
    "href": "index.html#additional-ideas-for-things-to-include",
    "title": "Unveiling the Secrets of Audience Engagement: Decoding the Factors Behind YouTube Video Success",
    "section": "Additional Ideas for things to include",
    "text": "Additional Ideas for things to include\n\nAudience: Who is this for? Data professionals, businesses, researchers, or curious readers.\nHeadline: A captivating title introducing the data science theme (e.g., “Unlocking Insights Through Data Stories”).\nIntroduction: A brief, engaging overview of what the website offers (e.g., data-driven stories, insights, or case studies).\nQuestions You Are Addressing: What do you hope to learn?\nMotivation: Explain why this topic matters, highlighting the importance of data in solving real-world problems.\nKey Topics: List the main focus areas (e.g., machine learning, data visualization, predictive modeling).\nUse Cases/Examples: A brief teaser of compelling stories or case studies you’ve worked on.\nCall to Action: Invite visitors to explore the content, follow along, or contact you for more information.\nVisual/Infographic: Add a simple graphic or visual element to make the page more dynamic."
  },
  {
    "objectID": "technical-details/progress-log.html",
    "href": "technical-details/progress-log.html",
    "title": "Progress log",
    "section": "",
    "text": "Use this page to track your progress and keep a log of your contributions to the project, please update this each time you work on your project, it is generally a good habit to adopt.\nIf you are working as a team, at the end, you can duplicate the project and add it to your individual portfolio websites. If you do, you MUST retain attribution to your teammates. Removing attribution would constitute plagiarism."
  },
  {
    "objectID": "technical-details/progress-log.html#to-do",
    "href": "technical-details/progress-log.html#to-do",
    "title": "Progress log",
    "section": "To-do",
    "text": "To-do\n\nExplore possible topics by brainstorming with GPT\nIdentify the purpose of the analysis\nLiterative review: 3 paper per member\nConnect with Youtube API, collect data\nExplore dataset and clean unformmated data\nRemove duplicated data\nAnalyze trends in views, likes, and comments\nEDA: Visualize data distributions (e.g., bar charts, heatmaps).\nModeling: unsupervised learning (clustering)\nModeling: supervised learning (Random Forest)\nWebsite Development\nIntegrate front end with back end\nDeploy Analysis on GU domains\nwrite a technical methods sections for supervised learning models\nwrite a technical methods sections for unsupervised learning models\nTest website responsiveness and functionality.\nWrite a README or user guide for the project.\nFinal report"
  },
  {
    "objectID": "technical-details/progress-log.html#yuxi-shen",
    "href": "technical-details/progress-log.html#yuxi-shen",
    "title": "Progress log",
    "section": "Yuxi Shen:",
    "text": "Yuxi Shen:\nProvide their name, a link to their “About Me” page.\nAlso, describe a log of their project roles.\n\nAttend first group meeting\nUnderstand Data Source\nWas assigned research paper topics to explore: video analysis, psychology behind video attraction.\nCollect Data\nEDA\nUnsupervised learning Model training\nReport generation - Executive Summary\nReport generation - Objective\nReport generation - Key Insights\nReport generation - Visualizations\nReport generation - Business Implications\nReport generation - Recommendations\nReport generation - Conclusion\nLanding page\nconnect with GU domain website\nFinal report"
  },
  {
    "objectID": "technical-details/progress-log.html#xun-lei",
    "href": "technical-details/progress-log.html#xun-lei",
    "title": "Progress log",
    "section": "Xun Lei",
    "text": "Xun Lei\nProvide their name, a link to their “About Me” page.\nAlso, describe a log of their project roles.\n\nAttend first group meeting\nUnderstand Data Source\nClean data\nEDA\nBuild Supervised learning model, train models\nGenerate technical report\nReport generation - Executive Summary\nReport generation - Objective\nReport generation - Key Insights\nReport generation - Visualizations\nReport generation - Business Implications\nReport generation - Recommendations\nReport generation - Conclusion\nconnect with GU domain website\nLanding page\nFinal report"
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Unveiling the Secrets of Audience Engagement: Decoding the Factors Behind YouTube Video Success",
    "section": "Contributing",
    "text": "Contributing\nInterested in contributing? Check out the contributing guidelines.\nClone and set up the repository with\ngit clone https://github.com/dsan-5000/project-YuxiShen1079.git\ncd &lt;root-of-this-project &gt; \npip install -e"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Unveiling the Secrets of Audience Engagement: Decoding the Factors Behind YouTube Video Success",
    "section": "License",
    "text": "License\nThis project was created by Yuxi Shen and Xun Lei. Please contact us directly through emails."
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "Unveiling the Secrets of Audience Engagement: Decoding the Factors Behind YouTube Video Success",
    "section": "Credits",
    "text": "Credits\nThe structure of this project was provided by the DSAN 5000 course at Georgetown University, and the deployment was implemented using Quarto."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#regression-section",
    "href": "technical-details/supervised-learning/main.html#regression-section",
    "title": "Supervised Learning",
    "section": "Regression Section",
    "text": "Regression Section\n\nRandom Forest Regressor\n\nWhat it is:\nRandom Forest is an ensemble learning method that combines multiple decision trees to improve predictive accuracy and control overfitting.\nHow it works:\nEach tree is trained on a random subset of data and features. The final prediction is the average of predictions from all trees.\nInputs:\nFeatures include both numerical and categorical variables:\n\nlikeCount: Numeric (e.g., 12,000).\n\nduration: Numeric (e.g., 300 seconds).\n\ndefinition: Categorical, binary encoded (1 for hd, 0 for sd).\n\ntopicCategories: Categorical, one-hot encoded or frequency counts.\n\npopularity: Categorical, binary encoded (1 for high, 0 for low).\n\nOutputs:\nContinuous numerical predictions representing estimated view counts.\nExample: For the above inputs, the model might predict 1,200,000 views.\nKey Hyperparameters:\n\nn_estimators: Number of trees.\n\nmax_depth: Maximum depth of trees.\n\nmin_samples_split: Minimum samples required to split a node.\n\n\n\n\n\nLinear Regressor\n\nWhat it is:\nA simple, interpretable model predicting continuous outcomes, such as video view counts.\nHow it works:\nAssigns weights to each feature to predict the target variable by minimizing the difference between actual and predicted values.\nInputs:\n\nlikeCount: Numeric (e.g., 12,000).\n\nduration: Numeric (e.g., 300 seconds).\n\ndefinition: Categorical, binary encoded (1 for hd, 0 for sd).\n\ntopicCategories: Categorical, one-hot encoded or frequency counts.\n\npopularity: Categorical, binary encoded (1 for high, 0 for low).\n\nOutputs:\nContinuous numerical predictions of view counts.\nExample: For the above inputs, the model might predict 1,200,000 views.\nKey Hyperparameters:\n\nalpha: Regularization strength.\n\nsolver: Optimization method.\n\nmax_iter: Maximum number of iterations.\n\n\n\n\n\nGradient Boosting Regressor\n\nWhat it is:\nGradient Boosting is an ensemble method that builds a series of models sequentially, correcting errors from previous models.\nHow it works:\nSequentially adds decision trees trained on residual errors. The final prediction is the weighted sum of tree predictions.\nInputs:\n\nlikeCount: Numeric (e.g., 12,000).\n\nduration: Numeric (e.g., 300 seconds).\n\ndefinition: Categorical, binary encoded (1 for hd, 0 for sd).\n\ntopicCategories: Categorical, one-hot encoded or frequency counts.\n\npopularity: Categorical, binary encoded (1 for high, 0 for low).\n\nOutputs:\nPredicted view counts.\nExample: For the above inputs, the model might predict 1,200,000 views.\nKey Hyperparameters:\n\nn_estimators: Number of trees.\n\nmax_depth: Tree depth.\n\nlearning_rate: Contribution of each tree.\n\nsubsample: Fraction of data used for each tree."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#binary-classification",
    "href": "technical-details/supervised-learning/main.html#binary-classification",
    "title": "Supervised Learning",
    "section": "Binary classification",
    "text": "Binary classification\n\nRandom Forest classifier\nLogistic Regression\nGradient Boosting Classifier"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#multi-class-classification-section",
    "href": "technical-details/supervised-learning/main.html#multi-class-classification-section",
    "title": "Supervised Learning",
    "section": "Multi-Class Classification Section",
    "text": "Multi-Class Classification Section\n\nGradient Boosting Classifier\n\nWhat it is:\nAn ensemble method combining decision trees to classify multi-class problems effectively.\nHow it works:\nSequentially trains trees to minimize classification error, with predictions as weighted averages.\nInputs:\n\nlikeCount: Numeric.\n\nduration: Numeric.\n\ndefinition: Binary encoded.\n\ntopicCategories: One-hot encoded.\n\nOutputs:\n\nClass Labels: Predicted categories(multi-class popularity label).\n\nClass Probabilities: Probabilities for each class.\n\nKey Hyperparameters:\n\nn_estimators: Number of trees.\n\nlearning_rate: Step size for boosting iterations.\n\nsubsample: Fraction of data used per tree.\n\n\n\n\n\nK-Nearest Neighbors (KNN)\n\nWhat it is:\nA non-parametric method that classifies data points based on the majority vote of their k nearest neighbors.\nHow it works:\nCalculates distances to all training points, identifies the nearest k, and predicts the majority class.\nInputs:\n\nlikeCount, duration, definition, and topicCategories, scaled for distance calculations.\n\nOutputs:\n\nClass Labels: Predicted categories.\n\nClass Probabilities: Proportions of nearest neighbors for each class.\n\nKey Hyperparameters:\n\nn_neighbors: Number of neighbors.\n\nweights: Voting method.\n\nmetric: Distance metric (euclidean, manhattan, etc.).\n\n\n\n\n\nDecision Tree Classifier\n\nWhat it is: A supervised machine learning algorithm used for classification and regression tasks. It splits the dataset into smaller subsets based on feature values, forming a tree-like structure where each node represents a decision rule and each leaf node represents an outcome.\nHow it works: The dataset is split into branches based on feature values, applying criteria like Gini Impurity, Entropy, or Log Loss, until a stopping criterion is met, and predictions are made by traversing the tree to a leaf node that provides the final class or value.\nInputs:\n\nlikeCount, duration, definition, and topicCategories, scaled for distance calculations.\n\nOutputs:\n\nClass Labels: Predicted categories for popularity_multi_class.\n\nClass Probabilities: The probabilities for each class, calculated from the proportion of samples in a leaf node.\n\nKey Hyperparameters:\n\nCriterion: The function to measure the quality of a split (e.g., Gini Impurity, Entropy, or Log Loss).\n\nMax Depth: Maximum depth of the tree to prevent overfitting.\n\nMin Samples Split: Minimum number of samples required to split an internal node.\n\nMin Samples Leaf: Minimum number of samples required to be at a leaf node.\n\nMax Features: The number of features to consider when looking for the best split."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#multi-class-classification-section-k-nearest-neighbors-knn",
    "href": "technical-details/supervised-learning/main.html#multi-class-classification-section-k-nearest-neighbors-knn",
    "title": "Supervised Learning",
    "section": "Multi-class classification section （K-Nearest Neighbors (KNN)）",
    "text": "Multi-class classification section （K-Nearest Neighbors (KNN)）\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, log_loss, cohen_kappa_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Features and target variable\nfeatures = [\"likeCount\", \"duration\", \"definition\", \"topicCategories\"]\nX = data[features]  # Feature matrix\nX = X.fillna(0)  # Replace NaN values with 0\n\ny = data[\"popularity\"]  # Target variable (multi-class labels)\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n# Ensure categorical features are encoded properly\nX = pd.get_dummies(X, columns=[\"definition\", \"topicCategories\"], drop_first=True)\n\n# Feature scaling (important for KNN because it relies on distance metrics)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Cross-validation setup\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nclf = KNeighborsClassifier(n_neighbors=5)  # K=5 is a common default\n\n# Initialize metric storage\naccuracies, precisions_macro, recalls_macro, f1_scores_macro = [], [], [], []\nprecisions_weighted, recalls_weighted, f1_scores_weighted = [], [], []\nlog_losses, kappa_scores = [], []\n\n# Cross-validation loop\nfor train_idx, test_idx in cv.split(X_scaled, y):\n    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    \n    # Train the K-Nearest Neighbors Classifier\n    clf.fit(X_train, y_train)\n    \n    # Predictions\n    y_pred = clf.predict(X_test)\n    y_proba = clf.predict_proba(X_test)  # Probability estimates for all classes\n    \n    # Evaluate metrics\n    accuracies.append(accuracy_score(y_test, y_pred))\n    precisions_macro.append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n    recalls_macro.append(recall_score(y_test, y_pred, average='macro', zero_division=0))\n    f1_scores_macro.append(f1_score(y_test, y_pred, average='macro', zero_division=0))\n    \n    precisions_weighted.append(precision_score(y_test, y_pred, average='weighted', zero_division=0))\n    recalls_weighted.append(recall_score(y_test, y_pred, average='weighted', zero_division=0))\n    f1_scores_weighted.append(f1_score(y_test, y_pred, average='weighted', zero_division=0))\n    \n    log_losses.append(log_loss(y_test, y_proba))\n    kappa_scores.append(cohen_kappa_score(y_test, y_pred))\n\n# Print evaluation metrics\nprint(\"K-Nearest Neighbors (KNN) - Multi-Class Classification\")\nprint(\"=====================================================\")\nprint(f\"Accuracy: {np.mean(accuracies):.4f}\")\nprint(f\"Macro Precision: {np.mean(precisions_macro):.4f}\")\nprint(f\"Macro Recall: {np.mean(recalls_macro):.4f}\")\nprint(f\"Macro F1 Score: {np.mean(f1_scores_macro):.4f}\")\nprint(f\"Weighted Precision: {np.mean(precisions_weighted):.4f}\")\nprint(f\"Weighted Recall: {np.mean(recalls_weighted):.4f}\")\nprint(f\"Weighted F1 Score: {np.mean(f1_scores_weighted):.4f}\")\nprint(f\"Log Loss: {np.mean(log_losses):.4f}\")\nprint(f\"Cohen Kappa Score: {np.mean(kappa_scores):.4f}\")\n\n# Confusion matrix visualization\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plot the confusion matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n\n# Add titles and labels\nplt.title(\"Confusion Matrix - Multi-Class KNN\", fontsize=16)\nplt.xlabel(\"Predicted\", fontsize=14)\nplt.ylabel(\"Actual\", fontsize=14)\n\n# Show the plot\nplt.show()\n\n# Detailed classification report\nprint(\"\\nClassification Report:\\n\")\nprint(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n\nK-Nearest Neighbors (KNN) - Multi-Class Classification\n=====================================================\nAccuracy: 0.9357\nMacro Precision: 0.8851\nMacro Recall: 0.7792\nMacro F1 Score: 0.8195\nWeighted Precision: 0.9315\nWeighted Recall: 0.9357\nWeighted F1 Score: 0.9303\nLog Loss: 0.9993\nCohen Kappa Score: 0.6405\n\n\n\n\n\n\n\n\n\n\nClassification Report:\n\n              precision    recall  f1-score   support\n\n        high       0.82      0.60      0.69        45\n         low       0.95      0.98      0.97       343\n\n    accuracy                           0.94       388\n   macro avg       0.88      0.79      0.83       388\nweighted avg       0.93      0.94      0.93       388"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#random-forest-regressor",
    "href": "technical-details/supervised-learning/main.html#random-forest-regressor",
    "title": "Supervised Learning",
    "section": "Random Forest Regressor",
    "text": "Random Forest Regressor\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom scipy.stats import randint\nimport matplotlib.pyplot as plt\n\n# Load and preprocess data\nfeatures = [\"likeCount\", \"duration\", \"definition\", \"topicCategories\",\"popularity\"]\nX = data[features]\ny = data[\"viewCount\"]\n\n# Remove outliers using percentile\nlower_bound = y.quantile(0.01)\nupper_bound = y.quantile(0.99)\ndata = data[(y &gt;= lower_bound) & (y &lt;= upper_bound)]\n\nX = data[features]\ny = data[\"viewCount\"]\n\n# Feature engineering\nX.loc[:, \"topicCategories\"] = X[\"topicCategories\"].map(X[\"topicCategories\"].value_counts())\nX.loc[:, \"definition\"] = (X[\"definition\"] == \"hd\").astype(int)\nX.loc[:, \"popularity\"] = (X[\"popularity\"] == \"high\").astype(int)\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Random Forest hyperparameter search\nparam_dist = {\n    'n_estimators': randint(100, 500),  # Increase range for the number of trees\n    'max_depth': [10, 20, 30, None],   # Add 'None' for unlimited depth\n    'min_samples_split': randint(2, 20),\n    'min_samples_leaf': randint(1, 10),\n    'max_features': ['sqrt', 'log2', None],  # Add 'None' to consider all features\n    'bootstrap': [True, False],  # Use bootstrap sampling\n}\n\nclf = RandomForestRegressor(random_state=42)\nrandom_search = RandomizedSearchCV(\n    estimator=clf,\n    param_distributions=param_dist,\n    n_iter=100,  # Increase iterations for better parameter search\n    scoring='neg_mean_squared_error',\n    cv=3,\n    n_jobs=-1,\n    random_state=42,\n    verbose=0  # Show progress\n)\n\n# Fit the RandomizedSearchCV\nrandom_search.fit(X_train, y_train)\n\n# Best parameters\nprint(\"Best Parameters:\", random_search.best_params_)\n\n# Train the best model\nbest_clf = random_search.best_estimator_\nbest_clf.fit(X_train, y_train)\n\n# Predict\ny_pred = best_clf.predict(X_test)\n\n# Evaluate regression performance\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Regression Metrics:\")\nprint(f\"RMSE: {rmse}\")\nprint(f\"MAE: {mae}\")\nprint(f\"R-squared: {r2}\")\n\n# Visualization: Actual vs Predicted\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n         color='red', linestyle='--', label=\"Perfect Fit\")\nplt.xlabel(\"Actual View Count\")\nplt.ylabel(\"Predicted View Count\")\nplt.title(\"Actual vs Predicted View Counts\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nBest Parameters: {'bootstrap': True, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 5, 'n_estimators': 445}\nRegression Metrics:\nRMSE: 0.24950307870026947\nMAE: 0.10995794979659819\nR-squared: 0.7712702907389977"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#linear-regressor",
    "href": "technical-details/supervised-learning/main.html#linear-regressor",
    "title": "Supervised Learning",
    "section": "Linear Regressor",
    "text": "Linear Regressor\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport matplotlib.pyplot as plt\n\n# Load and preprocess data\nfeatures = [\"likeCount\", \"duration\", \"definition\", \"topicCategories\",\"popularity\"]\nX = data[features]\ny = data[\"viewCount\"]\n\n# Remove outliers using percentile\nlower_bound = y.quantile(0.01)\nupper_bound = y.quantile(0.99)\ndata = data[(y &gt;= lower_bound) & (y &lt;= upper_bound)]\n\nX = data[features]\ny = data[\"viewCount\"]\n\n# Handle missing values\nX = X.fillna(0)\n\n# Feature engineering\nX.loc[:, \"topicCategories\"] = X[\"topicCategories\"].map(X[\"topicCategories\"].value_counts())\nX.loc[:, \"definition\"] = (X[\"definition\"] == \"hd\").astype(int)\nX.loc[:, \"popularity\"] = (X[\"popularity\"] == \"high\").astype(int)\n# Standardize features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Hyperparameter tuning for Ridge Regression\nparam_grid = {\n    'alpha': [0.01, 0.1, 1, 10, 100, 1000],  # Regularization strength\n    'solver': ['auto', 'svd', 'cholesky', 'lsqr']  # Avoid sag and saga for stability\n}\n\nridge = Ridge(max_iter=5000)  # Increase max_iter for convergence\ngrid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\n# Best parameters\nprint(\"Best Parameters:\", grid_search.best_params_)\n\n# Train the best model\nbest_regressor = grid_search.best_estimator_\nbest_regressor.fit(X_train, y_train)\n\n# Predict\ny_pred = best_regressor.predict(X_test)\n\n# Evaluate regression performance\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Regression Metrics:\")\nprint(f\"RMSE: {rmse}\")\nprint(f\"MAE: {mae}\")\nprint(f\"R-squared: {r2}\")\n\n# Visualization: Actual vs Predicted\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n         color='red', linestyle='--', label=\"Perfect Fit\")\nplt.xlabel(\"Actual View Count\")\nplt.ylabel(\"Predicted View Count\")\nplt.title(\"Actual vs Predicted View Counts\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\nBest Parameters: {'alpha': 10, 'solver': 'auto'}\nRegression Metrics:\nRMSE: 0.2056597055110518\nMAE: 0.12407352920784746\nR-squared: 0.8032625833891794"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#gradient-boosting-regressor",
    "href": "technical-details/supervised-learning/main.html#gradient-boosting-regressor",
    "title": "Supervised Learning",
    "section": "Gradient Boosting Regressor",
    "text": "Gradient Boosting Regressor\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport matplotlib.pyplot as plt\n\n# Load and preprocess data\nfeatures = [\"likeCount\", \"duration\", \"definition\", \"topicCategories\",\"popularity\"]\nX = data[features]\ny = data[\"viewCount\"]\n\n# Remove outliers using percentile\nlower_bound = y.quantile(0.01)\nupper_bound = y.quantile(0.99)\ndata = data[(y &gt;= lower_bound) & (y &lt;= upper_bound)]\n\nX = data[features]\ny = data[\"viewCount\"]\n# Handle missing values and ensure proper data types\nX = X.fillna(0)\n\n# Feature engineering\nX.loc[:, \"topicCategories\"] = X[\"topicCategories\"].map(X[\"topicCategories\"].value_counts()).fillna(0).astype(int)\nX.loc[:, \"definition\"] = (X[\"definition\"] == \"hd\").astype(int)\n\nX[\"popularity\"] = X[\"popularity\"].fillna(\"low\")  # Fill missing values\nX[\"popularity\"] = (X[\"popularity\"] == \"high\").astype(int)  # Convert to binary\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Gradient Boosting hyperparameter search\nparam_grid = {\n    'n_estimators': [100, 200, 300],  # Number of boosting stages\n    'learning_rate': [0.01, 0.1, 0.2],  # Learning rate\n    'max_depth': [3, 5, 7],  # Maximum depth of the trees\n    'min_samples_split': [2, 5, 10],  # Minimum samples required to split an internal node\n    'min_samples_leaf': [1, 2, 4],  # Minimum samples required at a leaf node\n    'subsample': [0.8, 1.0],  # Fraction of samples used for fitting the individual base learners\n    'max_features': ['sqrt', 'log2', None]  # Number of features to consider for best split\n}\n\ngbr = GradientBoostingRegressor(random_state=42)\ngrid_search = GridSearchCV(\n    estimator=gbr,\n    param_grid=param_grid,\n    scoring='neg_mean_squared_error',\n    cv=3,\n    n_jobs=-1,\n    verbose=0\n)\n\n# Fit the GridSearchCV\ngrid_search.fit(X_train, y_train)\n\n# Best parameters\nprint(\"Best Parameters:\", grid_search.best_params_)\n\n# Train the best model\nbest_gbr = grid_search.best_estimator_\nbest_gbr.fit(X_train, y_train)\n\n# Predict\ny_pred = best_gbr.predict(X_test)\n\n# Evaluate regression performance\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Regression Metrics:\")\nprint(f\"RMSE: {rmse}\")\nprint(f\"MAE: {mae}\")\nprint(f\"R-squared: {r2}\")\n\n# Visualization: Actual vs Predicted\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n         color='red', linestyle='--', label=\"Perfect Fit\")\nplt.xlabel(\"Actual View Count\")\nplt.ylabel(\"Predicted View Count\")\nplt.title(\"Actual vs Predicted View Counts\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nBest Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 300, 'subsample': 0.8}\nRegression Metrics:\nRMSE: 0.12945966218516167\nMAE: 0.07980190234589664\nR-squared: 0.8727429441887932"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#random-forest-classifier",
    "href": "technical-details/supervised-learning/main.html#random-forest-classifier",
    "title": "Supervised Learning",
    "section": "Random Forest classifier",
    "text": "Random Forest classifier\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Features and target variable\nfeatures = [\"likeCount\", \"duration\", \"definition\", \"topicCategories\"]\nX = data[features]  # Feature matrix\nX = X.fillna(0)  # Replace NaN values with 0\n\ny = data[\"popularity\"]  # Target variable (labels)\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n# Ensure categorical features are encoded properly\nX = pd.get_dummies(X, columns=[\"definition\", \"topicCategories\"], drop_first=True)\n\n# Feature scaling\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Cross-validation setup\ncv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\nclf = LogisticRegression(random_state=42)\n\n# Initialize metric storage\naccuracies, precisions, recalls, f1_scores, roc_aucs = [], [], [], [], []\n\n# Cross-validation loop\nfor train_idx, test_idx in cv.split(X_scaled, y):\n    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n    y_train, y_test = np.array(y)[train_idx], np.array(y)[test_idx]\n    \n    clf.fit(X_train, y_train)  # Train the model\n    y_pred = clf.predict(X_test)  # Predict on test data\n    y_proba = clf.predict_proba(X_test)[:, 1]  # Predicted probabilities\n    \n    # Evaluate metrics\n    accuracies.append(accuracy_score(y_test, y_pred))\n    precisions.append(precision_score(y_test, y_pred, zero_division=0))\n    recalls.append(recall_score(y_test, y_pred, zero_division=0))\n    f1_scores.append(f1_score(y_test, y_pred, zero_division=0))\n    if len(np.unique(y_test)) &gt; 1:  # Check if both classes exist\n        roc_aucs.append(roc_auc_score(y_test, y_proba))\n    else:\n        roc_aucs.append(float(\"nan\"))\n\n# Print evaluation metrics\nprint(\"Accuracy:\", np.mean(accuracies))\nprint(\"Precision:\", np.mean(precisions))\nprint(\"Recall:\", np.mean(recalls))\nprint(\"F1 Score:\", np.mean(f1_scores))\nprint(\"ROC AUC:\", np.nanmean(roc_aucs))\n\n# Confusion matrix visualization\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plot the confusion matrix with more readable labels\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n\n# Add titles and labels\nplt.title(\"Confusion Matrix\", fontsize=16)\nplt.xlabel(\"Predicted\", fontsize=14)\nplt.ylabel(\"Actual\", fontsize=14)\n\n# Show the plot\nplt.show()\n\nAccuracy: 0.9410058027079304\nPrecision: 0.9514721919302072\nRecall: 0.9814398200224972\nF1 Score: 0.9662236987818382\nROC AUC: 0.9437686668476786"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#logistic-regression",
    "href": "technical-details/supervised-learning/main.html#logistic-regression",
    "title": "Supervised Learning",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Binary classification using Logistic Regression\n\n# Features and target variable\nfeatures = [\"likeCount\", \"duration\", \"definition\", \"topicCategories\"]\nX = data[features]  # Feature matrix\nX = X.fillna(0)  # Replace NaN values with 0\n\ny = data[\"popularity\"]  # Target variable (labels)\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n# Ensure categorical features are encoded properly\nX = pd.get_dummies(X, columns=[\"definition\", \"topicCategories\"], drop_first=True)\n\n# Feature scaling\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Cross-validation setup\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nclf = LogisticRegression(random_state=42)\n\n# Initialize metric storage\naccuracies, precisions, recalls, f1_scores, roc_aucs = [], [], [], [], []\n\n# Cross-validation loop\nfor train_idx, test_idx in cv.split(X_scaled, y):\n    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n    y_train, y_test = np.array(y)[train_idx], np.array(y)[test_idx]\n    \n    clf.fit(X_train, y_train)  # Train the model\n    y_pred = clf.predict(X_test)  # Predict on test data\n    y_proba = clf.predict_proba(X_test)[:, 1]  # Predicted probabilities\n    \n    # Evaluate metrics\n    accuracies.append(accuracy_score(y_test, y_pred))\n    precisions.append(precision_score(y_test, y_pred, zero_division=0))\n    recalls.append(recall_score(y_test, y_pred, zero_division=0))\n    f1_scores.append(f1_score(y_test, y_pred, zero_division=0))\n    roc_aucs.append(roc_auc_score(y_test, y_proba))\n\n# Print evaluation metrics\nprint(\"Binary Classification Evaluation:\")\nprint(\"Accuracy:\", np.mean(accuracies))\nprint(\"Precision:\", np.mean(precisions))\nprint(\"Recall:\", np.mean(recalls))\nprint(\"F1 Score:\", np.mean(f1_scores))\nprint(\"ROC AUC:\", np.mean(roc_aucs))\n\n# Confusion matrix visualization\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plot the confusion matrix with more readable labels\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n\n# Add titles and labels\nplt.title(\"Confusion Matrix\", fontsize=16)\nplt.xlabel(\"Predicted\", fontsize=14)\nplt.ylabel(\"Actual\", fontsize=14)\n\n# Show the plot\nplt.show()\n\nBinary Classification Evaluation:\nAccuracy: 0.9371454305131535\nPrecision: 0.9513389422480332\nRecall: 0.9769488843171388\nF1 Score: 0.9639416586669458\nROC AUC: 0.9439163769911214"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#gradient-boosting-classifier",
    "href": "technical-details/supervised-learning/main.html#gradient-boosting-classifier",
    "title": "Supervised Learning",
    "section": "Gradient Boosting Classifier",
    "text": "Gradient Boosting Classifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Features and target variable\nfeatures = [\"likeCount\", \"duration\", \"definition\", \"topicCategories\"]\nX = data[features]  # Feature matrix\nX = X.fillna(0)  # Replace NaN values with 0\n\ny = data[\"popularity\"]  # Target variable (binary labels)\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n# Ensure categorical features are encoded properly\nX = pd.get_dummies(X, columns=[\"definition\", \"topicCategories\"], drop_first=True)\n\n# Feature scaling\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Cross-validation setup\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nclf = GradientBoostingClassifier(random_state=42)\n\n# Initialize metric storage\naccuracies, precisions, recalls, f1_scores, roc_aucs = [], [], [], [], []\n\n# Cross-validation loop\nfor train_idx, test_idx in cv.split(X_scaled, y):\n    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n    y_train, y_test = np.array(y)[train_idx], np.array(y)[test_idx]\n    \n    # Train the Gradient Boosting Classifier\n    clf.fit(X_train, y_train)\n    \n    # Predictions\n    y_pred = clf.predict(X_test)\n    y_proba = clf.predict_proba(X_test)[:, 1]  # Probability estimates for the positive class\n    \n    # Evaluate metrics\n    accuracies.append(accuracy_score(y_test, y_pred))\n    precisions.append(precision_score(y_test, y_pred, zero_division=0))\n    recalls.append(recall_score(y_test, y_pred, zero_division=0))\n    f1_scores.append(f1_score(y_test, y_pred, zero_division=0))\n    if len(np.unique(y_test)) &gt; 1:  # Ensure both classes exist in y_test\n        roc_aucs.append(roc_auc_score(y_test, y_proba))\n    else:\n        roc_aucs.append(float(\"nan\"))\n\n# Print evaluation metrics\nprint(\"Gradient Boosting Classifier - Binary Classification\")\nprint(\"Accuracy:\", np.mean(accuracies))\nprint(\"Precision:\", np.mean(precisions))\nprint(\"Recall:\", np.mean(recalls))\nprint(\"F1 Score:\", np.mean(f1_scores))\nprint(\"ROC AUC:\", np.mean(roc_aucs))\n# Confusion matrix visualization\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plot the confusion matrix with more readable labels\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n\n# Add titles and labels\nplt.title(\"Confusion Matrix - Gradient Boosting Classifier\", fontsize=16)\nplt.xlabel(\"Predicted\", fontsize=14)\nplt.ylabel(\"Actual\", fontsize=14)\n\n# Show the plot\nplt.show()\n\nGradient Boosting Classifier - Binary Classification\nAccuracy: 0.9434303026049526\nPrecision: 0.9591385728194808\nRecall: 0.9758205412248774\nF1 Score: 0.9673761902109981\nROC AUC: 0.9679896016938516"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#gradient-boosting-classifier-1",
    "href": "technical-details/supervised-learning/main.html#gradient-boosting-classifier-1",
    "title": "Supervised Learning",
    "section": "Gradient Boosting Classifier",
    "text": "Gradient Boosting Classifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Features and target variable\nfeatures = [\"likeCount\", \"duration\", \"definition\", \"topicCategories\"]\nX = data[features]  # Feature matrix\nX = X.fillna(0)  # Replace NaN values with 0\n\ny = data[\"popularity\"]  # Target variable (binary labels)\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n# Ensure categorical features are encoded properly\nX = pd.get_dummies(X, columns=[\"definition\", \"topicCategories\"], drop_first=True)\n\n# Feature scaling\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Hyperparameter search space for Gradient Boosting Classifier\nparam_grid = {\n    'n_estimators': [100, 200, 300],         # Number of boosting stages\n    'learning_rate': [0.01, 0.1, 0.2],      # Learning rate\n    'max_depth': [3, 5, 7],                 # Maximum depth of each tree\n    'min_samples_split': [2, 5, 10],        # Minimum samples required to split an internal node\n    'min_samples_leaf': [1, 2, 4],          # Minimum samples required at a leaf node\n    'subsample': [0.8, 1.0],                # Fraction of samples used for fitting individual trees\n    'max_features': ['sqrt', 'log2', None]  # Number of features considered when looking for the best split\n}\n\n# Cross-validation setup\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Hyperparameter tuning with GridSearchCV\nclf = GradientBoostingClassifier(random_state=42)\ngrid_search = GridSearchCV(\n    estimator=clf,\n    param_grid=param_grid,\n    scoring='accuracy',\n    cv=cv,\n    n_jobs=-1,\n    verbose=1\n)\n\n# Perform the grid search\ngrid_search.fit(X_scaled, y)\n\n# Best hyperparameters\nbest_params = grid_search.best_params_\nprint(\"Best Parameters:\", best_params)\n\n# Train the best model on the entire dataset\nbest_gbc = grid_search.best_estimator_\n\n# Initialize metric storage for cross-validation evaluation\naccuracies, precisions, recalls, f1_scores, roc_aucs = [], [], [], [], []\n\n# Cross-validation evaluation with best parameters\nfor train_idx, test_idx in cv.split(X_scaled, y):\n    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    \n    best_gbc.fit(X_train, y_train)  # Train the best model\n    y_pred = best_gbc.predict(X_test)  # Predict on test data\n    y_proba = best_gbc.predict_proba(X_test)[:, 1]  # Predicted probabilities\n    \n    # Evaluate metrics\n    accuracies.append(accuracy_score(y_test, y_pred))\n    precisions.append(precision_score(y_test, y_pred, zero_division=0))\n    recalls.append(recall_score(y_test, y_pred, zero_division=0))\n    f1_scores.append(f1_score(y_test, y_pred, zero_division=0))\n    roc_aucs.append(roc_auc_score(y_test, y_proba))\n\n# Print evaluation metrics\nprint(\"\\nGradient Boosting Classifier - Binary Classification\")\nprint(\"Accuracy:\", np.mean(accuracies))\nprint(\"Precision:\", np.mean(precisions))\nprint(\"Recall:\", np.mean(recalls))\nprint(\"F1 Score:\", np.mean(f1_scores))\nprint(\"ROC AUC:\", np.mean(roc_aucs))\n\n# Confusion matrix visualization\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plot the confusion matrix with more readable labels\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n\n# Add titles and labels\nplt.title(\"Confusion Matrix - Gradient Boosting Classifier\", fontsize=16)\nplt.xlabel(\"Predicted\", fontsize=14)\nplt.ylabel(\"Actual\", fontsize=14)\n\n# Show the plot\nplt.show()\n\n\nFitting 5 folds for each of 1458 candidates, totalling 7290 fits\nBest Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300, 'subsample': 0.8}\n\nGradient Boosting Classifier - Binary Classification\nAccuracy: 0.9516316973788237\nPrecision: 0.9642341340048605\nRecall: 0.9806494431494432\nF1 Score: 0.9723572674490519\nROC AUC: 0.9695507727270917"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#decision-tree-classifier",
    "href": "technical-details/supervised-learning/main.html#decision-tree-classifier",
    "title": "Supervised Learning",
    "section": "Decision Tree Classifier",
    "text": "Decision Tree Classifier\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, log_loss, cohen_kappa_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Features and target variable\nfeatures = [\"likeCount\", \"duration\", \"definition\", \"topicCategories\"]\nX = data[features]  # Feature matrix\nX = X.fillna(0)  # Replace NaN values with 0\n\ny = data[\"popularity_multi_class\"]  # Target variable (multi-class labels)\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n# Ensure categorical features are encoded properly\nX = pd.get_dummies(X, columns=[\"definition\", \"topicCategories\"], drop_first=True)\n\n# Feature scaling (not strictly needed for Decision Tree but kept for consistency)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Cross-validation setup\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Hyperparameter grid for DecisionTreeClassifier\nparam_grid = {\n    \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n    \"max_depth\": [None, 5, 10, 20, 30],\n    \"min_samples_split\": [2, 5, 10],\n    \"min_samples_leaf\": [1, 2, 4],\n    \"max_features\": [None, \"sqrt\", \"log2\"]\n}\n\n# Decision Tree Classifier\nclf = DecisionTreeClassifier(random_state=42)\n\n# GridSearchCV for hyperparameter tuning\ngrid_search = GridSearchCV(\n    estimator=clf,\n    param_grid=param_grid,\n    scoring=\"accuracy\",\n    cv=cv,\n    n_jobs=-1,\n    verbose=1\n)\n\n# Fit GridSearchCV\ngrid_search.fit(X_scaled, y)\n\n# Best hyperparameters\nbest_params = grid_search.best_params_\nprint(\"Best Parameters:\", best_params)\n\n# Use the best model\nbest_clf = grid_search.best_estimator_\n\n# Initialize metric storage for evaluation\naccuracies, precisions_macro, recalls_macro, f1_scores_macro = [], [], [], []\nprecisions_weighted, recalls_weighted, f1_scores_weighted = [], [], []\nlog_losses, kappa_scores = [], []\n\n# Cross-validation loop with the best model\nfor train_idx, test_idx in cv.split(X_scaled, y):\n    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    \n    # Train the best Decision Tree Classifier\n    best_clf.fit(X_train, y_train)\n    \n    # Predictions\n    y_pred = best_clf.predict(X_test)\n    y_proba = best_clf.predict_proba(X_test)  # Probability estimates for all classes\n    \n    # Evaluate metrics\n    accuracies.append(accuracy_score(y_test, y_pred))\n    precisions_macro.append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n    recalls_macro.append(recall_score(y_test, y_pred, average='macro', zero_division=0))\n    f1_scores_macro.append(f1_score(y_test, y_pred, average='macro', zero_division=0))\n    \n    precisions_weighted.append(precision_score(y_test, y_pred, average='weighted', zero_division=0))\n    recalls_weighted.append(recall_score(y_test, y_pred, average='weighted', zero_division=0))\n    f1_scores_weighted.append(f1_score(y_test, y_pred, average='weighted', zero_division=0))\n    \n    log_losses.append(log_loss(y_test, y_proba))\n    kappa_scores.append(cohen_kappa_score(y_test, y_pred))\n\n# Print evaluation metrics\nprint(\"Decision Tree Classifier with Hyperparameter Tuning - Multi-Class Classification\")\nprint(\"===========================================================================\")\nprint(f\"Accuracy: {np.mean(accuracies)}\")\nprint(f\"Macro Precision: {np.mean(precisions_macro)}\")\nprint(f\"Macro Recall: {np.mean(recalls_macro)}\")\nprint(f\"Macro F1 Score: {np.mean(f1_scores_macro)}\")\nprint(f\"Weighted Precision: {np.mean(precisions_weighted)}\")\nprint(f\"Weighted Recall: {np.mean(recalls_weighted)}\")\nprint(f\"Weighted F1 Score: {np.mean(f1_scores_weighted)}\")\nprint(f\"Log Loss: {np.mean(log_losses)}\")\nprint(f\"Cohen Kappa Score: {np.mean(kappa_scores)}\")\n\n# Confusion matrix visualization\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plot the confusion matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n\n# Add titles and labels\nplt.title(\"Confusion Matrix - Multi-Class Decision Tree (Tuned)\", fontsize=16)\nplt.xlabel(\"Predicted\", fontsize=14)\nplt.ylabel(\"Actual\", fontsize=14)\n\n# Show the plot\nplt.show()\n\n# Detailed classification report\nprint(\"\\nClassification Report:\\n\")\nprint(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n\nFitting 5 folds for each of 405 candidates, totalling 2025 fits\nBest Parameters: {'criterion': 'gini', 'max_depth': 5, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\nDecision Tree Classifier with Hyperparameter Tuning - Multi-Class Classification\n===========================================================================\nAccuracy: 0.8401339929911359\nMacro Precision: 0.833451423276734\nMacro Recall: 0.8258330198476095\nMacro F1 Score: 0.828370209357443\nWeighted Precision: 0.8393017350579939\nWeighted Recall: 0.8401339929911359\nWeighted F1 Score: 0.8386803790157578\nLog Loss: 0.7277971131663302\nCohen Kappa Score: 0.7402908330435282\n\n\n\n\n\n\n\n\n\n\nClassification Report:\n\n              precision    recall  f1-score   support\n\n        high       0.83      0.80      0.81        71\n         low       0.84      0.91      0.87       204\n      medium       0.78      0.72      0.75       165\n\n    accuracy                           0.82       440\n   macro avg       0.82      0.81      0.81       440\nweighted avg       0.82      0.82      0.82       440"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#k-nearest-neighbors-knn",
    "href": "technical-details/supervised-learning/main.html#k-nearest-neighbors-knn",
    "title": "Supervised Learning",
    "section": "K-Nearest Neighbors (KNN)",
    "text": "K-Nearest Neighbors (KNN)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, log_loss, cohen_kappa_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Features and target variable\nfeatures = [\"likeCount\", \"duration\", \"definition\", \"topicCategories\"]\nX = data[features]  # Feature matrix\nX = X.fillna(0)  # Replace NaN values with 0\n\ny = data[\"popularity\"]  # Target variable (multi-class labels)\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n# Ensure categorical features are encoded properly\nX = pd.get_dummies(X, columns=[\"definition\", \"topicCategories\"], drop_first=True)\n\n# Feature scaling (important for KNN because it relies on distance metrics)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Cross-validation setup\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nclf = KNeighborsClassifier(n_neighbors=5)  # K=5 is a common default\n\n# Initialize metric storage\naccuracies, precisions_macro, recalls_macro, f1_scores_macro = [], [], [], []\nprecisions_weighted, recalls_weighted, f1_scores_weighted = [], [], []\nlog_losses, kappa_scores = [], []\n\n# Cross-validation loop\nfor train_idx, test_idx in cv.split(X_scaled, y):\n    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    \n    # Train the K-Nearest Neighbors Classifier\n    clf.fit(X_train, y_train)\n    \n    # Predictions\n    y_pred = clf.predict(X_test)\n    y_proba = clf.predict_proba(X_test)  # Probability estimates for all classes\n    \n    # Evaluate metrics\n    accuracies.append(accuracy_score(y_test, y_pred))\n    precisions_macro.append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n    recalls_macro.append(recall_score(y_test, y_pred, average='macro', zero_division=0))\n    f1_scores_macro.append(f1_score(y_test, y_pred, average='macro', zero_division=0))\n    \n    precisions_weighted.append(precision_score(y_test, y_pred, average='weighted', zero_division=0))\n    recalls_weighted.append(recall_score(y_test, y_pred, average='weighted', zero_division=0))\n    f1_scores_weighted.append(f1_score(y_test, y_pred, average='weighted', zero_division=0))\n    \n    log_losses.append(log_loss(y_test, y_proba))\n    kappa_scores.append(cohen_kappa_score(y_test, y_pred))\n\n# Print evaluation metrics\nprint(\"K-Nearest Neighbors (KNN) - Multi-Class Classification\")\nprint(\"=====================================================\")\nprint(f\"Accuracy: {np.mean(accuracies):.4f}\")\nprint(f\"Macro Precision: {np.mean(precisions_macro):.4f}\")\nprint(f\"Macro Recall: {np.mean(recalls_macro):.4f}\")\nprint(f\"Macro F1 Score: {np.mean(f1_scores_macro):.4f}\")\nprint(f\"Weighted Precision: {np.mean(precisions_weighted):.4f}\")\nprint(f\"Weighted Recall: {np.mean(recalls_weighted):.4f}\")\nprint(f\"Weighted F1 Score: {np.mean(f1_scores_weighted):.4f}\")\nprint(f\"Log Loss: {np.mean(log_losses):.4f}\")\nprint(f\"Cohen Kappa Score: {np.mean(kappa_scores):.4f}\")\n\n# Confusion matrix visualization\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plot the confusion matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n\n# Add titles and labels\nplt.title(\"Confusion Matrix - Multi-Class KNN\", fontsize=16)\nplt.xlabel(\"Predicted\", fontsize=14)\nplt.ylabel(\"Actual\", fontsize=14)\n\n# Show the plot\nplt.show()\n\n# Detailed classification report\nprint(\"\\nClassification Report:\\n\")\nprint(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n\nK-Nearest Neighbors (KNN) - Multi-Class Classification\n=====================================================\nAccuracy: 0.9294\nMacro Precision: 0.9006\nMacro Recall: 0.7872\nMacro F1 Score: 0.8293\nWeighted Precision: 0.9264\nWeighted Recall: 0.9294\nWeighted F1 Score: 0.9233\nLog Loss: 1.1397\nCohen Kappa Score: 0.6605\n\n\n\n\n\n\n\n\n\n\nClassification Report:\n\n              precision    recall  f1-score   support\n\n        high       0.91      0.53      0.67        58\n         low       0.93      0.99      0.96       355\n\n    accuracy                           0.93       413\n   macro avg       0.92      0.76      0.82       413\nweighted avg       0.93      0.93      0.92       413"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#binary-classification-section",
    "href": "technical-details/supervised-learning/main.html#binary-classification-section",
    "title": "Supervised Learning",
    "section": "Binary Classification Section",
    "text": "Binary Classification Section\n\nRandom Forest Classifier\n\nWhat it is:\nAn ensemble learning method for classification tasks that aggregates predictions from multiple decision trees.\nHow it works:\nEach tree votes for a class, and the majority vote determines the final output.\nInputs:\n\nlikeCount: Numeric (e.g., 12,000).\n\nduration: Numeric (e.g., 300 seconds).\n\ndefinition: Binary encoded (1 for hd, 0 for sd).\n\ntopicCategories: One-hot encoded or frequency counts.\n\nOutputs:\nBinary class labels (0 for low, 1 for high popularity).\nKey Hyperparameters:\n\nn_estimators: Number of trees.\n\nmax_depth: Tree depth.\n\nmin_samples_split: Minimum samples required to split a node.\n\n\n\n\n\nLogistic Regression\n\nWhat it is:\nA statistical method for binary classification tasks.\nHow it works:\nEstimates probabilities using a logistic function and classifies based on a threshold (default: 0.5).\nInputs:\n\nlikeCount: Numeric (e.g., 12,000).\n\nduration: Numeric (e.g., 300 seconds).\n\ndefinition: Binary encoded (1 for hd, 0 for sd).\n\ntopicCategories: One-hot encoded.\n\nOutputs:\n\nClass Labels: 0 or 1.\n\nClass Probabilities: Probabilities for each class.\nExample: For the above inputs, the model might predict 1 with a probability of 0.85.\n\nKey Hyperparameters:\n\npenalty: Regularization type (l1, l2, etc.).\n\nC: Inverse of regularization strength.\n\nmax_iter: Maximum iterations.\n\n\n\n\n\nGradient Boosting Classifier\n\nWhat it is:\nAn ensemble method combining decision trees to classify multi-class problems effectively.\nHow it works:\nSequentially trains trees to minimize classification error, with predictions as weighted averages.\nInputs:\n\nlikeCount: Numeric.\n\nduration: Numeric.\n\ndefinition: Binary encoded.\n\ntopicCategories: One-hot encoded.\n\nOutputs:\n\nClass Labels: Predicted categories.\n\nClass Probabilities: Probabilities for each class.\n\nKey Hyperparameters:\n\nn_estimators: Number of trees.\n\nlearning_rate: Step size for boosting iterations.\n\nsubsample: Fraction of data used per tree."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#random-forest-regressor-1",
    "href": "technical-details/supervised-learning/main.html#random-forest-regressor-1",
    "title": "Supervised Learning",
    "section": "Random Forest Regressor",
    "text": "Random Forest Regressor\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom scipy.stats import randint\nimport matplotlib.pyplot as plt\n\n# Load and preprocess data\nfeatures = [\"likeCount\", \"duration\", \"definition\", \"topicCategories\",\"popularity\"]\nX = data[features]\ny = data[\"viewCount\"]\n\n# Remove outliers using percentile\nlower_bound = y.quantile(0.01)\nupper_bound = y.quantile(0.99)\ndata = data[(y &gt;= lower_bound) & (y &lt;= upper_bound)]\n\nX = data[features]\ny = data[\"viewCount\"]\n\n# Feature engineering\nX.loc[:, \"topicCategories\"] = X[\"topicCategories\"].map(X[\"topicCategories\"].value_counts())\nX.loc[:, \"definition\"] = (X[\"definition\"] == \"hd\").astype(int)\nX.loc[:, \"popularity\"] = (X[\"popularity\"] == \"high\").astype(int)\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Random Forest hyperparameter search\nparam_dist = {\n    'n_estimators': randint(100, 500),  # Increase range for the number of trees\n    'max_depth': [10, 20, 30, None],   # Add 'None' for unlimited depth\n    'min_samples_split': randint(2, 20),\n    'min_samples_leaf': randint(1, 10),\n    'max_features': ['sqrt', 'log2', None],  # Add 'None' to consider all features\n    'bootstrap': [True, False],  # Use bootstrap sampling\n}\n\nclf = RandomForestRegressor(random_state=42)\nrandom_search = RandomizedSearchCV(\n    estimator=clf,\n    param_distributions=param_dist,\n    n_iter=100,  # Increase iterations for better parameter search\n    scoring='neg_mean_squared_error',\n    cv=3,\n    n_jobs=-1,\n    random_state=42,\n    verbose=0  # Show progress\n)\n\n# Fit the RandomizedSearchCV\nrandom_search.fit(X_train, y_train)\n\n# Best parameters\nprint(\"Best Parameters:\", random_search.best_params_)\n\n# Train the best model\nbest_clf = random_search.best_estimator_\nbest_clf.fit(X_train, y_train)\n\n# Predict\ny_pred = best_clf.predict(X_test)\n\n# Evaluate regression performance\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Regression Metrics:\")\nprint(f\"RMSE: {rmse}\")\nprint(f\"MAE: {mae}\")\nprint(f\"R-squared: {r2}\")\n\n# Visualization: Actual vs Predicted\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n         color='red', linestyle='--', label=\"Perfect Fit\")\nplt.xlabel(\"Actual View Count\")\nplt.ylabel(\"Predicted View Count\")\nplt.title(\"Actual vs Predicted View Counts\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nBest Parameters: {'bootstrap': True, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 5, 'n_estimators': 445}\nRegression Metrics:\nRMSE: 0.24950307870026947\nMAE: 0.10995794979659819\nR-squared: 0.7712702907389977"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#linear-regressor-1",
    "href": "technical-details/supervised-learning/main.html#linear-regressor-1",
    "title": "Supervised Learning",
    "section": "Linear Regressor",
    "text": "Linear Regressor\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport matplotlib.pyplot as plt\n\n# Load and preprocess data\nfeatures = [\"likeCount\", \"duration\", \"definition\", \"topicCategories\",\"popularity\"]\nX = data[features]\ny = data[\"viewCount\"]\n\n# Remove outliers using percentile\nlower_bound = y.quantile(0.01)\nupper_bound = y.quantile(0.99)\ndata = data[(y &gt;= lower_bound) & (y &lt;= upper_bound)]\n\nX = data[features]\ny = data[\"viewCount\"]\n\n# Handle missing values\nX = X.fillna(0)\n\n# Feature engineering\nX.loc[:, \"topicCategories\"] = X[\"topicCategories\"].map(X[\"topicCategories\"].value_counts())\nX.loc[:, \"definition\"] = (X[\"definition\"] == \"hd\").astype(int)\nX.loc[:, \"popularity\"] = (X[\"popularity\"] == \"high\").astype(int)\n# Standardize features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Hyperparameter tuning for Ridge Regression\nparam_grid = {\n    'alpha': [0.01, 0.1, 1, 10, 100, 1000],  # Regularization strength\n    'solver': ['auto', 'svd', 'cholesky', 'lsqr']  # Avoid sag and saga for stability\n}\n\nridge = Ridge(max_iter=5000)  # Increase max_iter for convergence\ngrid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\n# Best parameters\nprint(\"Best Parameters:\", grid_search.best_params_)\n\n# Train the best model\nbest_regressor = grid_search.best_estimator_\nbest_regressor.fit(X_train, y_train)\n\n# Predict\ny_pred = best_regressor.predict(X_test)\n\n# Evaluate regression performance\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Regression Metrics:\")\nprint(f\"RMSE: {rmse}\")\nprint(f\"MAE: {mae}\")\nprint(f\"R-squared: {r2}\")\n\n# Visualization: Actual vs Predicted\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n         color='red', linestyle='--', label=\"Perfect Fit\")\nplt.xlabel(\"Actual View Count\")\nplt.ylabel(\"Predicted View Count\")\nplt.title(\"Actual vs Predicted View Counts\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\nBest Parameters: {'alpha': 10, 'solver': 'auto'}\nRegression Metrics:\nRMSE: 0.2056597055110518\nMAE: 0.12407352920784746\nR-squared: 0.8032625833891794"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#gradient-boosting-regressor-1",
    "href": "technical-details/supervised-learning/main.html#gradient-boosting-regressor-1",
    "title": "Supervised Learning",
    "section": "Gradient Boosting Regressor",
    "text": "Gradient Boosting Regressor\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport matplotlib.pyplot as plt\n\n# Load and preprocess data\nfeatures = [\"likeCount\", \"duration\", \"definition\", \"topicCategories\",\"popularity\"]\nX = data[features]\ny = data[\"viewCount\"]\n\n# Remove outliers using percentile\nlower_bound = y.quantile(0.01)\nupper_bound = y.quantile(0.99)\ndata = data[(y &gt;= lower_bound) & (y &lt;= upper_bound)]\n\nX = data[features]\ny = data[\"viewCount\"]\n# Handle missing values and ensure proper data types\nX = X.fillna(0)\n\n# Feature engineering\nX.loc[:, \"topicCategories\"] = X[\"topicCategories\"].map(X[\"topicCategories\"].value_counts()).fillna(0).astype(int)\nX.loc[:, \"definition\"] = (X[\"definition\"] == \"hd\").astype(int)\n\nX[\"popularity\"] = X[\"popularity\"].fillna(\"low\")  # Fill missing values\nX[\"popularity\"] = (X[\"popularity\"] == \"high\").astype(int)  # Convert to binary\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Gradient Boosting hyperparameter search\nparam_grid = {\n    'n_estimators': [100, 200, 300],  # Number of boosting stages\n    'learning_rate': [0.01, 0.1, 0.2],  # Learning rate\n    'max_depth': [3, 5, 7],  # Maximum depth of the trees\n    'min_samples_split': [2, 5, 10],  # Minimum samples required to split an internal node\n    'min_samples_leaf': [1, 2, 4],  # Minimum samples required at a leaf node\n    'subsample': [0.8, 1.0],  # Fraction of samples used for fitting the individual base learners\n    'max_features': ['sqrt', 'log2', None]  # Number of features to consider for best split\n}\n\ngbr = GradientBoostingRegressor(random_state=42)\ngrid_search = GridSearchCV(\n    estimator=gbr,\n    param_grid=param_grid,\n    scoring='neg_mean_squared_error',\n    cv=3,\n    n_jobs=-1,\n    verbose=0\n)\n\n# Fit the GridSearchCV\ngrid_search.fit(X_train, y_train)\n\n# Best parameters\nprint(\"Best Parameters:\", grid_search.best_params_)\n\n# Train the best model\nbest_gbr = grid_search.best_estimator_\nbest_gbr.fit(X_train, y_train)\n\n# Predict\ny_pred = best_gbr.predict(X_test)\n\n# Evaluate regression performance\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Regression Metrics:\")\nprint(f\"RMSE: {rmse}\")\nprint(f\"MAE: {mae}\")\nprint(f\"R-squared: {r2}\")\n\n# Visualization: Actual vs Predicted\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n         color='red', linestyle='--', label=\"Perfect Fit\")\nplt.xlabel(\"Actual View Count\")\nplt.ylabel(\"Predicted View Count\")\nplt.title(\"Actual vs Predicted View Counts\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nBest Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 300, 'subsample': 0.8}\nRegression Metrics:\nRMSE: 0.12945966218516167\nMAE: 0.07980190234589664\nR-squared: 0.8727429441887932"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#random-forest-classifier-1",
    "href": "technical-details/supervised-learning/main.html#random-forest-classifier-1",
    "title": "Supervised Learning",
    "section": "Random Forest classifier",
    "text": "Random Forest classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Features and target variable\nfeatures = [\"likeCount\", \"duration\", \"definition\", \"topicCategories\"]\nX = data[features]  # Feature matrix\nX = X.fillna(0)  # Replace NaN values with 0\n\ny = data[\"popularity\"]  # Target variable (labels)\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n# Ensure categorical features are encoded properly\nX = pd.get_dummies(X, columns=[\"definition\", \"topicCategories\"], drop_first=True)\n\n# Feature scaling (not strictly necessary for Random Forest)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Hyperparameter search space for Random Forest\nparam_grid = {\n    'n_estimators': [100, 200, 300],        # Number of trees\n    'max_depth': [None, 10, 20, 30],       # Maximum depth of trees\n    'min_samples_split': [2, 5, 10],       # Minimum samples to split a node\n    'min_samples_leaf': [1, 2, 4],         # Minimum samples at a leaf node\n    'max_features': ['sqrt', 'log2', None],# Features to consider for the best split\n    'class_weight': [None, 'balanced'],    # Class weights for imbalance\n    'bootstrap': [True, False]             # Whether to use bootstrapping\n}\n\n# Cross-validation setup\ncv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n\n# Initialize the Random Forest Classifier\nclf = RandomForestClassifier(random_state=42)\n\n# Hyperparameter tuning with GridSearchCV\ngrid_search = GridSearchCV(\n    estimator=clf,\n    param_grid=param_grid,\n    scoring='accuracy',\n    cv=cv,\n    n_jobs=-1,\n    verbose=1\n)\n\n# Perform the grid search\ngrid_search.fit(X_scaled, y)\n\n# Best hyperparameters\nbest_params = grid_search.best_params_\nprint(\"Best Parameters:\", best_params)\n\n# Train the best model on the entire dataset\nbest_rf = grid_search.best_estimator_\n\n# Initialize metric storage for cross-validation evaluation\naccuracies, precisions, recalls, f1_scores, roc_aucs = [], [], [], [], []\n\n# Cross-validation evaluation with best parameters\nfor train_idx, test_idx in cv.split(X_scaled, y):\n    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n    y_train, y_test = np.array(y)[train_idx], np.array(y)[test_idx]\n    \n    best_rf.fit(X_train, y_train)  # Train the best model\n    y_pred = best_rf.predict(X_test)  # Predict on test data\n    y_proba = best_rf.predict_proba(X_test)[:, 1]  # Predicted probabilities for the positive class\n    \n    # Evaluate metrics\n    accuracies.append(accuracy_score(y_test, y_pred))\n    precisions.append(precision_score(y_test, y_pred, zero_division=0))\n    recalls.append(recall_score(y_test, y_pred, zero_division=0))\n    f1_scores.append(f1_score(y_test, y_pred, zero_division=0))\n    if len(np.unique(y_test)) &gt; 1:  # Check if both classes exist\n        roc_aucs.append(roc_auc_score(y_test, y_proba))\n    else:\n        roc_aucs.append(float(\"nan\"))\n\n# Print evaluation metrics\nprint(\"\\nRandom Forest Classifier - Binary Classification Metrics:\")\nprint(\"Accuracy:\", np.mean(accuracies))\nprint(\"Precision:\", np.mean(precisions))\nprint(\"Recall:\", np.mean(recalls))\nprint(\"F1 Score:\", np.mean(f1_scores))\nprint(\"ROC AUC:\", np.nanmean(roc_aucs))\n\n# Confusion matrix visualization\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plot the confusion matrix with more readable labels\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n\n# Add titles and labels\nplt.title(\"Confusion Matrix - Random Forest Classifier\", fontsize=16)\nplt.xlabel(\"Predicted\", fontsize=14)\nplt.ylabel(\"Actual\", fontsize=14)\n\n# Show the plot\nplt.show()\n\n\n\nFitting 3 folds for each of 1296 candidates, totalling 3888 fits\nBest Parameters: {'bootstrap': True, 'class_weight': None, 'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 300}\n\nRandom Forest Classifier - Binary Classification Metrics:\nAccuracy: 0.9496530060632625\nPrecision: 0.9626261725812437\nRecall: 0.9800851783786938\nF1 Score: 0.9712351897838586\nROC AUC: 0.9642452763319597"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#logistic-regression-1",
    "href": "technical-details/supervised-learning/main.html#logistic-regression-1",
    "title": "Supervised Learning",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nX = data[[\"likeCount\", \"duration\", \"definition\", \"topicCategories\"]].copy()\n\n\nX[\"likeCount\"] = X[\"likeCount\"].fillna(X[\"likeCount\"].median())\nX[\"duration\"] = X[\"duration\"].fillna(X[\"duration\"].median())\nX = X.fillna(\"missing\")  \ny = data[\"popularity\"]\n\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\nX = pd.get_dummies(X, columns=[\"definition\", \"topicCategories\"], drop_first=True)\n\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n\nparam_grid = [\n    {'penalty': ['l2'], 'C': [0.01, 0.1, 1, 10, 100], 'solver': ['lbfgs'], 'max_iter': [5000]},\n    {'penalty': ['l1'], 'C': [0.01, 0.1, 1, 10, 100], 'solver': ['liblinear'], 'max_iter': [5000]},\n    {'penalty': ['elasticnet'], 'C': [0.01, 0.1, 1, 10, 100], 'solver': ['saga'], 'l1_ratio': [0.1, 0.5, 0.9], 'max_iter': [5000]}\n]\n\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n\nclf = LogisticRegression(random_state=42)\n\n\ngrid_search = GridSearchCV(\n    estimator=clf,\n    param_grid=param_grid,\n    scoring='accuracy',\n    cv=cv,\n    n_jobs=-1,\n    verbose=0\n)\n\n\ngrid_search.fit(X_scaled, y)\n\n\nbest_params = grid_search.best_params_\nprint(\"Best Parameters:\", best_params)\n\n\nbest_lr = grid_search.best_estimator_\nmetrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': [], 'roc_auc': []}\n\nfor train_idx, test_idx in cv.split(X_scaled, y):\n    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    \n    best_lr.fit(X_train, y_train)\n    y_pred = best_lr.predict(X_test)\n    y_proba = best_lr.predict_proba(X_test)[:, 1]\n    \n    metrics['accuracy'].append(accuracy_score(y_test, y_pred))\n    metrics['precision'].append(precision_score(y_test, y_pred, zero_division=0))\n    metrics['recall'].append(recall_score(y_test, y_pred, zero_division=0))\n    metrics['f1_score'].append(f1_score(y_test, y_pred, zero_division=0))\n    metrics['roc_auc'].append(roc_auc_score(y_test, y_proba))\n\n\nfor metric, scores in metrics.items():\n    print(f\"{metric.capitalize()}: {np.mean(scores)}\")\n\n\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\nplt.title(\"Confusion Matrix - Logistic Regression\", fontsize=16)\nplt.xlabel(\"Predicted\", fontsize=14)\nplt.ylabel(\"Actual\", fontsize=14)\nplt.show()\n\n\n\n\n\n\nBest Parameters: {'C': 0.1, 'max_iter': 5000, 'penalty': 'l1', 'solver': 'liblinear'}\nAccuracy: 0.9422550629447182\nPrecision: 0.9531104160219265\nRecall: 0.9817906630406631\nF1_score: 0.9672090134595874\nRoc_auc: 0.962362779409425"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#gradient-boosting-classifier-2",
    "href": "technical-details/supervised-learning/main.html#gradient-boosting-classifier-2",
    "title": "Supervised Learning",
    "section": "Gradient Boosting Classifier",
    "text": "Gradient Boosting Classifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Features and target variable\nfeatures = [\"likeCount\", \"duration\", \"definition\", \"topicCategories\"]\nX = data[features]  # Feature matrix\nX = X.fillna(0)  # Replace NaN values with 0\n\ny = data[\"popularity\"]  # Target variable (binary labels)\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n# Ensure categorical features are encoded properly\nX = pd.get_dummies(X, columns=[\"definition\", \"topicCategories\"], drop_first=True)\n\n# Feature scaling\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Hyperparameter search space for Gradient Boosting Classifier\nparam_grid = {\n    'n_estimators': [100, 200, 300],         # Number of boosting stages\n    'learning_rate': [0.01, 0.1, 0.2],      # Learning rate\n    'max_depth': [3, 5, 7],                 # Maximum depth of each tree\n    'min_samples_split': [2, 5, 10],        # Minimum samples required to split an internal node\n    'min_samples_leaf': [1, 2, 4],          # Minimum samples required at a leaf node\n    'subsample': [0.8, 1.0],                # Fraction of samples used for fitting individual trees\n    'max_features': ['sqrt', 'log2', None]  # Number of features considered when looking for the best split\n}\n\n# Cross-validation setup\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Hyperparameter tuning with GridSearchCV\nclf = GradientBoostingClassifier(random_state=42)\ngrid_search = GridSearchCV(\n    estimator=clf,\n    param_grid=param_grid,\n    scoring='accuracy',\n    cv=cv,\n    n_jobs=-1,\n    verbose=1\n)\n\n# Perform the grid search\ngrid_search.fit(X_scaled, y)\n\n# Best hyperparameters\nbest_params = grid_search.best_params_\nprint(\"Best Parameters:\", best_params)\n\n# Train the best model on the entire dataset\nbest_gbc = grid_search.best_estimator_\n\n# Initialize metric storage for cross-validation evaluation\naccuracies, precisions, recalls, f1_scores, roc_aucs = [], [], [], [], []\n\n# Cross-validation evaluation with best parameters\nfor train_idx, test_idx in cv.split(X_scaled, y):\n    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    \n    best_gbc.fit(X_train, y_train)  # Train the best model\n    y_pred = best_gbc.predict(X_test)  # Predict on test data\n    y_proba = best_gbc.predict_proba(X_test)[:, 1]  # Predicted probabilities\n    \n    # Evaluate metrics\n    accuracies.append(accuracy_score(y_test, y_pred))\n    precisions.append(precision_score(y_test, y_pred, zero_division=0))\n    recalls.append(recall_score(y_test, y_pred, zero_division=0))\n    f1_scores.append(f1_score(y_test, y_pred, zero_division=0))\n    roc_aucs.append(roc_auc_score(y_test, y_proba))\n\n# Print evaluation metrics\nprint(\"\\nGradient Boosting Classifier - Binary Classification\")\nprint(\"Accuracy:\", np.mean(accuracies))\nprint(\"Precision:\", np.mean(precisions))\nprint(\"Recall:\", np.mean(recalls))\nprint(\"F1 Score:\", np.mean(f1_scores))\nprint(\"ROC AUC:\", np.mean(roc_aucs))\n\n# Confusion matrix visualization\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plot the confusion matrix with more readable labels\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n\n# Add titles and labels\nplt.title(\"Confusion Matrix - Gradient Boosting Classifier\", fontsize=16)\nplt.xlabel(\"Predicted\", fontsize=14)\nplt.ylabel(\"Actual\", fontsize=14)\n\n# Show the plot\nplt.show()\n\n\nFitting 5 folds for each of 1458 candidates, totalling 7290 fits\nBest Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300, 'subsample': 0.8}\n\nGradient Boosting Classifier - Binary Classification\nAccuracy: 0.9516316973788237\nPrecision: 0.9642341340048605\nRecall: 0.9806494431494432\nF1 Score: 0.9723572674490519\nROC AUC: 0.9695507727270917"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#k-nearest-neighbors-knn-1",
    "href": "technical-details/supervised-learning/main.html#k-nearest-neighbors-knn-1",
    "title": "Supervised Learning",
    "section": "K-Nearest Neighbors (KNN)",
    "text": "K-Nearest Neighbors (KNN)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, log_loss, cohen_kappa_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Features and target variable\nfeatures = [\"likeCount\", \"duration\", \"definition\", \"topicCategories\"]\nX = data[features]  # Feature matrix\nX = X.fillna(0)  # Replace NaN values with 0\n\ny = data[\"popularity_multi_class\"]  # Target variable (multi-class labels)\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n# Ensure categorical features are encoded properly\nX = pd.get_dummies(X, columns=[\"definition\", \"topicCategories\"], drop_first=True)\n\n# Feature scaling (important for KNN because it relies on distance metrics)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Cross-validation setup\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Hyperparameter grid for KNN\nparam_grid = {\n    'n_neighbors': [3, 5, 7, 9, 11],  # Number of neighbors\n    'weights': ['uniform', 'distance'],  # Weighting scheme\n    'metric': ['euclidean', 'manhattan', 'minkowski']  # Distance metrics\n}\n\n# K-Nearest Neighbors Classifier\nclf = KNeighborsClassifier()\n\n# GridSearchCV for hyperparameter tuning\ngrid_search = GridSearchCV(\n    estimator=clf,\n    param_grid=param_grid,\n    scoring=\"accuracy\",\n    cv=cv,\n    n_jobs=-1,\n    verbose=1\n)\n\n# Fit GridSearchCV\ngrid_search.fit(X_scaled, y)\n\n# Best hyperparameters\nbest_params = grid_search.best_params_\nprint(\"Best Parameters:\", best_params)\n\n# Use the best model\nbest_clf = grid_search.best_estimator_\n\n# Initialize metric storage for evaluation\naccuracies, precisions_macro, recalls_macro, f1_scores_macro = [], [], [], []\nprecisions_weighted, recalls_weighted, f1_scores_weighted = [], [], []\nlog_losses, kappa_scores = [], []\n\n# Cross-validation loop with the best model\nfor train_idx, test_idx in cv.split(X_scaled, y):\n    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    \n    # Train the best KNN model\n    best_clf.fit(X_train, y_train)\n    \n    # Predictions\n    y_pred = best_clf.predict(X_test)\n    y_proba = best_clf.predict_proba(X_test)  # Probability estimates for all classes\n    \n    # Evaluate metrics\n    accuracies.append(accuracy_score(y_test, y_pred))\n    precisions_macro.append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n    recalls_macro.append(recall_score(y_test, y_pred, average='macro', zero_division=0))\n    f1_scores_macro.append(f1_score(y_test, y_pred, average='macro', zero_division=0))\n    \n    precisions_weighted.append(precision_score(y_test, y_pred, average='weighted', zero_division=0))\n    recalls_weighted.append(recall_score(y_test, y_pred, average='weighted', zero_division=0))\n    f1_scores_weighted.append(f1_score(y_test, y_pred, average='weighted', zero_division=0))\n    \n    log_losses.append(log_loss(y_test, y_proba))\n    kappa_scores.append(cohen_kappa_score(y_test, y_pred))\n\n# Print evaluation metrics\nprint(\"K-Nearest Neighbors (KNN) with Hyperparameter Tuning - Multi-Class Classification\")\nprint(\"===========================================================================\")\nprint(f\"Accuracy: {np.mean(accuracies)}\")\nprint(f\"Macro Precision: {np.mean(precisions_macro)}\")\nprint(f\"Macro Recall: {np.mean(recalls_macro)}\")\nprint(f\"Macro F1 Score: {np.mean(f1_scores_macro)}\")\nprint(f\"Weighted Precision: {np.mean(precisions_weighted)}\")\nprint(f\"Weighted Recall: {np.mean(recalls_weighted)}\")\nprint(f\"Weighted F1 Score: {np.mean(f1_scores_weighted)}\")\nprint(f\"Log Loss: {np.mean(log_losses)}\")\nprint(f\"Cohen Kappa Score: {np.mean(kappa_scores)}\")\n\n# Confusion matrix visualization\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plot the confusion matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n\n# Add titles and labels\nplt.title(\"Confusion Matrix - Multi-Class KNN (Tuned)\", fontsize=16)\nplt.xlabel(\"Predicted\", fontsize=14)\nplt.ylabel(\"Actual\", fontsize=14)\n\n# Show the plot\nplt.show()\n\n# Detailed classification report\nprint(\"\\nClassification Report:\\n\")\nprint(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n\n\nFitting 5 folds for each of 30 candidates, totalling 150 fits\nBest Parameters: {'metric': 'manhattan', 'n_neighbors': 9, 'weights': 'distance'}\nK-Nearest Neighbors (KNN) with Hyperparameter Tuning - Multi-Class Classification\n===========================================================================\nAccuracy: 0.7724695938981653\nMacro Precision: 0.7794176006169204\nMacro Recall: 0.7433875665336453\nMacro F1 Score: 0.7568255245158504\nWeighted Precision: 0.7721904010101952\nWeighted Recall: 0.7724695938981653\nWeighted F1 Score: 0.7687121833806551\nLog Loss: 1.4498988304474825\nCohen Kappa Score: 0.6249967306705042\n\n\n\n\n\n\n\n\n\n\nClassification Report:\n\n              precision    recall  f1-score   support\n\n        high       0.86      0.61      0.71        71\n         low       0.75      0.87      0.81       204\n      medium       0.69      0.65      0.67       165\n\n    accuracy                           0.74       440\n   macro avg       0.77      0.71      0.73       440\nweighted avg       0.75      0.74      0.74       440"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#regression-section-2",
    "href": "technical-details/supervised-learning/main.html#regression-section-2",
    "title": "Supervised Learning",
    "section": "Regression Section",
    "text": "Regression Section\n\nRandom Forest Regressor\n\nBest Hyperparameters:\n\nbootstrap: True\n\nmax_depth: 20\n\nmax_features: ‘sqrt’\n\nmin_samples_leaf: 3\n\nmin_samples_split: 5\n\nn_estimators: 445\n\nPerformance Metrics:\n\nRMSE: 0.2495\n\nMAE: 0.1099\n\nR²: 0.7713\n\n\n\nModel Performance Summary:\nThe Random Forest Regressor achieved optimal performance with the following hyperparameters: bootstrap=True, max_depth=20, max_features='sqrt', min_samples_leaf=3, min_samples_split=5, and n_estimators=445. The RMSE of 0.2495 and MAE of 0.1099 are relatively low, showing that the model’s predictions are close to the actual view counts, with only minor errors. The R² value of 0.7713 is also high. This demonstrates strong predictive capability, but there is room for improvement, as 22.87% of the variance remains unexplained.\n\n\nInterpretation/Technical implications:\nAs shown in the figure, most points cluster near the origin, indicating that the model performs well for lower view counts. This aligns with expectations, as the number of highly popular videos is relatively small. However, as the view count increases, the deviation from the line becomes more significant, suggesting that the model struggles to predict higher values. This is likely due to data sparsity caused by the limited number of highly popular videos. Overall, while the model performs effectively for the majority of the data, its performance on extreme view counts could be improved through better feature engineering or outlier handling.\n\n\n\n\nLinear Regression\n\nBest Hyperparameters:\n\nalpha: 10\n\nsolver: ‘auto’\n\nPerformance Metrics:\n\nRMSE: 0.2057\n\nMAE: 0.1241\n\nR²: 0.8033\n\n\n\nModel Performance Summary:\nThe Linear Regression model achieved optimal performance with the following hyperparameters: alpha=10 and solver=‘auto’. The RMSE of 0.2057 and MAE of 0.1241 are relatively low, indicating that the model’s predictions closely match the actual view counts with minimal errors. The R² value of 0.8033 is relatively high, demonstrating that the model explains 80.33% of the variance in view counts. However, there is still room for improvement, as 19.67% of the variance remains unexplained, which could be addressed with additional features or refined model tuning.\n\n\nInterpretation/Technical implications:\nAs shown in the figure, most points align closely with the red dashed “Perfect Fit” line for lower view counts, indicating that the model performs well in this range. This outcome aligns with expectations, as lower view counts are more common in the dataset. However, as view counts increase, the deviation from the line becomes more pronounced, highlighting the model’s struggle to accurately predict higher values. This issue is likely due to data sparsity, as the dataset contains a limited number of highly popular videos. Overall, while the model performs effectively for the majority of the data, its accuracy for extreme view counts could be improved through better feature engineering, the addition of relevant predictive features, or addressing data imbalance.\n\n\n\n\nGradient Boosting Regressor\n\nBest Hyperparameters:\n\nlearning_rate: 0.01\n\nmax_depth: 5\n\nmax_features: None\n\nmin_samples_leaf: 4\n\nmin_samples_split: 2\n\nn_estimators: 300\n\nsubsample: 0.8\n\nPerformance Metrics:\n\nRMSE: 0.1295\n\nMAE: 0.0798\n\nR²: 0.8727\n\n\n\nModel Performance Summary:\nThe Gradient Boosting Regressor achieved optimal performance with the following hyperparameters: learning_rate=0.01, max_depth=5, max_features=None, min_samples_leaf=4, min_samples_split=2, n_estimators=300, and subsample=0.8. The model demonstrated strong predictive accuracy with an RMSE of 0.1295 and MAE of 0.0798, indicating small prediction errors. The R² value of 0.8727 signifies that the model explains approximately 87.27% of the variance in view counts, reflecting high reliability and effectiveness in predicting view counts. However, there is still room for improvement to address the unexplained variance (12.73%). Overall, the model balances precision and generalization effectively across the dataset.\n\n\nInterpretation/Technical implications:\nMost data points cluster near the origin, indicating strong model performance for low view counts, where the predictions align closely with the perfect fit line. In the moderate view count range (0.25 to 0.75), the points show increased dispersion, suggesting slightly reduced accuracy. For high view counts (above 0.75), the scatter becomes sparser with significant deviations from the line, highlighting the model’s difficulty in predicting high view counts accurately. This is likely due to data sparsity in the high view count range and potential limitations in the selected features. Overall, the model performs well for lower and moderate intervals but struggles with higher intervals, where improvements in feature engineering or data balance could enhance performance.\n\n\n\n\nDiscussion for Regression Section\nResult Interpretation:\nThis analysis highlights the performance of three regression models — Random Forest Regressor, Linear Regression, and Gradient Boosting Regressor — in predicting view counts. Among all models, the selected features (likeCount, duration, definition, topicCategories, popularity) proved effective in explaining the variance in view counts. The Gradient Boosting Regressor demonstrated the strongest performance, achieving an R² of 0.8727, which indicates its superior ability to explain the variance in view counts. The Linear Regression followed with an R² of 0.8033, showcasing strong predictive capabilities but limited by its linear assumptions. Lastly, the Random Forest Regressor achieved an R² of 0.7713, reflecting good performance overall, but less accurate compared to the other models.\nModel Performance Comparison:\n\nGradient Boosting Regressor achieved the best performance, as indicated by its lowest RMSE (0.1295) and MAE (0.0798), alongside the highest R² value (0.8727). This suggests that the model handles both variance and bias effectively, leveraging its sequential learning process to iteratively minimize errors.\nLinear Regression, with an RMSE of 0.2057 and MAE of 0.1241, performed well for a simple model. However, its R² of 0.8033 indicates that it struggles to capture non-linear relationships, particularly for high view count predictions.\nRandom Forest Regressor delivered a solid performance with an RMSE of 0.2495 and MAE of 0.1099 but lagged in explaining variance, as evidenced by its R² of 0.7713. Its performance was limited in the higher view count range, where data sparsity likely reduced its effectiveness.\n\nInsights Gained:\nThe analysis revealed that the selected features (likeCount, duration, definition, topicCategories, popularity) demonstrate strong predictive power, particularly for lower and moderate view counts, though additional features may be needed to improve predictions for extreme values. A key challenge across all models was data sparsity in the high view count range, highlighting the need for better handling of sparse data through techniques like feature engineering, outlier management, or data augmentation. Among the models, the Gradient Boosting Regressor consistently outperformed others, demonstrating its ability to capture complex patterns in the data and effectively balancing bias and variance, making it the most reliable choice for view count prediction."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#binary-classification-section-1",
    "href": "technical-details/supervised-learning/main.html#binary-classification-section-1",
    "title": "Supervised Learning",
    "section": "Binary Classification Section",
    "text": "Binary Classification Section\n\nRandom Forest Classifier\n\nBest Hyperparameters:\n\nbootstrap: True\n\nclass_weight: None\n\nmax_depth: 30\n\nmax_features: ‘sqrt’\n\nmin_samples_leaf: 1\n\nmin_samples_split: 10\n\nn_estimators: 300\n\nPerformance Metrics:\n\nAccuracy: 0.9496\n\nPrecision: 0.9626\n\nRecall: 0.9800\n\nF1 Score: 0.9712\n\nROC AUC: 0.9644\n\n\n\nModel Performance Summary:\nThe Random Forest Classifier demonstrated excellent performance with high accuracy (94.96%) and F1 score (97.12%), indicating a reliable and well-balanced model. The precision score (96.26%) reflects strong specificity, correctly identifying most “high” popularity videos, while the recall (98.00%) highlights its ability to capture nearly all actual “high” popularity cases. A ROC AUC of 96.44% confirms the model’s strong capability in distinguishing between “high” and “low” popularity videos, making its overall performance robust and effective.\n\n\nInterpretation/Technical Implications:\nThe confusion matrix shows that the model performs well overall, correctly predicting 572 “low” popularity videos and accurately identifying 66 “high” popularity videos. However, there are 23 false negatives (actual “high” predicted as “low”) and 14 false positives (actual “low” predicted as “high”). The relatively higher number of false negatives may be due to the smaller sample size of “high” popularity videos in the dataset. This suggests that while the model excels in predicting “low” popularity videos, it has room for improvement in handling “high” popularity predictions, potentially through addressing class imbalance.\n\n\n\n\nLogistic Regression\n\nBest Hyperparameters:\n\nC: 0.1\n\nmax_iter: 5000\n\npenalty: ‘l1’\n\nsolver: ‘liblinear’\n\nPerformance Metrics:\n\nAccuracy: 0.9423\n\nPrecision: 0.9531\n\nRecall: 0.9818\n\nF1 Score: 0.9672\n\nROC AUC: 0.9623\n\n\n\nModel Performance Summary:\nThe Logistic Regression model exhibited strong performance with high accuracy (94.23%) and F1 score (96.72%), demonstrating its effectiveness as a balanced and reliable classifier. The precision score (95.31%) indicates strong specificity, correctly identifying most “high” popularity videos, while the recall (98.18%) shows the model’s ability to capture nearly all actual “high” popularity cases. A ROC AUC of 96.23% confirms its robust capability in distinguishing between “high” and “low” popularity videos.\n\n\nInterpretation/Technical Implications:\nThe confusion matrix shows that the model correctly predicted 346 “low” popularity videos and 37 “high” popularity videos. However, it also resulted in 16 false negatives (actual “high” predicted as “low”) and 6 false positives (actual “low” predicted as “high”). The relatively higher rate of false negatives indicates that the model is slightly conservative when predicting “high” popularity videos, likely due to the smaller sample size of “high” popularity cases in the dataset. While the model’s overall performance is strong, its ability to predict “high” popularity videos could be improved, potentially by lowering the threshold for classifying videos as “high” popularity.\n\n\n\n\nGradient Boosting Classifier\n\nBest Hyperparameters:\n\nlearning_rate: 0.01\n\nmax_depth: 5\n\nmax_features: None\n\nmin_samples_leaf: 1\n\nmin_samples_split: 2\n\nn_estimators: 300\n\nsubsample: 0.8\n\nPerformance Metrics:\n\nAccuracy: 0.9516\n\nPrecision: 0.9642\n\nRecall: 0.9806\n\nF1 Score: 0.9724\n\nROC AUC: 0.9696\n\n\n\nModel Performance Summary:\nThe Gradient Boosting Classifier demonstrated excellent performance, achieving high accuracy and F1 scores, indicating a well-balanced and effective model. The precision of 96.42% reflects strong specificity, accurately identifying most “high” popularity cases, while the recall of 98.06% highlights its ability to capture nearly all true “high” popularity videos. The ROC AUC score of 96.96% confirms the model’s robust discriminatory power between “high” and “low” popularity classes.\n\n\nInterpretation/Technical Implications:\nThe confusion matrix shows that the model correctly predicted 345 “low” popularity videos and 40 “high” popularity videos. However, it also produced 13 false negatives (actual “high” predicted as “low”) and 7 false positives (actual “low” predicted as “high”). These results suggest that the model performs better at identifying “low” popularity cases but remains effective overall in both classes. Due to the smaller sample size of “high” popularity cases in the dataset, the model’s ability to predict “high” popularity videos could be improved, potentially by lowering the threshold for classifying videos as “high” popularity.\n\n\n\n\nDiscussion for Binary Classification Section\nResult Interpretation:\nThe analysis highlights the performance of three binary classification models — Random Forest Classifier, Logistic Regression, and Gradient Boosting Classifier — in predicting video popularity. All three models demonstrated strong overall performance, with accuracy, precision, recall, F1 score, and ROC AUC metrics consistently high. Among them, the Gradient Boosting Classifier achieved the best overall performance, with an F1 score of 97.24% and a ROC AUC of 96.96%, indicating its superior ability to balance precision and recall while effectively distinguishing between “high” and “low” popularity classes. The Random Forest Classifier and Logistic Regression also delivered excellent results, with only minor differences in their ability to handle false positives and false negatives.\nModel Performance Comparison:\n\nGradient Boosting Classifier emerged as the top-performing model, with the highest ROC AUC (96.96%) and F1 score (97.24%). It showed the strongest balance between precision and recall, making it effective in identifying both “high” and “low” popularity videos. However, it produced slightly more false negatives compared to the Random Forest Classifier.\nRandom Forest Classifier performed slightly below Gradient Boosting, with an F1 score of 97.12% and ROC AUC of 96.44%. It excelled in identifying “low” popularity videos but was slightly more conservative in predicting “high” popularity cases, resulting in 23 false negatives.\nLogistic Regression demonstrated robust performance with an F1 score of 96.72% and ROC AUC of 96.23%. While it maintained a strong balance between precision and recall, its slightly higher false-negative rate compared to the other models indicates a conservative bias when predicting “high” popularity videos.\n\nInsights Gained:\nThe analysis shows that all three models effectively utilized the selected features (“likeCount,” “duration,” “definition,” and “topicCategories”) to distinguish between “high” and “low” popularity videos. All three models performed better in predicting “low” popularity videos compared to “high” popularity ones. However, class imbalance remains a challenge, as evidenced by the relatively higher misclassification rates, particularly for the “high” popularity category. Fine-tuning the classification threshold could improve the ability to correctly identify “high” popularity videos without significantly increasing the false positive rate. Overall, the Gradient Boosting Classifier achieved a strong balance between precision, recall, and robustness when handling imbalanced data."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#multi-class-classification-section-2",
    "href": "technical-details/supervised-learning/main.html#multi-class-classification-section-2",
    "title": "Supervised Learning",
    "section": "Multi-class Classification Section",
    "text": "Multi-class Classification Section\n\nGradient Boosting Classifier\n\nBest Hyperparameters:\n\nlearning_rate: 0.01\n\nmax_depth: 5\n\nmin_samples_leaf: 1\n\nmin_samples_split: 5\n\nn_estimators: 200\n\nsubsample: 0.8\n\nPerformance Metrics:\n\nAccuracy: 0.853\n\nMacro Precision: 0.852\n\nMacro Recall: 0.836\n\nMacro F1 Score: 0.843\n\nWeighted Precision: 0.853\n\nWeighted Recall: 0.853\n\nWeighted F1 Score: 0.852\n\nLog Loss: 0.407\n\nCohen Kappa Score: 0.761\n\n\n\nModel Performance Summary:\nThe Gradient Boosting Classifier demonstrated strong overall performance, with an accuracy of 85.3% and a weighted F1 score of 85.2%. These values indicate that the model performs reliably across all classes, striking a balance between precision (85.3%) and recall (85.3%). The macro precision (85.2%) and macro F1 score (84.3%) further reinforce its effectiveness in multi-class classification scenarios. Additionally, the low log loss value (0.407) suggests that the model provides well-calibrated probability estimates, which is a critical requirement for applications where probabilistic outputs are needed. The Cohen Kappa score of 0.761 indicates substantial agreement between the model’s predictions and the actual class labels.\n\n\nInterpretation/Technical Implications:\nThe model excelled in predicting the “low” class, correctly classifying 186 instances with minimal errors, which reflects its strong ability to identify this category. For the “high” class, 58 instances were correctly classified, but 13 were misclassified as “medium,” indicating some confusion between these two classes. For the “medium” class, the model identified 122 instances correctly, but 34 were misclassified as “low,” and 9 as “high,” which highlights the inherent difficulty in distinguishing the medium class due to overlapping features. While the model’s overall performance is good, the challenges in correctly predicting the “medium” class suggest opportunities for improvement. Better feature engineering, such as creating more discriminative features or addressing potential data imbalance, could enhance the model’s ability to separate the classes effectively. Despite these challenges, the model’s consistent performance across precision, recall, and F1 score metrics makes it a reliable choice for this classification task.\n\n\n\n\nDecision Tree Classifier\n\nBest Hyperparameters:\n\ncriterion: gini\n\nmax_depth: 5\n\nmax_features: None\n\nmin_samples_leaf: 1\n\nmin_samples_split: 2\n\nPerformance Metrics:\n\nAccuracy: 0.84\n\nMacro Precision: 0.83\n\nMacro Recall: 0.82\n\nMacro F1 Score: 0.83\n\nWeighted Precision: 0.84\n\nWeighted Recall: 0.84\n\nWeighted F1 Score: 0.84\n\nLog Loss: 0.73\n\nCohen Kappa Score: 0.74\n\n\n\nModel Performance Summary:\nThe Decision Tree Classifier demonstrated solid overall performance, achieving an accuracy of 84.0% and a weighted F1 score of 83.9%, indicating reliable classification across all classes. Its macro precision (83.3%) and macro F1 score (82.8%) reflect balanced performance across the “high,” “medium,” and “low” classes, which is critical for multi-class tasks. The low log loss value of 0.73 highlights the model’s ability to provide reasonably calibrated probability estimates, essential for applications requiring confidence-based decision-making. Additionally, the Cohen Kappa score of 0.74 indicates substantial agreement between the model’s predictions and actual labels. These metrics collectively demonstrate the model’s effectiveness in multi-class classification scenarios, though its misclassification of some “medium” instances as “low” or “high” points to potential improvements through enhanced feature engineering or parameter tuning.\n\n\nInterpretation/Technical Implications:\nThe confusion matrix reveals that the model excels in predicting the “low” class, with 185 correctly classified instances and only 19 misclassified as “medium.” The “high” class is also predicted well, with 57 correct classifications, but 14 instances were misclassified as “medium.” The “medium” class presented the greatest challenge, with 118 correctly predicted instances, but 35 were misclassified as “low” and 12 as “high.” This indicates that the “medium” class is more prone to misclassification, likely due to overlapping feature distributions with the other two classes. The classification report shows that the precision, recall, and F1 scores for the “high” and “low” classes are relatively strong, exceeding 80%, whereas the “medium” class lags slightly behind with an F1 score of 75%. This suggests that while the model performs well overall, additional work is needed to improve the distinction between “medium” and the other two classes. Enhanced feature engineering, such as creating features that better capture the nuances of the “medium” class, or using ensemble methods, may improve performance.\n\n\n\n\nK-Nearest Neighbors (KNN)\n\nBest Hyperparameters:\n\nmetric: manhattan\n\nn_neighbors: 9\n\nweights: distance\n\nPerformance Metrics:\n\nAccuracy: 0.772\n\nMacro Precision: 0.779\n\nMacro Recall: 0.743\n\nMacro F1 Score: 0.757\n\nWeighted Precision: 0.772\n\nWeighted Recall: 0.772\n\nWeighted F1 Score: 0.769\n\nLog Loss: 1.450\n\nCohen Kappa Score: 0.625\n\n\n\nModel Performance Summary:\nThe K-Nearest Neighbors (KNN) classifier achieved moderate performance, with an accuracy of 77.2% and a weighted F1 score of 76.9%, indicating that it provides reasonable predictions across all classes. The macro F1 score of 75.7% and macro precision of 77.9% highlight its balanced performance but show some room for improvement in distinguishing among the “high,” “medium,” and “low” classes. The log loss value of 1.450, while relatively high compared to other models, suggests less confidence in its probabilistic estimates. The Cohen Kappa score of 0.625 indicates a moderate level of agreement between the model’s predictions and the actual class labels.\n\n\nInterpretation/Technical Implications:\nThe confusion matrix reveals that the KNN model performed well in predicting the “low” class, correctly classifying 177 instances, but struggled with the “high” and “medium” classes. For the “high” class, the model correctly identified 43 instances but misclassified 22 as “medium,” suggesting some overlap in the feature space between these two classes. Similarly, for the “medium” class, it identified 107 instances correctly but misclassified 52 as “low” and 6 as “high.” These misclassifications indicate that KNN struggles with feature boundaries, particularly for intermediate classes. The reliance on the Manhattan distance metric and distance-based weighting helped improve performance, but further feature engineering or dimensionality reduction may enhance its discriminative power. Despite its limitations, KNN provides a good baseline for multi-class classification and is effective when computational efficiency and interpretability are priorities.\n\n\n\n\nDiscussion for Multi-class Classification Section\nResult Interpretation:\nThe analysis evaluates the performance of three multi-class classification models — Gradient Boosting Classifier, Decision Tree Classifier, and K-Nearest Neighbors (KNN). Among the models, Gradient Boosting Classifier demonstrated the best overall performance, achieving the highest accuracy (85.3%) and weighted F1 score (85.2%). The Decision Tree Classifier followed closely with an accuracy of 84.0% and a weighted F1 score of 83.9%, showing solid classification performance. The KNN Classifier, while effective for “low” class predictions, lagged with an accuracy of 77.2% and a weighted F1 score of 76.9%, reflecting its struggles with overlapping feature spaces.\nModel Performance Comparison:\n\nGradient Boosting Classifier excelled in balancing precision, recall, and F1 scores across all classes. Its low log loss (0.407) and high Cohen Kappa score (0.761) reflect its robust performance and reliable probabilistic outputs.\nDecision Tree Classifier offered competitive results, particularly for the “low” class, but struggled with the “medium” class due to feature overlap. Despite these challenges, its low log loss (0.73) and substantial Cohen Kappa score (0.74) highlight its reliability.\nKNN Classifier showed moderate performance, with strong predictions for the “low” class but difficulties in distinguishing the “high” and “medium” classes. Its high log loss (1.450) and lower Cohen Kappa score (0.625) indicate less robust performance.\n\nInsights Gained:\nThe analysis highlights that all three models effectively use the selected features (“likeCount,” “duration,” “definition,” and “topicCategories”) to distinguish between “low,” “medium,” and “high” popularity videos. However, the “medium” class consistently posed challenges, with high misclassification rates across all models, likely due to feature overlap. Future work should focus on enhancing feature separability through advanced feature engineering or dimensionality reduction. Additionally, addressing class imbalance and optimizing hyperparameters further could enhance performance, particularly for the “medium” and “high” classes. Overall, the Gradient Boosting Classifier is the most reliable model for this task, combining high accuracy with well-calibrated probabilistic predictions."
  },
  {
    "objectID": "technical-details/data-cleaning/closing.html",
    "href": "technical-details/data-cleaning/closing.html",
    "title": "Summary",
    "section": "",
    "text": "we finally gatered youtube video data set with 2,202 rows and 16 columns in ‘video_id’, ‘channelTitle’, ‘title’, ‘tags’, ‘publishedAt’, ‘viewCount’, ‘likeCount’, ‘dislikeCount’, ‘favoriteCount’, ‘commentCount’, ‘duration’, ‘definition’, ‘caption’, ‘topicCategories’, ‘popularity’, ‘popularity_multi_class’.\nWe saved the dataset into csv file for future usage. In which, the dataset has regression target such as ‘viewCount’, ‘likeCount’; binary classification target such as ‘definition’, ‘caption’,‘popularity’, and multiclass-classification target such as ‘topicCategories’ and ‘popularity_multi_class’.\n\n\nDuring the data cleaning phase, several technical challenges were encountered. These included:\n\nHandling Missing or Inconsistent Values Missing or inconsistent data in key fields, such as tags, topicCategories, and likeCount, required careful imputation or replacement with placeholders to maintain dataset integrity.\nDetecting and Removing Non-English Text: Non-English text in fields like title posed a challenge, as it could lead to inconsistencies in analysis. A language detection algorithm was applied to retain only English text, ensuring language uniformity.\nStandardizing Text Fields Text fields such as title and tags contained punctuation, special characters, and redundant information. These were systematically removed to create clean, standardized text data for better analysis and modeling.\nExtracting Structured Information from Unstructured Fields\n\n\nDuration Conversion: Video durations in ISO 8601 format (e.g., PT1H2M30S) were converted into numerical seconds for consistency and usability.\nTopic Parsing: Topics in the topicCategories field were extracted from complex URLs to isolate meaningful labels.\n\n\nNormalizing Large Numerical Fields Fields like viewCount and likeCount contained large numerical values that required normalization for compatibility with machine learning algorithms.\n\n\n\n\n\nAddressing Class Imbalance: Use resampling methods or advanced techniques to balance classification tasks.\nAdvanced Analysis: Explore machine learning models to predict video performance and popularity.\n\nFuture work should focus on utilizing this dataset to uncover trends in viewer engagement, analyze YouTube’s recommendation system, and provide actionable insights for content creators and marketers. These next steps will use the cleaned data to build models to forecast video performance, helping creators estimate potential engagement"
  },
  {
    "objectID": "technical-details/data-cleaning/closing.html#challenges",
    "href": "technical-details/data-cleaning/closing.html#challenges",
    "title": "Summary",
    "section": "",
    "text": "During the data cleaning phase, several technical challenges were encountered. These included:\n\nHandling Missing or Inconsistent Values Missing or inconsistent data in key fields, such as tags, topicCategories, and likeCount, required careful imputation or replacement with placeholders to maintain dataset integrity.\nDetecting and Removing Non-English Text: Non-English text in fields like title posed a challenge, as it could lead to inconsistencies in analysis. A language detection algorithm was applied to retain only English text, ensuring language uniformity.\nStandardizing Text Fields Text fields such as title and tags contained punctuation, special characters, and redundant information. These were systematically removed to create clean, standardized text data for better analysis and modeling.\nExtracting Structured Information from Unstructured Fields\n\n\nDuration Conversion: Video durations in ISO 8601 format (e.g., PT1H2M30S) were converted into numerical seconds for consistency and usability.\nTopic Parsing: Topics in the topicCategories field were extracted from complex URLs to isolate meaningful labels.\n\n\nNormalizing Large Numerical Fields Fields like viewCount and likeCount contained large numerical values that required normalization for compatibility with machine learning algorithms."
  },
  {
    "objectID": "technical-details/data-cleaning/closing.html#conclusion-and-future-steps",
    "href": "technical-details/data-cleaning/closing.html#conclusion-and-future-steps",
    "title": "Summary",
    "section": "",
    "text": "Addressing Class Imbalance: Use resampling methods or advanced techniques to balance classification tasks.\nAdvanced Analysis: Explore machine learning models to predict video performance and popularity.\n\nFuture work should focus on utilizing this dataset to uncover trends in viewer engagement, analyze YouTube’s recommendation system, and provide actionable insights for content creators and marketers. These next steps will use the cleaned data to build models to forecast video performance, helping creators estimate potential engagement"
  },
  {
    "objectID": "technical-details/data-cleaning/overview.html",
    "href": "technical-details/data-cleaning/overview.html",
    "title": "Overview",
    "section": "",
    "text": "Overview\nThis section provides a high-level summary for technical staff, outlining the key tasks and processes undertaken in this project. It establishes the context, motivation, and objectives of the work.\n\nGoals:\nThis document focuses on cleaning and preparing a dataset generated from YouTube API data, ensuring it is accurate, consistent, and suitable for analysis. It includes handling missing values, resolving inconsistencies, and transforming raw features for effective use in supervised-learning and unsupervised-learning tasks.\nMotivation:\nThe primary motivation behind this project is to leverage data collected from the YouTube API to better understand and predict video performance, popularity, and trends. By analyzing and refining the dataset, the goal is to develop reliable models for regression and classification tasks, providing actionable insights into what drives video success on the platform. This effort aims to bridge the gap between raw data and meaningful predictions\nObjectives:\n\nHandle Missing and Erroneous Data\nRemove punctuation and unnecessary symbols from text fields\nExtract meaningful topics from the topicCategories field for better analysis\nConvert duration into seconds for numerical consistency\nNormalize numerical fields\nClassify videos into multi-class (low, medium, high)and binary (low, high) popularity categories based on viewCount\nRemove duplicate entries\nSave Processed Data"
  },
  {
    "objectID": "technical-details/data-cleaning/methods.html",
    "href": "technical-details/data-cleaning/methods.html",
    "title": "Methods",
    "section": "",
    "text": "Methods\nThis section provides an overview of the methods used in this project, including key techniques, tools, and processes for data cleaning, Below, we outline the approach taken to achieve the project’s objectives.\n\nHandle Missing and Erroneous Data\n\n\nMissing values (NaN) in the dataset were replaced with the string \"nan\" to maintain data integrity and ensure compatibility with text-processing functions.\n\nError-prone fields were systematically validated and transformed using custom cleaning functions, including language detection, punctuation removal, topic extraction, duration conversion, numerical normalization, popularity classification, and deduplication,\n\n\nRemove punctuation and unnecessary symbols from text fields\n\n\nA regular expression-based function was applied to remove punctuation and irrelevant symbols from text fields such as title and tags.\n\nStopwords were filtered out from text-heavy columns to retain only meaningful terms.\n\n\nExtract meaningful topics from the topicCategories field for better analysis\n\n\nTopics were extracted from URLs in the topicCategories field using regular expressions, isolating only the relevant text.\n\nNon-informative or missing topics were replaced with \"nan\".\n\n\nConvert duration into seconds for numerical consistency\n\n\nThe ISO-8601 formatted duration field was parsed using a regex-based function to extract hours, minutes, and seconds.\n\nThe durations were converted into a single numerical field (total seconds) for standardization.\n\n\nNormalize numerical fields \n\n\nNumerical columns, including viewCount,likeCount and commentCount, were normalized to a range of 0 to 1 using Min-Max scaling.\n\n\nClassify videos into multi-class (low, medium, high)and binary (low, high) popularity categories based on viewCount\nMulti-Class Popularity:\n\nlow: &lt; 1 million views.\n\nmedium: 1–6 million views.\n\nhigh: ≥ 6 million views.\n\n\nBinary Popularity:\n\nlow: &lt; 6 million views.\n\nhigh: ≥ 6 million views.\n\nRemove duplicate entries Duplicate entries were identified and removed using the unique video identifier (video_id)\nSave Processed Data\n\nThe cleaned and processed dataset was saved in .csv format in the data/processed-data folder\n\nData Summary\n\nAt the end of this data cleaning workflow, we compiled a refined dataset comprising 2,202 rows and 16 columns, containing a mix of string data, numerical values, Boolean indicators, and dates, all prepared for subsequent analysis"
  },
  {
    "objectID": "report/report.html#executive-summary",
    "href": "report/report.html#executive-summary",
    "title": "Final Report",
    "section": "",
    "text": "This report investigates the key factors that influence the popularity of YouTube videos, providing actionable insights for content creators to optimize their strategies. By analyzing video metrics such as content topics, video duration, engagement rates, and audience behavior, the findings aim to help YouTubers refine their content strategies to increase view counts and, ultimately, maximize monetary rewards through higher audience engagement and ad revenue. The insights derived from this analysis can also assist marketers and advertisers in identifying effective promotional content."
  },
  {
    "objectID": "report/report.html#objective-define-the-problem-or-question-addressed.",
    "href": "report/report.html#objective-define-the-problem-or-question-addressed.",
    "title": "Final Report",
    "section": "",
    "text": "The objective of this project is to identify and analyze the key drivers of YouTube video popularity, focusing on:\n\nGathering and selecting useful variables related to YouTube videos.\nInvestigating the correlations between video variables based on visualizations.\nUnderstanding how content topics, video duration, and other metrics influence engagement and popularity.\nHelping content creators and marketers optimize their strategies to enhance video performance and audience reach."
  },
  {
    "objectID": "report/report.html#key-insights-present-the-most-important-actionable-insights.",
    "href": "report/report.html#key-insights-present-the-most-important-actionable-insights.",
    "title": "Final Report",
    "section": "",
    "text": "We classified popular videos with more than 6 million views and found that their views are concentrated in shorter durations (typically under 10 minutes). In contrast, less popular videos tend to vary widely in length.\n\n\n\nVideos on trending topics in the entertainment and gaming categories tend to have higher engagement rates. This is a key finding derived from our clustering analysis.\n\n\n\n\n\n\nVideo View Like Comment Count Correlation\n\n\nView counts, like counts are highly correlated with each other. Subscribers, who actively choose to interact with video creators, are more likely to engage in behaviors such as liking, commenting, or saving videos. This suggests that fostering active audience engagement can help convert viewers into subscribers.\n\n\n\nThe three key features influencing YouTube video popularity are likeCount, duration, and topicCategories. LikeCount reflects audience engagement, duration represents the video’s length, and topicCategories capture the type of content, all contributing to what makes a video popular. \n\n\n\n\nContent Creators: Focus on aligning videos with trending topics, particularly in high-engagement categories like entertainment and gaming, to boost viewership. Additionally, actively encouraging likes and comments during videos can strengthen audience connections and foster long-term subscriber growth.\nAdvertisers: Target advertising efforts on high-engagement videos within popular categories to maximize visibility and ROI. Partnering with creators who consistently produce engaging content can amplify the reach and effectiveness of promotional campaigns.\n\n\n\n\n\nContent Creators:\n\n\nEncourage Engagement: Include clear and consistent call-to-actions (CTAs) within videos, encouraging viewers to like, comment, and subscribe. For example, creators can verbally remind viewers to “hit the like button if you’re enjoying this content” at strategic points in the video.\nEngaging Intros: Capture attention within the first 30 seconds by highlighting the video’s value and encouraging immediate engagement.\nIncorporate Visual Prompts: Use on-screen graphics, animations, or pop-ups during the video to subtly remind viewers to engage without interrupting the flow of content.\nLeveraging Trends: Videos that align with trending topics, especially in popular categories like entertainment and gaming, tend to attract higher audience engagement and view counts.\n\n\nAdvertisers:\n\n\nCollaborations with Engaged Creators: Partner with creators who actively foster audience interaction and have high like and comment rates.\nTarget High-Engagement Content: Focus ad placements on videos that incorporate engagement-driving strategies to ensure higher visibility and effectiveness.\n\n\n\n\nThe analysis highlights the key factors influencing YouTube video popularity, including audience engagement (likes and comments), video duration, and alignment with trending topics. These findings emphasize the importance of producing concise, engaging content and leveraging popular categories like entertainment and gaming to maximize viewership and interaction.\nTo capitalize on these insights, content creators should focus on fostering audience engagement through consistent call-to-actions and strategic content planning. Marketers and advertisers can enhance campaign effectiveness by collaborating with high-performing creators and targeting videos with proven engagement metrics.\nBy addressing these opportunities, creators and advertisers can drive sustained audience growth, improve engagement, and achieve greater profitability.\n\n\n\nIn our analysis, we initially classified YouTube videos into two categories based on social benchmarks and the median view count in our dataset: highly popular and low-popular videos, with the threshold set at 6 million views. The videos span a variety of topics, with some overlap in content. For instance, videos categorized as “entertainment” may also be labeled as “gaming” or “video games” at times. In the unsupervised learning section, we clustered the dimensionally reduced video data into groups using different models, including K-Means, DBSCAN, and Agglomerative Clustering. The clustering results suggest that videos can be categorized into distinct types, such as high-engagement content, niche topics, and viral videos, each reflecting specific audience behaviors. While K-Means and BIRCH identified around six clusters, Agglomerative Clustering uncovered more granular subcategories, and DBSCAN highlighted outliers, which may correspond to viral or exceptional videos. These findings provide valuable insights into how YouTube videos resonate with different audiences and the factors influencing video popularity.\nIn our analysis of video duration and popularity, we found that most highly popular videos tend to be relatively short, typically under 5000 seconds (less than one hour). Our investigation revealed that entertainment videos make up a large proportion of popular content, with their durations generally ranging from 0 to 30 minutes. When examining music videos, we observed significant variation in audience preferences based on music genre and the purpose of listening. For instance, long music videos are preferred as background noise for studying, while shorter music videos, such as those from Korean singer ROSÉ, attract a high level of engagement from viewers who enjoy short, focused content.\nWe found that the number of comments on a video is weakly correlated with view counts, like counts, and even video duration. This suggests that, while a video may be widely viewed or liked, audiences may not always feel inclined to comment. This reluctance could stem from concerns about revealing personal information or a preference for passively consuming content without engaging. However, platforms like YouTube and other video-sharing services commonly use metrics such as “Likes,” “Favorites,” and “Comments” as key indicators of interaction between creators and audiences. To benefit the video creators, our findings suggest that these platforms should consider incorporating additional variables to better evaluate the success of video content and refine their recommendation algorithms accordingly.\nAdvertisers should prioritize creators with high like counts, engaging entertainment-focused content, and shorter video durations. It is recommended to collaborate with viral creators on TikTok, as the platform’s content aligns well with these characteristics, ensuring maximum advertising effectiveness.\nIt is possible to predict the popularity trends of specific video categories to help creators gain a competitive advantage. Our analysis reveals that most viral videos are concentrated in categories such as entertainment and gaming, which consistently show high audience engagement and view counts. By monitoring real-time trends, keyword usage, and engagement patterns within these categories, creators can align their content with emerging topics and audience preferences. Leveraging predictive models and clustering analysis can further identify subcategories or niche trends, enabling creators to produce timely, relevant videos that capitalize on shifting viewer interests and maximize their reach.\nThe three key features influencing YouTube video popularity are likeCount, duration, and topicCategories, each providing critical insights for content success. LikeCount reflects audience engagement and satisfaction, with higher likes boosting visibility through YouTube’s recommendation algorithm, making it essential for creators to include clear call-to-actions (CTAs) encouraging likes. Duration, particularly shorter videos under 3 minutes, aligns with audience attention spans and performs well for entertainment or viral content, though longer videos can succeed in niches like tutorials or educational topics. Finally, topicCategories such as entertainment and gaming attract broader audiences and higher engagement, making them prime opportunities for creators to align content with trending topics and audience interests. By strategically leveraging these features—maximizing engagement, optimizing video length, and targeting popular content categories—creators can enhance visibility, audience retention, and overall video performance."
  },
  {
    "objectID": "report/report.html#methodology-overview-briefly-explain-relevant-methods.",
    "href": "report/report.html#methodology-overview-briefly-explain-relevant-methods.",
    "title": "Final Report",
    "section": "Methodology Overview: Briefly explain relevant methods.",
    "text": "Methodology Overview: Briefly explain relevant methods.\n\nData Input & Data Preprocessing\nData from the real world is messy and cannot be analyzed directly. To provide insights from the data, it is crucial to focus on data collection and data cleaning as the first steps of the project. In our analysis of popular YouTube videos, we spent about one-third of the time gathering and cleaning data by refining incomplete data, removing noisy data, and maintaining consistency in data formatting. The observational unit in this project is the video.\nCollecting video variable data from the YouTube API, we aimed to diversify our selected data as much as possible: we focused on videos generated by creators from English-speaking countries.\nThe preprocessing section provides a foundation for the following Exploratory Data Analysis (EDA) and model fitting sections. We adhered to the principles of Tidy Data: each row has its own column, each observational unit has its own row, and each value has its own cell. Our tidying process includes removing special characters (like emojis and punctuation), removing non-English words, eliminating stop words, standardizing data using MinMax scaling (scaling numbers in the range of 0 to 1), and converting time data into seconds. In the following sections, we normalized and standardized our processed data for specific purposes at each step. For example, we scaled the numerical data using standard scaling and transformed textual data using vectorization.\n\n\nExploratory Data Analysis (EDA):\nTBC\n\n\nModel Fitting:\n\nSupervised Learning Models: TBC\nUnsupervised Learning Models: TBC"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#challenges",
    "href": "technical-details/data-cleaning/main.html#challenges",
    "title": "Data Cleaning",
    "section": "Challenges",
    "text": "Challenges\nDuring the data cleaning phase, several technical challenges were encountered. These included:\n\nHandling Missing or Inconsistent Values Missing or inconsistent data in key fields, such as tags, topicCategories, and likeCount, required careful imputation or replacement with placeholders to maintain dataset integrity.\nDetecting and Removing Non-English Text: Non-English text in fields like title posed a challenge, as it could lead to inconsistencies in analysis. A language detection algorithm was applied to retain only English text, ensuring language uniformity.\nStandardizing Text Fields Text fields such as title and tags contained punctuation, special characters, and redundant information. These were systematically removed to create clean, standardized text data for better analysis and modeling.\nExtracting Structured Information from Unstructured Fields\n\n\nDuration Conversion: Video durations in ISO 8601 format (e.g., PT1H2M30S) were converted into numerical seconds for consistency and usability.\nTopic Parsing: Topics in the topicCategories field were extracted from complex URLs to isolate meaningful labels.\n\n\nNormalizing Large Numerical Fields Fields like viewCount and likeCount contained large numerical values that required normalization for compatibility with machine learning algorithms."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#conclusion-and-future-steps",
    "href": "technical-details/data-cleaning/main.html#conclusion-and-future-steps",
    "title": "Data Cleaning",
    "section": "Conclusion and Future Steps",
    "text": "Conclusion and Future Steps\n\nImproving Features: Create new features like ratios (e.g., like-to-view ratio) to capture additional insights.\nAddressing Class Imbalance: Use resampling methods or advanced techniques to balance classification tasks.\nAdvanced Analysis: Explore machine learning models to predict video performance and popularity.\n\nFuture work should focus on utilizing this dataset to uncover trends in viewer engagement, analyze YouTube’s recommendation system, and provide actionable insights for content creators and marketers. These next steps will use the cleaned data to build models to forecast video performance, helping creators estimate potential engagement"
  },
  {
    "objectID": "report/report.html#executive-summary-1",
    "href": "report/report.html#executive-summary-1",
    "title": "Final Report",
    "section": "Executive Summary",
    "text": "Executive Summary\nThis report investigates the key factors that influence the popularity of YouTube videos, providing actionable insights for content creators to optimize their strategies. By analyzing video metrics such as content topics, video duration, engagement rates, and audience behavior, the findings aim to help YouTubers refine their content strategies to increase view counts and, ultimately, maximize monetary rewards through higher audience engagement and ad revenue. The insights derived from this analysis can also assist marketers and advertisers in identifying effective promotional content."
  },
  {
    "objectID": "report/report.html#objective-define-the-problem-or-question-addressed.-1",
    "href": "report/report.html#objective-define-the-problem-or-question-addressed.-1",
    "title": "Final Report",
    "section": "Objective: Define the problem or question addressed.",
    "text": "Objective: Define the problem or question addressed.\nThe objective of this project is to identify and analyze the key drivers of YouTube video popularity, focusing on:\n\nGathering and selecting useful variables related to YouTube videos.\nInvestigating the correlations between video variables based on visualizations.\nUnderstanding how content topics, video duration, and other metrics influence engagement and popularity.\nHelping content creators and marketers optimize their strategies to enhance video performance and audience reach."
  },
  {
    "objectID": "report/report.html#key-insights-present-the-most-important-actionable-insights.-1",
    "href": "report/report.html#key-insights-present-the-most-important-actionable-insights.-1",
    "title": "Final Report",
    "section": "Key Insights: Present the most important, actionable insights.",
    "text": "Key Insights: Present the most important, actionable insights.\n\nShort Videos Are Winning Audiences Over Long Videos\nWe classified popular videos with more than 6 million views and found that their views are concentrated in shorter durations (typically under 10 minutes). In contrast, less popular videos tend to vary widely in length.\n\n\nVideo Topics Are Highly Overlapped\nVideos on trending topics in the entertainment and gaming categories tend to have higher engagement rates. This is a key finding derived from our clustering analysis.\n\n\nConverting Audiences to Subscribers\nView counts, like counts, and comment counts are highly correlated with each other. Subscribers, who actively choose to interact with video creators, are more likely to engage in behaviors such as liking, commenting, or saving videos. This suggests that fostering active audience engagement can help convert viewers into subscribers."
  },
  {
    "objectID": "report/report.html#objective",
    "href": "report/report.html#objective",
    "title": "Final Report",
    "section": "",
    "text": "The objective of this project is to identify and analyze the key drivers of YouTube video popularity, focusing on:\n\nGathering and selecting useful variables related to YouTube videos.\nInvestigating the correlations between video variables based on visualizations.\nUnderstanding how content topics, video duration, and other metrics influence engagement and popularity.\nHelping content creators and marketers optimize their strategies to enhance video performance and audience reach."
  },
  {
    "objectID": "report/report.html#key-insights",
    "href": "report/report.html#key-insights",
    "title": "Final Report",
    "section": "",
    "text": "Video Duration by Popularity\n\n\n\n\n\nVideo Duration by Topic\n\n\nWe classified popular videos with more than 6 million views and found that their views are concentrated in shorter durations (typically under 10 minutes). In contrast, less popular videos tend to vary widely in length.\n\n\n\nVideos on trending topics in the entertainment and gaming categories tend to have higher engagement rates. This is a key finding derived from our clustering analysis.\n\n\n\nObjective: Identifying patterns and structures in YouTube video data using clustering and dimensionality reduction.\n\n\n\nPCA (Principal Component Analysis) was used to reduce data dimensionality but struggled with non-linear relationships.\n\nt-SNE (t-Distributed Stochastic Neighbor Embedding) performed better in capturing complex structures, revealing clearer video clusters than PCA.\n\nBest Perplexity for t-SNE: 50, leading to the best visualization of clustered data.\n\nBest Number of PCA Components: 47, capturing ~10% variance, indicating high data complexity.\n\n\n\n\n\nK-Means clustering revealed two optimal clusters, grouping videos based on engagement levels.\n\nDBSCAN identified outlier videos, potentially viral content.\n\nAgglomerative Clustering provided more granular subcategories, differentiating niche and trending videos.\n\nBIRCH Clustering handled the large dataset efficiently, highlighting groups of videos based on shared attributes.\n\n\n\n\n\nVideo engagement and viewership trends can be categorized into distinct clusters.\n\nNonlinear relationships exist, suggesting that traditional dimensionality reduction (PCA) is insufficient.\n\nClustering models can identify viral content and niche categories, which may help content creators tailor strategies.\n\n\n\n\n\nObjective: Predicting YouTube video popularity and view counts using regression and classification models.\n\n\n\nRandom Forest, Gradient Boosting, and Ridge Regression were used to predict view counts.\n\nGradient Boosting performed well with:\n\nRMSE: 0.249\n\nR²: 0.771\n\n\nRandom Forest had better overall predictive power, capturing nonlinear interactions.\n\nFeature Importance:\n\nLike count, duration, and topic category were the most influential predictors.\n\n\n\n\n\n\nVideos were classified as high or low popularity (binary), and further categorized into high, medium, and low popularity (multi-class).\n\nBest Model: Gradient Boosting Classifier\n\nBinary Classification Metrics:\n\nAccuracy: 95.1%\n\nF1 Score: 97.2%\n\n\nMulti-Class Classification Metrics:\n\nAccuracy: 85.3%\n\nMacro F1 Score: 84.2%\n\n\n\n\n\n\n\n\n\n\nVideo View Like Comment Count Correlation\n\n\nView counts, like counts are highly correlated with each other. Subscribers, who actively choose to interact with video creators, are more likely to engage in behaviors such as liking, commenting, or saving videos. This suggests that fostering active audience engagement can help convert viewers into subscribers.\n\n\n\nThe three key features influencing YouTube video popularity are likeCount, duration, and topicCategories. LikeCount reflects audience engagement, duration represents the video’s length, and topicCategories capture the type of content, all contributing to what makes a video popular. \n\n\n\n\nContent Creators: Focus on aligning videos with trending topics, particularly in high-engagement categories like entertainment and gaming, to boost viewership. Additionally, actively encouraging likes and comments during videos can strengthen audience connections and foster long-term subscriber growth.\nAdvertisers: Target advertising efforts on high-engagement videos within popular categories to maximize visibility and ROI. Partnering with creators who consistently produce engaging content can amplify the reach and effectiveness of promotional campaigns.\n\n\n\n\n\nContent Creators:\n\n\nEncourage Engagement: Include clear and consistent call-to-actions (CTAs) within videos, encouraging viewers to like, comment, and subscribe. For example, creators can verbally remind viewers to “hit the like button if you’re enjoying this content” at strategic points in the video.\nEngaging Intros: Capture attention within the first 30 seconds by highlighting the video’s value and encouraging immediate engagement.\nIncorporate Visual Prompts: Use on-screen graphics, animations, or pop-ups during the video to subtly remind viewers to engage without interrupting the flow of content.\nLeveraging Trends: Videos that align with trending topics, especially in popular categories like entertainment and gaming, tend to attract higher audience engagement and view counts.\n\n\nAdvertisers:\n\n\nCollaborations with Engaged Creators: Partner with creators who actively foster audience interaction and have high like and comment rates.\nTarget High-Engagement Content: Focus ad placements on videos that incorporate engagement-driving strategies to ensure higher visibility and effectiveness.\n\n\n\n\nThe analysis highlights the key factors influencing YouTube video popularity, including audience engagement (likes and comments), video duration, and alignment with trending topics. These findings emphasize the importance of producing concise, engaging content and leveraging popular categories like entertainment and gaming to maximize viewership and interaction. To capitalize on these insights, content creators should focus on fostering audience engagement through consistent call-to-actions and strategic content planning. Marketers and advertisers can enhance campaign effectiveness by collaborating with high-performing creators and targeting videos with proven engagement metrics. By addressing these opportunities, creators and advertisers can drive sustained audience growth, improve engagement, and achieve greater profitability.\n\n\n\nIn our analysis, we initially classified YouTube videos into two categories based on social benchmarks and the median view count in our dataset: highly popular and low-popular videos, with the threshold set at 6 million views. The videos span a variety of topics, with some overlap in content. For instance, videos categorized as “entertainment” may also be labeled as “gaming” or “video games” at times. In the unsupervised learning section, we clustered the dimensionally reduced video data into groups using different models, including K-Means, DBSCAN, and Agglomerative Clustering. The clustering results suggest that videos can be categorized into distinct types, such as high-engagement content, niche topics, and viral videos, each reflecting specific audience behaviors. While K-Means and BIRCH identified around six clusters, Agglomerative Clustering uncovered more granular subcategories, and DBSCAN highlighted outliers, which may correspond to viral or exceptional videos. These findings provide valuable insights into how YouTube videos resonate with different audiences and the factors influencing video popularity.\nIn our analysis of video duration and popularity, we found that most highly popular videos tend to be relatively short, typically under 5000 seconds (less than one hour). Our investigation revealed that entertainment videos make up a large proportion of popular content, with their durations generally ranging from 0 to 30 minutes. When examining music videos, we observed significant variation in audience preferences based on music genre and the purpose of listening. For instance, long music videos are preferred as background noise for studying, while shorter music videos, such as those from Korean singer ROSÉ, attract a high level of engagement from viewers who enjoy short, focused content.\nWe found that the number of comments on a video is weakly correlated with view counts, like counts, and even video duration. This suggests that, while a video may be widely viewed or liked, audiences may not always feel inclined to comment. This reluctance could stem from concerns about revealing personal information or a preference for passively consuming content without engaging. However, platforms like YouTube and other video-sharing services commonly use metrics such as “Likes,” “Favorites,” and “Comments” as key indicators of interaction between creators and audiences. To benefit the video creators, our findings suggest that these platforms should consider incorporating additional variables to better evaluate the success of video content and refine their recommendation algorithms accordingly.\nAdvertisers should prioritize creators with high like counts, engaging entertainment-focused content, and shorter video durations. It is recommended to collaborate with viral creators on TikTok, as the platform’s content aligns well with these characteristics, ensuring maximum advertising effectiveness.\nIt is possible to predict the popularity trends of specific video categories to help creators gain a competitive advantage. Our analysis reveals that most viral videos are concentrated in categories such as entertainment and gaming, which consistently show high audience engagement and view counts. By monitoring real-time trends, keyword usage, and engagement patterns within these categories, creators can align their content with emerging topics and audience preferences. Leveraging predictive models and clustering analysis can further identify subcategories or niche trends, enabling creators to produce timely, relevant videos that capitalize on shifting viewer interests and maximize their reach.\nThe three key features influencing YouTube video popularity are likeCount, duration, and topicCategories, each providing critical insights for content success. LikeCount reflects audience engagement and satisfaction, with higher likes boosting visibility through YouTube’s recommendation algorithm, making it essential for creators to include clear call-to-actions (CTAs) encouraging likes. Duration, particularly shorter videos under 3 minutes, aligns with audience attention spans and performs well for entertainment or viral content, though longer videos can succeed in niches like tutorials or educational topics. Finally, topicCategories such as entertainment and gaming attract broader audiences and higher engagement, making them prime opportunities for creators to align content with trending topics and audience interests. By strategically leveraging these features—maximizing engagement, optimizing video length, and targeting popular content categories—creators can enhance visibility, audience retention, and overall video performance.1"
  },
  {
    "objectID": "technical-details/data-collection/main.html#youtube-data-collection-from-youtube-data-api",
    "href": "technical-details/data-collection/main.html#youtube-data-collection-from-youtube-data-api",
    "title": "Data Collection",
    "section": "YouTube Data Collection from YouTube Data API",
    "text": "YouTube Data Collection from YouTube Data API\n\nimport pandas as pd\nimport numpy as np\nfrom dateutil import parser\n\n# Data visualization libraries\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport seaborn as sns\nsns.set(style=\"darkgrid\", color_codes=True)\nfrom IPython.display import JSON\nfrom IPython.display import display_json\nimport json\n# Google API\nfrom googleapiclient.discovery import build\n\n\n# NLP libraries\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom wordcloud import WordCloud\n\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/shenyuxi/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /Users/shenyuxi/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\n# pip install --upgrade google-api-python-client\n# youtube_api_key = \"YOUR API KEY\"\nyoutube = build('youtube', 'v3', developerKey=youtube_api_key)\n\n\nimport random\n\ndef retrieve_randomized_videos(token=None):\n    regions = ['US','GB', 'AU', 'NZ', 'IE']  # Example region codes\n    random_region = random.choice(regions)  # Choose a random region\n    \n    # Retrieve available categories for the selected region\n    random_category = random.choice([1, 10, 17, 20, 28, 23, 24, 25, 2])\n    \n\n    # Set up the request with or without pagination (pageToken)\n    if token:\n        request = youtube.videos().list(\n            part=\"contentDetails,snippet,statistics,topicDetails\",\n            maxResults=50,\n            chart=\"mostPopular\",\n            regionCode=random_region,\n            videoCategoryId=random_category,  # Pick one random category for the search\n            pageToken=token  # Use nextPageToken for pagination if available\n        )\n    else:\n        request = youtube.videos().list(\n            part=\"contentDetails,snippet,statistics,topicDetails\",\n            maxResults=50,\n            chart=\"mostPopular\",\n            regionCode=random_region,\n            videoCategoryId=random_category  # Pick one random category for the search\n        )\n\n    response = request.execute()\n\n    return response\n\n\nimport time\nfrom googleapiclient.errors import HttpError\n\ndef retrieve_200_videos():\n    all_videos = []  # To store all retrieved videos\n\n    # Set a target count\n    target_count = 200\n    current_count = 0\n\n    # First request\n    response = retrieve_randomized_videos()\n\n    # Loop to gather data until we reach the target count\n    while current_count &lt; target_count:\n        try:\n            # Process current page of results\n            for item in response.get('items', []):\n                all_videos.append(item)\n                current_count += 1\n                \n                # Stop if we have reached the target count\n                if current_count &gt;= target_count:\n                    break\n\n            # Check for nextPageToken\n            next_page_token = response.get('nextPageToken')\n            if not next_page_token:\n                print(\"No more pages available.\")\n                break  # Exit the loop if no more pages\n\n            # Fetch the next page\n            response = retrieve_randomized_videos(next_page_token)\n\n        except HttpError as e:\n            current_count -= 1\n            if current_count &lt;= 0:\n                break\n\n\n    # Display the number of collected videos\n    print(f\"Total videos collected: {len(all_videos)}\")\n    return all_videos\n\n\nimport pandas as pd\n\ndef get_video_details(all_videos):\n    \"\"\"\n    Extract relevant video details from a list of video data and return a structured DataFrame.\n\n    Args:\n        all_videos (list): List of video data dictionaries.\n\n    Returns:\n        pd.DataFrame: DataFrame containing relevant video details.\n    \"\"\"\n    all_video_info = []\n\n    # Define the fields to extract from each part of the API response\n    stats_to_keep = {\n        'snippet': ['channelTitle', 'title', 'tags', 'publishedAt'],\n        'statistics': ['viewCount', 'likeCount', 'dislikeCount', 'favoriteCount', 'commentCount'],\n        'contentDetails': ['duration', 'definition', 'caption'],\n        'topicDetails': ['topicCategories']\n    }\n\n    # Iterate through all videos in the list\n    for video in all_videos:\n        video_info = {}\n        video_info['video_id'] = video.get('id', None)\n\n        # Extract details from each specified part\n        for part, fields in stats_to_keep.items():\n            for field in fields:\n                try:\n                    # Access nested fields safely\n                    video_info[field] = video.get(part, {}).get(field, None)\n                except Exception as e:\n                    video_info[field] = None\n\n        all_video_info.append(video_info)\n\n    # Convert the list of dictionaries into a DataFrame\n    return pd.DataFrame(all_video_info)\n\n# Example Usage:\n# Assuming `all_videos` contains the aggregated list of video data\nall_videos = retrieve_200_videos()\ndf = get_video_details(all_videos)\ndf.head()\n\nNo more pages available.\nTotal videos collected: 150\n\n\n\n\n\n\n\n\n\nvideo_id\nchannelTitle\ntitle\ntags\npublishedAt\nviewCount\nlikeCount\ndislikeCount\nfavoriteCount\ncommentCount\nduration\ndefinition\ncaption\ntopicCategories\n\n\n\n\n0\nJQHZL3KefNk\nDate\nKai Cenat Got PAYBACK The Helmet Game AGAIN! 😭💀\n[Kai Cenat, Kai, Cenat, Kai Cenat Live, Kai Ce...\n2024-11-19T19:39:52Z\n67783623\n2105665\nNone\n0\n2469\nPT40S\nhd\nfalse\n[https://en.wikipedia.org/wiki/Sport]\n\n\n1\nMkoGUJFtt0Q\nVazho\nThe World's Most Hilarious and Hostile Mascot\nNone\n2024-11-17T19:20:51Z\n17693526\n446088\nNone\n0\n1461\nPT16S\nhd\nfalse\n[https://en.wikipedia.org/wiki/Sport]\n\n\n2\n6enUpPF-WA4\nRowan University\nHey Coach! 👋 How many can Coach Jespersen gues...\nNone\n2024-11-14T21:23:59Z\n21672109\n1592856\nNone\n0\n1701\nPT1M\nhd\nfalse\n[https://en.wikipedia.org/wiki/Sport]\n\n\n3\nxUokiJu4rUE\nAutumn Nations Series\nHIGHLIGHTS | ITALY V NEW ZEALAND | AUTUMN NATI...\nNone\n2024-11-23T22:48:19Z\n461939\n3772\nNone\n0\n758\nPT5M48S\nhd\nfalse\n[https://en.wikipedia.org/wiki/Sport]\n\n\n4\nU2UQ7Io4OuU\nBattleground MMA\n🕵️‍♂️Rampage Jackson Admits Cheating🧴\n[ufc, jaxxon podcast, mma, mma shorts, ufc sho...\n2024-11-17T20:49:36Z\n6572493\n360710\nNone\n0\n1206\nPT22S\nhd\nfalse\n[https://en.wikipedia.org/wiki/Mixed_martial_a...\n\n\n\n\n\n\n\n\ndf = pd.DataFrame()\nfor _ in range(100):\n    all_videos = retrieve_200_videos()\n    current_df = get_video_details(all_videos)\n    df = pd.concat([df, current_df],  axis=0, ignore_index=True)\n\nprint(df)\n\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 183\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 40\nNo more pages available.\nTotal videos collected: 150\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 100\nNo more pages available.\nTotal videos collected: 150\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 150\nNo more pages available.\nTotal videos collected: 40\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 50\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 50\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 150\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 39\nNo more pages available.\nTotal videos collected: 50\nNo more pages available.\nTotal videos collected: 30\nNo more pages available.\nTotal videos collected: 150\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 150\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 39\nNo more pages available.\nTotal videos collected: 150\nNo more pages available.\nTotal videos collected: 150\nNo more pages available.\nTotal videos collected: 30\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 123\nNo more pages available.\nTotal videos collected: 50\nNo more pages available.\nTotal videos collected: 100\nNo more pages available.\nTotal videos collected: 100\nNo more pages available.\nTotal videos collected: 159\nNo more pages available.\nTotal videos collected: 100\nNo more pages available.\nTotal videos collected: 100\nNo more pages available.\nTotal videos collected: 100\nNo more pages available.\nTotal videos collected: 100\nNo more pages available.\nTotal videos collected: 50\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 168\nNo more pages available.\nTotal videos collected: 30\nNo more pages available.\nTotal videos collected: 50\nNo more pages available.\nTotal videos collected: 50\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 131\nNo more pages available.\nTotal videos collected: 142\nNo more pages available.\nTotal videos collected: 123\nNo more pages available.\nTotal videos collected: 181\nNo more pages available.\nTotal videos collected: 50\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 100\nNo more pages available.\nTotal videos collected: 185\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 150\nNo more pages available.\nTotal videos collected: 150\nNo more pages available.\nTotal videos collected: 185\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 50\nNo more pages available.\nTotal videos collected: 50\nNo more pages available.\nTotal videos collected: 50\nNo more pages available.\nTotal videos collected: 100\nNo more pages available.\nTotal videos collected: 100\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 183\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 100\nNo more pages available.\nTotal videos collected: 30\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 128\nNo more pages available.\nTotal videos collected: 150\nNo more pages available.\nTotal videos collected: 150\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 150\nNo more pages available.\nTotal videos collected: 39\nNo more pages available.\nTotal videos collected: 150\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 150\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 50\nNo more pages available.\nTotal videos collected: 30\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 200\nNo more pages available.\nTotal videos collected: 30\nNo more pages available.\nTotal videos collected: 30\nNo more pages available.\nTotal videos collected: 100\nNo more pages available.\nTotal videos collected: 50\nNo more pages available.\nTotal videos collected: 200\n          video_id                channelTitle  \\\n0      b6FTKe-u8XI         Richard Restatement   \n1      W_19sjpQttw         Succesful Celebrity   \n2      udVz8GN_DP0                   Kieran Ta   \n3      iOecn1gSBZY                   M2M Heart   \n4      u1IuChCXplQ                 9-1-1 house   \n...            ...                         ...   \n13593  -n0IhrYBDYk                 LifeofLogos   \n13594  pr7rKo4qe2Y  Fire Department Chronicles   \n13595  jqj3z9dU_fY             TRIPLE F.  T.V.   \n13596  dYutqAk2b2M             ReelRenaissance   \n13597  8i5zS9s_NnY               Aurora Cinema   \n\n                                                   title  \\\n0      Can I get scout badge?#therookie #viralvideo #...   \n1      Bill Nye The Science Guy Shows Kai Cenat A Sci...   \n2      You’re not gonna hit me again, are you? #super...   \n3      Open the cash with money🤣🥵||This is funniest r...   \n4             He realized that they did not stop death😱😳   \n...                                                  ...   \n13593  Shane Gillis Turns On Tony Hinchcliffe!!!😂😂😂| ...   \n13594  Real things I’ve seen as a Paramedic. Ginger s...   \n13595  AMERICAN DAD GOT IT RIGHT😂 #2A #fyp #shorts #s...   \n13596  This is not a simple cold.#shorts #doctor #series   \n13597            Louise became Glenn's wife#shorts#funny   \n\n                                                    tags  \\\n0                                                   None   \n1      [Kai Cenat, Kai, Cenat, ImKaiCenat, AMP, Kai C...   \n2                                                   None   \n3                                                   None   \n4                                                   None   \n...                                                  ...   \n13593                                               None   \n13594                                               None   \n13595                                               None   \n13596                                               None   \n13597                                               None   \n\n                publishedAt viewCount likeCount dislikeCount favoriteCount  \\\n0      2024-11-20T22:42:00Z  10794179    877554         None             0   \n1      2024-11-17T17:01:47Z  18001205   1219798         None             0   \n2      2024-11-18T03:13:34Z   9926158    662739         None             0   \n3      2024-11-16T13:00:53Z  15292454   1128563         None             0   \n4      2024-11-20T19:52:08Z   7105386    439418         None             0   \n...                     ...       ...       ...          ...           ...   \n13593  2024-11-21T18:52:33Z   1399156     54085         None             0   \n13594  2024-11-19T14:15:33Z   2797964    240984         None             0   \n13595  2024-11-06T10:49:54Z   9416536    843711         None             0   \n13596  2024-11-18T15:00:45Z    853024     36275         None             0   \n13597  2024-11-13T23:00:12Z   6379147    364406         None             0   \n\n      commentCount duration definition caption  \\\n0             1504    PT59S         hd   false   \n1             5472    PT26S         hd   false   \n2             1257    PT58S         hd   false   \n3             2204    PT59S         hd   false   \n4             1255     PT1M         hd   false   \n...            ...      ...        ...     ...   \n13593          339    PT34S         hd   false   \n13594         1075    PT37S         hd   false   \n13595         5292    PT35S         hd   false   \n13596          127    PT58S         hd   false   \n13597          226    PT59S         hd   false   \n\n                                         topicCategories  \n0      [https://en.wikipedia.org/wiki/Entertainment, ...  \n1      [https://en.wikipedia.org/wiki/Entertainment, ...  \n2      [https://en.wikipedia.org/wiki/Entertainment, ...  \n3      [https://en.wikipedia.org/wiki/Entertainment, ...  \n4      [https://en.wikipedia.org/wiki/Entertainment, ...  \n...                                                  ...  \n13593  [https://en.wikipedia.org/wiki/Entertainment, ...  \n13594  [https://en.wikipedia.org/wiki/Entertainment, ...  \n13595  [https://en.wikipedia.org/wiki/Entertainment, ...  \n13596  [https://en.wikipedia.org/wiki/Entertainment, ...  \n13597  [https://en.wikipedia.org/wiki/Entertainment, ...  \n\n[13598 rows x 14 columns]\n\n\n\nlen(df.video_id.unique())\n\n2578\n\n\n\ndf[\"viewCount\"].describe()\n\ncount    1.359800e+04\nmean     5.373338e+06\nstd      1.084820e+07\nmin      6.958000e+03\n25%      7.280870e+05\n50%      2.552836e+06\n75%      6.193342e+06\nmax      2.172830e+08\nName: viewCount, dtype: float64\n\n\n\ndf[\"popularity\"] = np.where(df[\"viewCount\"] &gt; 6000000, \"high\", \"low\") # since the median is around 255000000\n\n\ndf.to_csv(\"../../data/raw-data/youtube_data_raw.csv\")"
  },
  {
    "objectID": "technical-details/data-cleaning/closing.html#future-steps",
    "href": "technical-details/data-cleaning/closing.html#future-steps",
    "title": "Summary",
    "section": "",
    "text": "Addressing Class Imbalance: Use resampling methods or advanced techniques to balance classification tasks.\nAdvanced Analysis: Explore machine learning models to predict video performance and popularity.\n\nFuture work should focus on utilizing this dataset to uncover trends in viewer engagement, analyze YouTube’s recommendation system, and provide actionable insights for content creators and marketers. These next steps will use the cleaned data to build models to forecast video performance, helping creators estimate potential engagement"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#future-steps",
    "href": "technical-details/data-cleaning/main.html#future-steps",
    "title": "Data Cleaning",
    "section": "Future Steps",
    "text": "Future Steps\n\nAddressing Class Imbalance: Use resampling methods or advanced techniques to balance classification tasks.\nAdvanced Analysis: Explore machine learning models to predict video performance and popularity.\n\nFuture work should focus on utilizing this dataset to uncover trends in viewer engagement, analyze YouTube’s recommendation system, and provide actionable insights for content creators and marketers. These next steps will use the cleaned data to build models to forecast video performance, helping creators estimate potential engagement"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#gradient-boosting-classifier-3",
    "href": "technical-details/supervised-learning/main.html#gradient-boosting-classifier-3",
    "title": "Supervised Learning",
    "section": "Gradient Boosting Classifier",
    "text": "Gradient Boosting Classifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, log_loss, cohen_kappa_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Features and target variable\nfeatures = [\"likeCount\", \"duration\", \"definition\", \"topicCategories\"]\nX = data[features].fillna(0)  # Replace NaN values with 0\ny = data[\"popularity_multi_class\"]  # Target variable (multi-class labels)\n\n# Label encode target variable\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n# Ensure categorical features are encoded properly\nX = pd.get_dummies(X, columns=[\"definition\", \"topicCategories\"], drop_first=True)\n\n# Feature scaling\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Define parameter grid for hyperparameter tuning\nparam_grid = {\n    'learning_rate': [0.01, 0.1, 0.2],\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 5, 7],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'subsample': [0.8, 1.0]\n}\n\n# Cross-validation setup\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Initialize Gradient Boosting Classifier\nclf = GradientBoostingClassifier(random_state=42)\n\n# Hyperparameter tuning with GridSearchCV\ngrid_search = GridSearchCV(\n    estimator=clf,\n    param_grid=param_grid,\n    scoring='accuracy',\n    cv=cv,\n    n_jobs=-1,\n    verbose=1\n)\n\n# Perform grid search\ngrid_search.fit(X_scaled, y)\n\n# Get best model and parameters\nbest_model = grid_search.best_estimator_\nbest_params = grid_search.best_params_\nprint(\"Best Hyperparameters:\", best_params)\n\n# Cross-validation evaluation with best model\naccuracies, precisions_macro, recalls_macro, f1_scores_macro = [], [], [], []\nprecisions_weighted, recalls_weighted, f1_scores_weighted = [], [], []\nlog_losses, kappa_scores = [], []\n\nfor train_idx, test_idx in cv.split(X_scaled, y):\n    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    \n    # Train the best model\n    best_model.fit(X_train, y_train)\n    \n    # Predictions\n    y_pred = best_model.predict(X_test)\n    y_proba = best_model.predict_proba(X_test)\n    \n    # Evaluate metrics\n    accuracies.append(accuracy_score(y_test, y_pred))\n    precisions_macro.append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n    recalls_macro.append(recall_score(y_test, y_pred, average='macro', zero_division=0))\n    f1_scores_macro.append(f1_score(y_test, y_pred, average='macro', zero_division=0))\n    precisions_weighted.append(precision_score(y_test, y_pred, average='weighted', zero_division=0))\n    recalls_weighted.append(recall_score(y_test, y_pred, average='weighted', zero_division=0))\n    f1_scores_weighted.append(f1_score(y_test, y_pred, average='weighted', zero_division=0))\n    log_losses.append(log_loss(y_test, y_proba))\n    kappa_scores.append(cohen_kappa_score(y_test, y_pred))\n\n# Print evaluation metrics\nprint(\"Gradient Boosting Classifier - Multi-Class Classification\")\nprint(\"=========================================================\")\nprint(f\"Accuracy: {np.mean(accuracies):.4f}\")\nprint(f\"Macro Precision: {np.mean(precisions_macro):.4f}\")\nprint(f\"Macro Recall: {np.mean(recalls_macro):.4f}\")\nprint(f\"Macro F1 Score: {np.mean(f1_scores_macro):.4f}\")\nprint(f\"Weighted Precision: {np.mean(precisions_weighted):.4f}\")\nprint(f\"Weighted Recall: {np.mean(recalls_weighted):.4f}\")\nprint(f\"Weighted F1 Score: {np.mean(f1_scores_weighted):.4f}\")\nprint(f\"Log Loss: {np.mean(log_losses):.4f}\")\nprint(f\"Cohen Kappa Score: {np.mean(kappa_scores):.4f}\")\n\n# Confusion matrix visualization\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\nplt.title(\"Confusion Matrix - Multi-Class Gradient Boosting\", fontsize=16)\nplt.xlabel(\"Predicted\", fontsize=14)\nplt.ylabel(\"Actual\", fontsize=14)\nplt.show()\n\n# Detailed classification report\nprint(\"\\nClassification Report:\\n\")\nprint(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n\n\nFitting 5 folds for each of 486 candidates, totalling 2430 fits\nBest Hyperparameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\nGradient Boosting Classifier - Multi-Class Classification\n=========================================================\nAccuracy: 0.8533\nMacro Precision: 0.8516\nMacro Recall: 0.8360\nMacro F1 Score: 0.8427\nWeighted Precision: 0.8526\nWeighted Recall: 0.8533\nWeighted F1 Score: 0.8521\nLog Loss: 0.4071\nCohen Kappa Score: 0.7610\n\n\n\n\n\n\n\n\n\n\nClassification Report:\n\n              precision    recall  f1-score   support\n\n        high       0.87      0.82      0.84        71\n         low       0.85      0.91      0.88       204\n      medium       0.80      0.74      0.77       165\n\n    accuracy                           0.83       440\n   macro avg       0.84      0.82      0.83       440\nweighted avg       0.83      0.83      0.83       440"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#k-nearest-neighbors-knn-2",
    "href": "technical-details/supervised-learning/main.html#k-nearest-neighbors-knn-2",
    "title": "Supervised Learning",
    "section": "K-Nearest Neighbors (KNN)",
    "text": "K-Nearest Neighbors (KNN)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, log_loss, cohen_kappa_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Features and target variable\nfeatures = [\"likeCount\", \"duration\", \"definition\", \"topicCategories\"]\nX = data[features]  # Feature matrix\nX = X.fillna(0)  # Replace NaN values with 0\n\ny = data[\"popularity_multi_class\"]  # Target variable (multi-class labels)\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n# Ensure categorical features are encoded properly\nX = pd.get_dummies(X, columns=[\"definition\", \"topicCategories\"], drop_first=True)\n\n# Feature scaling (important for KNN because it relies on distance metrics)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Cross-validation setup\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Hyperparameter grid for KNN\nparam_grid = {\n    'n_neighbors': [3, 5, 7, 9, 11],  # Number of neighbors\n    'weights': ['uniform', 'distance'],  # Weighting scheme\n    'metric': ['euclidean', 'manhattan', 'minkowski']  # Distance metrics\n}\n\n# K-Nearest Neighbors Classifier\nclf = KNeighborsClassifier()\n\n# GridSearchCV for hyperparameter tuning\ngrid_search = GridSearchCV(\n    estimator=clf,\n    param_grid=param_grid,\n    scoring=\"accuracy\",\n    cv=cv,\n    n_jobs=-1,\n    verbose=1\n)\n\n# Fit GridSearchCV\ngrid_search.fit(X_scaled, y)\n\n# Best hyperparameters\nbest_params = grid_search.best_params_\nprint(\"Best Parameters:\", best_params)\n\n# Use the best model\nbest_clf = grid_search.best_estimator_\n\n# Initialize metric storage for evaluation\naccuracies, precisions_macro, recalls_macro, f1_scores_macro = [], [], [], []\nprecisions_weighted, recalls_weighted, f1_scores_weighted = [], [], []\nlog_losses, kappa_scores = [], []\n\n# Cross-validation loop with the best model\nfor train_idx, test_idx in cv.split(X_scaled, y):\n    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    \n    # Train the best KNN model\n    best_clf.fit(X_train, y_train)\n    \n    # Predictions\n    y_pred = best_clf.predict(X_test)\n    y_proba = best_clf.predict_proba(X_test)  # Probability estimates for all classes\n    \n    # Evaluate metrics\n    accuracies.append(accuracy_score(y_test, y_pred))\n    precisions_macro.append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n    recalls_macro.append(recall_score(y_test, y_pred, average='macro', zero_division=0))\n    f1_scores_macro.append(f1_score(y_test, y_pred, average='macro', zero_division=0))\n    \n    precisions_weighted.append(precision_score(y_test, y_pred, average='weighted', zero_division=0))\n    recalls_weighted.append(recall_score(y_test, y_pred, average='weighted', zero_division=0))\n    f1_scores_weighted.append(f1_score(y_test, y_pred, average='weighted', zero_division=0))\n    \n    log_losses.append(log_loss(y_test, y_proba))\n    kappa_scores.append(cohen_kappa_score(y_test, y_pred))\n\n# Print evaluation metrics\nprint(\"K-Nearest Neighbors (KNN) with Hyperparameter Tuning - Multi-Class Classification\")\nprint(\"===========================================================================\")\nprint(f\"Accuracy: {np.mean(accuracies)}\")\nprint(f\"Macro Precision: {np.mean(precisions_macro)}\")\nprint(f\"Macro Recall: {np.mean(recalls_macro)}\")\nprint(f\"Macro F1 Score: {np.mean(f1_scores_macro)}\")\nprint(f\"Weighted Precision: {np.mean(precisions_weighted)}\")\nprint(f\"Weighted Recall: {np.mean(recalls_weighted)}\")\nprint(f\"Weighted F1 Score: {np.mean(f1_scores_weighted)}\")\nprint(f\"Log Loss: {np.mean(log_losses)}\")\nprint(f\"Cohen Kappa Score: {np.mean(kappa_scores)}\")\n\n# Confusion matrix visualization\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plot the confusion matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n\n# Add titles and labels\nplt.title(\"Confusion Matrix - Multi-Class KNN (Tuned)\", fontsize=16)\nplt.xlabel(\"Predicted\", fontsize=14)\nplt.ylabel(\"Actual\", fontsize=14)\n\n# Show the plot\nplt.show()\n\n# Detailed classification report\nprint(\"\\nClassification Report:\\n\")\nprint(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n\n\nFitting 5 folds for each of 30 candidates, totalling 150 fits\nBest Parameters: {'metric': 'manhattan', 'n_neighbors': 9, 'weights': 'distance'}\nK-Nearest Neighbors (KNN) with Hyperparameter Tuning - Multi-Class Classification\n===========================================================================\nAccuracy: 0.7724695938981653\nMacro Precision: 0.7794176006169204\nMacro Recall: 0.7433875665336453\nMacro F1 Score: 0.7568255245158504\nWeighted Precision: 0.7721904010101952\nWeighted Recall: 0.7724695938981653\nWeighted F1 Score: 0.7687121833806551\nLog Loss: 1.4498988304474825\nCohen Kappa Score: 0.6249967306705042\n\n\n\n\n\n\n\n\n\n\nClassification Report:\n\n              precision    recall  f1-score   support\n\n        high       0.86      0.61      0.71        71\n         low       0.75      0.87      0.81       204\n      medium       0.69      0.65      0.67       165\n\n    accuracy                           0.74       440\n   macro avg       0.77      0.71      0.73       440\nweighted avg       0.75      0.74      0.74       440"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#decision-tree-classifier-1",
    "href": "technical-details/supervised-learning/main.html#decision-tree-classifier-1",
    "title": "Supervised Learning",
    "section": "Decision Tree Classifier",
    "text": "Decision Tree Classifier\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, log_loss, cohen_kappa_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Features and target variable\nfeatures = [\"likeCount\", \"duration\", \"definition\", \"topicCategories\"]\nX = data[features]  # Feature matrix\nX = X.fillna(0)  # Replace NaN values with 0\n\ny = data[\"popularity_multi_class\"]  # Target variable (multi-class labels)\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n# Ensure categorical features are encoded properly\nX = pd.get_dummies(X, columns=[\"definition\", \"topicCategories\"], drop_first=True)\n\n# Feature scaling (not strictly needed for Decision Tree but kept for consistency)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Cross-validation setup\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Hyperparameter grid for DecisionTreeClassifier\nparam_grid = {\n    \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n    \"max_depth\": [None, 5, 10, 20, 30],\n    \"min_samples_split\": [2, 5, 10],\n    \"min_samples_leaf\": [1, 2, 4],\n    \"max_features\": [None, \"sqrt\", \"log2\"]\n}\n\n# Decision Tree Classifier\nclf = DecisionTreeClassifier(random_state=42)\n\n# GridSearchCV for hyperparameter tuning\ngrid_search = GridSearchCV(\n    estimator=clf,\n    param_grid=param_grid,\n    scoring=\"accuracy\",\n    cv=cv,\n    n_jobs=-1,\n    verbose=1\n)\n\n# Fit GridSearchCV\ngrid_search.fit(X_scaled, y)\n\n# Best hyperparameters\nbest_params = grid_search.best_params_\nprint(\"Best Parameters:\", best_params)\n\n# Use the best model\nbest_clf = grid_search.best_estimator_\n\n# Initialize metric storage for evaluation\naccuracies, precisions_macro, recalls_macro, f1_scores_macro = [], [], [], []\nprecisions_weighted, recalls_weighted, f1_scores_weighted = [], [], []\nlog_losses, kappa_scores = [], []\n\n# Cross-validation loop with the best model\nfor train_idx, test_idx in cv.split(X_scaled, y):\n    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    \n    # Train the best Decision Tree Classifier\n    best_clf.fit(X_train, y_train)\n    \n    # Predictions\n    y_pred = best_clf.predict(X_test)\n    y_proba = best_clf.predict_proba(X_test)  # Probability estimates for all classes\n    \n    # Evaluate metrics\n    accuracies.append(accuracy_score(y_test, y_pred))\n    precisions_macro.append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n    recalls_macro.append(recall_score(y_test, y_pred, average='macro', zero_division=0))\n    f1_scores_macro.append(f1_score(y_test, y_pred, average='macro', zero_division=0))\n    \n    precisions_weighted.append(precision_score(y_test, y_pred, average='weighted', zero_division=0))\n    recalls_weighted.append(recall_score(y_test, y_pred, average='weighted', zero_division=0))\n    f1_scores_weighted.append(f1_score(y_test, y_pred, average='weighted', zero_division=0))\n    \n    log_losses.append(log_loss(y_test, y_proba))\n    kappa_scores.append(cohen_kappa_score(y_test, y_pred))\n\n# Print evaluation metrics\nprint(\"Decision Tree Classifier with Hyperparameter Tuning - Multi-Class Classification\")\nprint(\"===========================================================================\")\nprint(f\"Accuracy: {np.mean(accuracies)}\")\nprint(f\"Macro Precision: {np.mean(precisions_macro)}\")\nprint(f\"Macro Recall: {np.mean(recalls_macro)}\")\nprint(f\"Macro F1 Score: {np.mean(f1_scores_macro)}\")\nprint(f\"Weighted Precision: {np.mean(precisions_weighted)}\")\nprint(f\"Weighted Recall: {np.mean(recalls_weighted)}\")\nprint(f\"Weighted F1 Score: {np.mean(f1_scores_weighted)}\")\nprint(f\"Log Loss: {np.mean(log_losses)}\")\nprint(f\"Cohen Kappa Score: {np.mean(kappa_scores)}\")\n\n# Confusion matrix visualization\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Plot the confusion matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n\n# Add titles and labels\nplt.title(\"Confusion Matrix - Multi-Class Decision Tree (Tuned)\", fontsize=16)\nplt.xlabel(\"Predicted\", fontsize=14)\nplt.ylabel(\"Actual\", fontsize=14)\n\n# Show the plot\nplt.show()\n\n# Detailed classification report\nprint(\"\\nClassification Report:\\n\")\nprint(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n\nFitting 5 folds for each of 405 candidates, totalling 2025 fits\nBest Parameters: {'criterion': 'gini', 'max_depth': 5, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\nDecision Tree Classifier with Hyperparameter Tuning - Multi-Class Classification\n===========================================================================\nAccuracy: 0.8401339929911359\nMacro Precision: 0.833451423276734\nMacro Recall: 0.8258330198476095\nMacro F1 Score: 0.828370209357443\nWeighted Precision: 0.8393017350579939\nWeighted Recall: 0.8401339929911359\nWeighted F1 Score: 0.8386803790157578\nLog Loss: 0.7277971131663302\nCohen Kappa Score: 0.7402908330435282\n\n\n\n\n\n\n\n\n\n\nClassification Report:\n\n              precision    recall  f1-score   support\n\n        high       0.83      0.80      0.81        71\n         low       0.84      0.91      0.87       204\n      medium       0.78      0.72      0.75       165\n\n    accuracy                           0.82       440\n   macro avg       0.82      0.81      0.81       440\nweighted avg       0.82      0.82      0.82       440"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#what-it-is",
    "href": "technical-details/supervised-learning/main.html#what-it-is",
    "title": "Supervised Learning",
    "section": "What it is",
    "text": "What it is\nA supervised machine learning algorithm used for classification and regression tasks. It splits the dataset into smaller subsets based on feature values, forming a tree-like structure where each node represents a decision rule and each leaf node represents an outcome."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#how-it-works",
    "href": "technical-details/supervised-learning/main.html#how-it-works",
    "title": "Supervised Learning",
    "section": "How it works",
    "text": "How it works\n\nRecursive Partitioning: The dataset is split into branches based on feature values, with the goal of maximizing information gain or minimizing impurity.\n\nSplitting Criteria: Decisions at each node are made using criteria like Gini Impurity, Entropy, or Log Loss.\n\nTermination: The process stops when a predefined stopping criterion is met, such as maximum tree depth or minimum number of samples per split.\n\nPrediction: A new data point traverses the tree according to its feature values, ending in a leaf node, which provides the predicted class or value."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#inputs",
    "href": "technical-details/supervised-learning/main.html#inputs",
    "title": "Supervised Learning",
    "section": "Inputs",
    "text": "Inputs\n\nFeatures:\n\nlikeCount: The number of likes a video has received.\n\nduration: The length of the video.\n\ndefinition: Categorical variable indicating video quality (e.g., HD, SD).\n\ntopicCategories: Categories describing the video’s content topic.\n\nPreprocessing:\n\nHandling missing values by replacing them with zero or suitable imputation.\n\nEncoding categorical variables (definition and topicCategories) into numeric format using one-hot encoding.\n\n(Optional) Scaling features for consistency, though not strictly required for decision trees."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#outputs",
    "href": "technical-details/supervised-learning/main.html#outputs",
    "title": "Supervised Learning",
    "section": "Outputs",
    "text": "Outputs\n\nClass Labels: Predicted categories for popularity_multi_class.\n\nClass Probabilities: The probabilities for each class, calculated from the proportion of samples in a leaf node."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#key-hyperparameters",
    "href": "technical-details/supervised-learning/main.html#key-hyperparameters",
    "title": "Supervised Learning",
    "section": "Key Hyperparameters",
    "text": "Key Hyperparameters\n\nCriterion: The function to measure the quality of a split (e.g., Gini Impurity, Entropy, or Log Loss).\n\nMax Depth: Maximum depth of the tree to prevent overfitting.\n\nMin Samples Split: Minimum number of samples required to split an internal node.\n\nMin Samples Leaf: Minimum number of samples required to be at a leaf node.\n\nMax Features: The number of features to consider when looking for the best split."
  },
  {
    "objectID": "short_report.html",
    "href": "short_report.html",
    "title": "Final Report Summary",
    "section": "",
    "text": "Details found on [https://yuxishen.georgetown.domains/portfolio_project/]\n\n\nThis report explores the key factors influencing YouTube video popularity, providing insights for content creators, marketers, and advertisers. By analyzing video topics, duration, engagement rates, and audience behavior, this study helps optimize content strategies to increase views, engagement, and ad revenue.\n\n\n\n\n\nPopular videos (6M+ views) are typically under 10 minutes, as shorter content retains audience attention better. Longer videos perform well only in tutorials or background music. Recommendation: Focus on concise, engaging videos to improve retention and visibility.\n\n\n\nEntertainment and gaming dominate high-engagement videos, often overlapping. Recommendation: Leverage trending topics and multi-category tagging to enhance discoverability.\n\n\n\n\n\n\nK-Means grouped videos by engagement; DBSCAN detected viral outliers; t-SNE provided clearer visualizations than PCA. Takeaway: Clustering segments videos for targeted strategies.\n\n\n\nAgglomerative Clustering identified subcategories, while BIRCH processed large datasets efficiently. Application: Viral content gains rapid engagement, while niche content builds long-term retention.\n\n\n\n\n\n\nGradient Boosting performed best (RMSE: 0.249, R²: 0.771). Key Predictors: (1) Like Count – Strongest engagement signal. (2) Duration – Shorter videos perform better. (3) Topic Category – Influences discoverability.\n\n\n\nGradient Boosting achieved 95.1% accuracy (binary classification) and 85.3% accuracy (multi-class classification). Key Takeaway: Likes, duration, and content category are the strongest popularity indicators.\n\n\n\n\nLikes, shares, and views strongly correlate, but comment count is weakly related, suggesting passive engagement. Recommendation: Use CTAs to increase interaction, optimize intros to capture attention within 30 seconds, and add visual engagement prompts.\n\n\n\nFor Content Creators: they should rioritize shorter, engaging content aligned with trends. Use CTAs and visual prompts to drive interaction. Apply multi-category tagging to broaden reach. The advisors should target high-engagement videos for ad placements. Partner with influential YouTubers in gaming and entertainment.\n\n\n\nFor Content Creators: 1. Encourage Engagement: Use CTAs to prompt likes, comments, and subscriptions. 2. Engaging Intros: Capture attention in the first 30 seconds to reduce drop-off. 3. Leverage Trends: Align content with trending entertainment and gaming topics. 4. Optimize Video Length: Keep videos under 10 minutes for maximum engagement.\nFor Advertisers: 1. Collaborate with Engaged Creators: Partner with YouTubers who foster audience interaction. 2. Target High-Engagement Content: Focus ad placements on videos with strong engagement metrics.\n\n\n\n\n\nVideos were classified as highly popular (6M+ views) or low-popularity, spanning multiple topics. Using unsupervised learning, K-Means separated high and low-engagement videos, DBSCAN identified viral outliers, Agglomerative Clustering revealed subcategories, and BIRCH efficiently grouped content. Engagement-driven videos cluster within entertainment and gaming, while niche content attracts dedicated audiences. Viral videos appear as outliers, suggesting external factors like trends and social media boosts contribute to their success.\n\n\n\nMost popular videos are under 30 minutes, with a strong concentration below 10 minutes. Shorter videos retain attention better, increasing engagement. Entertainment and gaming thrive in shorter formats, while longer durations suit educational content. Music videos show mixed trends—shorter ones generate high engagement, while longer videos work as background content. Optimal duration depends on content type and audience expectations.\n\n\n\nLike count strongly correlates with views, serving as a key engagement metric. However, comment count shows a weaker correlation, indicating passive consumption. Privacy concerns and content type influence comment behavior. While YouTube relies on likes, comments, and watch time, additional indicators like audience retention and rewatch rates should be considered. Creators can enhance engagement by prompting comments through interactive content and direct CTAs.\n\n\n\nAdvertisers should prioritize creators with high like-to-view ratios, as this signals strong engagement. Entertainment and gaming content consistently attract large audiences, making them ideal for ad placements. Shorter videos reduce ad-skipping rates, improving effectiveness. Many successful YouTubers expand to platforms like TikTok, increasing cross-platform reach. Strategic partnerships with engaged creators and targeted ad placements maximize ROI.\n\n\n\nPredictive models show content category is a strong indicator of popularity. Entertainment and gaming consistently attract large audiences, while trending topics and viral challenges boost visibility. Engagement patterns and keyword trends help forecast content performance. Clustering identifies emerging subcategories, enabling data-driven content strategy adjustments. Tools like Google Trends and YouTube Analytics allow real-time optimization, helping creators maximize reach and engagement.\n\n\n\nThe three strongest predictors are like count, duration, and topic category. Like count is the best engagement metric, directly influencing video visibility in YouTube’s recommendation system. Shorter videos tend to perform better in entertainment and viral content, while longer videos succeed in educational niches. Topic category affects audience reach, with entertainment and gaming drawing the most engagement. Creators should maximize engagement through CTAs, optimize video length based on audience expectations, and target high-engagement content categories.\n\n\n\n\nThis research highlights engagement-driven strategies as key to YouTube success. The recommendation system favors high-interaction videos, meaning creators should focus on increasing engagement through CTAs, aligning content with trends, and adapting to shifting audience interests. By applying these strategies, content creators can enhance their visibility, and advertisers can effectively target high-performing content.\n\n\n\n\n\n\nVideo Duration by Popularity\n\n\n\n\n\nVideo Duration by Topic"
  },
  {
    "objectID": "short_report.html#executive-summary",
    "href": "short_report.html#executive-summary",
    "title": "Final Report Summary",
    "section": "",
    "text": "This report explores the key factors influencing YouTube video popularity, providing insights for content creators, marketers, and advertisers. By analyzing video topics, duration, engagement rates, and audience behavior, this study helps optimize content strategies to increase views, engagement, and ad revenue."
  },
  {
    "objectID": "short_report.html#key-insights",
    "href": "short_report.html#key-insights",
    "title": "Final Report Summary",
    "section": "",
    "text": "Popular videos (6M+ views) are typically under 10 minutes, as shorter content retains audience attention better. Longer videos perform well only in tutorials or background music. Recommendation: Focus on concise, engaging videos to improve retention and visibility.\n\n\n\nEntertainment and gaming dominate high-engagement videos, often overlapping. Recommendation: Leverage trending topics and multi-category tagging to enhance discoverability."
  },
  {
    "objectID": "short_report.html#unsupervised-learning-insights",
    "href": "short_report.html#unsupervised-learning-insights",
    "title": "Final Report Summary",
    "section": "",
    "text": "K-Means grouped videos by engagement; DBSCAN detected viral outliers; t-SNE provided clearer visualizations than PCA. Takeaway: Clustering segments videos for targeted strategies.\n\n\n\nAgglomerative Clustering identified subcategories, while BIRCH processed large datasets efficiently. Application: Viral content gains rapid engagement, while niche content builds long-term retention."
  },
  {
    "objectID": "short_report.html#supervised-learning-insights",
    "href": "short_report.html#supervised-learning-insights",
    "title": "Final Report Summary",
    "section": "",
    "text": "Gradient Boosting performed best (RMSE: 0.249, R²: 0.771). Key Predictors: (1) Like Count – Strongest engagement signal. (2) Duration – Shorter videos perform better. (3) Topic Category – Influences discoverability.\n\n\n\nGradient Boosting achieved 95.1% accuracy (binary classification) and 85.3% accuracy (multi-class classification). Key Takeaway: Likes, duration, and content category are the strongest popularity indicators."
  },
  {
    "objectID": "short_report.html#converting-audiences-to-subscribers",
    "href": "short_report.html#converting-audiences-to-subscribers",
    "title": "Final Report Summary",
    "section": "",
    "text": "Likes, shares, and views strongly correlate, but comment count is weakly related, suggesting passive engagement. Recommendation: Use CTAs to increase interaction, optimize intros to capture attention within 30 seconds, and add visual engagement prompts."
  },
  {
    "objectID": "short_report.html#business-implications",
    "href": "short_report.html#business-implications",
    "title": "Final Report Summary",
    "section": "",
    "text": "For Content Creators: they should rioritize shorter, engaging content aligned with trends. Use CTAs and visual prompts to drive interaction. Apply multi-category tagging to broaden reach. The advisors should target high-engagement videos for ad placements. Partner with influential YouTubers in gaming and entertainment."
  },
  {
    "objectID": "short_report.html#recommendations",
    "href": "short_report.html#recommendations",
    "title": "Final Report Summary",
    "section": "",
    "text": "For Content Creators: 1. Encourage Engagement: Use CTAs to prompt likes, comments, and subscriptions. 2. Engaging Intros: Capture attention in the first 30 seconds to reduce drop-off. 3. Leverage Trends: Align content with trending entertainment and gaming topics. 4. Optimize Video Length: Keep videos under 10 minutes for maximum engagement.\nFor Advertisers: 1. Collaborate with Engaged Creators: Partner with YouTubers who foster audience interaction. 2. Target High-Engagement Content: Focus ad placements on videos with strong engagement metrics."
  },
  {
    "objectID": "short_report.html#answering-research-questions",
    "href": "short_report.html#answering-research-questions",
    "title": "Final Report Summary",
    "section": "",
    "text": "Videos were classified as highly popular (6M+ views) or low-popularity, spanning multiple topics. Using unsupervised learning, K-Means separated high and low-engagement videos, DBSCAN identified viral outliers, Agglomerative Clustering revealed subcategories, and BIRCH efficiently grouped content. Engagement-driven videos cluster within entertainment and gaming, while niche content attracts dedicated audiences. Viral videos appear as outliers, suggesting external factors like trends and social media boosts contribute to their success.\n\n\n\nMost popular videos are under 30 minutes, with a strong concentration below 10 minutes. Shorter videos retain attention better, increasing engagement. Entertainment and gaming thrive in shorter formats, while longer durations suit educational content. Music videos show mixed trends—shorter ones generate high engagement, while longer videos work as background content. Optimal duration depends on content type and audience expectations.\n\n\n\nLike count strongly correlates with views, serving as a key engagement metric. However, comment count shows a weaker correlation, indicating passive consumption. Privacy concerns and content type influence comment behavior. While YouTube relies on likes, comments, and watch time, additional indicators like audience retention and rewatch rates should be considered. Creators can enhance engagement by prompting comments through interactive content and direct CTAs.\n\n\n\nAdvertisers should prioritize creators with high like-to-view ratios, as this signals strong engagement. Entertainment and gaming content consistently attract large audiences, making them ideal for ad placements. Shorter videos reduce ad-skipping rates, improving effectiveness. Many successful YouTubers expand to platforms like TikTok, increasing cross-platform reach. Strategic partnerships with engaged creators and targeted ad placements maximize ROI.\n\n\n\nPredictive models show content category is a strong indicator of popularity. Entertainment and gaming consistently attract large audiences, while trending topics and viral challenges boost visibility. Engagement patterns and keyword trends help forecast content performance. Clustering identifies emerging subcategories, enabling data-driven content strategy adjustments. Tools like Google Trends and YouTube Analytics allow real-time optimization, helping creators maximize reach and engagement.\n\n\n\nThe three strongest predictors are like count, duration, and topic category. Like count is the best engagement metric, directly influencing video visibility in YouTube’s recommendation system. Shorter videos tend to perform better in entertainment and viral content, while longer videos succeed in educational niches. Topic category affects audience reach, with entertainment and gaming drawing the most engagement. Creators should maximize engagement through CTAs, optimize video length based on audience expectations, and target high-engagement content categories."
  },
  {
    "objectID": "short_report.html#final-thoughts",
    "href": "short_report.html#final-thoughts",
    "title": "Final Report Summary",
    "section": "",
    "text": "This research highlights engagement-driven strategies as key to YouTube success. The recommendation system favors high-interaction videos, meaning creators should focus on increasing engagement through CTAs, aligning content with trends, and adapting to shifting audience interests. By applying these strategies, content creators can enhance their visibility, and advertisers can effectively target high-performing content."
  },
  {
    "objectID": "short_report.html#appendix",
    "href": "short_report.html#appendix",
    "title": "Final Report Summary",
    "section": "",
    "text": "Video Duration by Popularity\n\n\n\n\n\nVideo Duration by Topic"
  }
]
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Cleaning\"\n",
    "format:\n",
    "    html: \n",
    "        code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include overview.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- After digesting the instructions, you can delete this cell, these are assignment instructions and do not need to be included in your final submission.  -->\n",
    "{{< include methods.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shenyuxi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import re\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../../data/raw-data/youtube_data_raw.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to ../../data/processed-data/cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from langdetect import detect\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define the function to classify multi-class popularity based on view count\n",
    "def classify_popularity(view_count):\n",
    "    if view_count < 1_000_000:  # Less than 1 million\n",
    "        return \"low\"\n",
    "    elif 1_000_000 <= view_count < 6_000_000:  # Between 1 million and 6 million\n",
    "        return \"medium\"\n",
    "    else:  # Greater than or equal to 6 million\n",
    "        return \"high\"\n",
    "\n",
    "# Define the function to classify binary popularity\n",
    "def classify_binary_popularity(view_count):\n",
    "    return \"high\" if view_count >= 6_000_000 else \"low\"\n",
    "\n",
    "# Fill NaN values with \"nan\"\n",
    "data = data.fillna(\"nan\")\n",
    "data[\"likeCount\"] = data[\"likeCount\"].fillna(0)\n",
    "data[\"dislikeCount\"] = data[\"dislikeCount\"].fillna(0)\n",
    "data[\"commentCount\"] = data[\"commentCount\"].fillna(0)\n",
    "# Function to remove punctuation from text\n",
    "def remove_punctuation(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return re.sub(r'[^\\w\\s]', '', str(text))  \n",
    "\n",
    "# Function to clean and remove unnecessary symbols from tags\n",
    "def remove_tags(text):\n",
    "    if pd.isna(text):\n",
    "        return \"nan\"\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", str(text))\n",
    "    \n",
    "    words = text.lower().split()  \n",
    "    filtered_words = [word for word in words if word not in stop_words] \n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Function to extract topics from the 'topicCategories' field\n",
    "def extract_topics(topic_str):\n",
    "    if pd.isna(topic_str):\n",
    "        return \"nan\"\n",
    "    matches = re.findall(r\"wiki/([^\\']+)\", topic_str)\n",
    "    return \" \".join(matches)\n",
    "\n",
    "# Function to check if a text is in English\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == \"en\"\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Function to convert duration to seconds\n",
    "def convert_to_seconds(duration):\n",
    "    hours = minutes = seconds = 0\n",
    "    match = re.match(r\"PT(\\d+H)?(\\d+M)?(\\d+S)?\", duration)\n",
    "    \n",
    "    if match:\n",
    "        if match.group(1):  # Hours\n",
    "            hours = int(match.group(1)[:-1])  # Remove 'H' and convert to int\n",
    "        if match.group(2):  # Minutes\n",
    "            minutes = int(match.group(2)[:-1])  # Remove 'M' and convert to int\n",
    "        if match.group(3):  # Seconds\n",
    "            seconds = int(match.group(3)[:-1])  # Remove 'S' and convert to int\n",
    "            \n",
    "    total_seconds = hours * 3600 + minutes * 60 + seconds\n",
    "    return total_seconds\n",
    "\n",
    "# Standardize view count\n",
    "def normalize_view_count(view_counts):\n",
    "    scaler = MinMaxScaler()\n",
    "    # Reshape the data to fit the scaler\n",
    "    view_counts_reshaped = np.array(view_counts).reshape(-1, 1)\n",
    "    normalized = scaler.fit_transform(view_counts_reshaped)\n",
    "    return normalized.flatten()\n",
    "\n",
    "# Classify popularity before standardizing 'viewCount'\n",
    "if 'viewCount' in data.columns:\n",
    "    data[\"popularity_multi_class\"] = data[\"viewCount\"].apply(classify_popularity)\n",
    "    data[\"popularity\"] = data[\"viewCount\"].apply(classify_binary_popularity)\n",
    "\n",
    "# Iterate through the columns and apply respective cleaning functions\n",
    "for column in data.columns:\n",
    "    if column == 'topicCategories':\n",
    "        # Apply the topic extraction and punctuation removal functions\n",
    "        data[column] = data[column].apply(extract_topics)  \n",
    "    elif column == 'title':\n",
    "        # Keep only rows with English titles and remove punctuation\n",
    "        data = data[data[column].apply(is_english)]  # Keep only rows with English titles\n",
    "        data[column] = data[column].apply(remove_punctuation)\n",
    "    elif column == 'duration':\n",
    "        data['duration'] = data['duration'].apply(convert_to_seconds)\n",
    "    elif column == 'tags':\n",
    "        # Clean tags and remove unwanted symbols\n",
    "        data[column] = data[column].apply(remove_tags)\n",
    "\n",
    "# Standardize 'viewCount' column if it exists\n",
    "if 'viewCount' in data.columns:\n",
    "    data['viewCount'] = normalize_view_count(data['viewCount'].astype(float))\n",
    "\n",
    "# Standardize 'likeCount' column if it exists\n",
    "if 'likeCount' in data.columns:\n",
    "    data['likeCount'] = normalize_view_count(data['likeCount'].astype(float))\n",
    "\n",
    "if 'commentCount' in data.columns:\n",
    "    data['commentCount'] = normalize_view_count(data['commentCount'].astype(float))\n",
    "\n",
    "# Drop duplicate entries based on 'video_id'\n",
    "data = data.drop_duplicates(\"video_id\")\n",
    "\n",
    "# Define the relative output path for saving the cleaned data\n",
    "output_path = '../../data/processed-data/cleaned_data.csv'  # Relative path from current script\n",
    "data.to_csv(output_path, index=False)  # Save the cleaned data to CSV\n",
    "print(f\"Cleaned data saved to {output_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2202"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include closing.qmd >}} "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

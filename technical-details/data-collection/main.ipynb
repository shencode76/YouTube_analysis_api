{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Collection\"\n",
    "format:\n",
    "    html: \n",
    "        code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- {{< include instructions.qmd >}}  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "{{< include overview.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include methods.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YouTube Data Collection from YouTube Data API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil import parser\n",
    "\n",
    "# Data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\", color_codes=True)\n",
    "from IPython.display import JSON\n",
    "from IPython.display import display_json\n",
    "import json\n",
    "# Google API\n",
    "from googleapiclient.discovery import build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shenyuxi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/shenyuxi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from wordcloud import WordCloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade google-api-python-client\n",
    "# youtube_api_key = \"YOUR API KEY\"\n",
    "youtube = build('youtube', 'v3', developerKey=youtube_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def retrieve_randomized_videos(token=None):\n",
    "    regions = ['US','GB', 'AU', 'NZ', 'IE']  # Example region codes\n",
    "    random_region = random.choice(regions)  # Choose a random region\n",
    "    \n",
    "    # Retrieve available categories for the selected region\n",
    "    random_category = random.choice([1, 10, 17, 20, 28, 23, 24, 25, 2])\n",
    "    \n",
    "\n",
    "    # Set up the request with or without pagination (pageToken)\n",
    "    if token:\n",
    "        request = youtube.videos().list(\n",
    "            part=\"contentDetails,snippet,statistics,topicDetails\",\n",
    "            maxResults=50,\n",
    "            chart=\"mostPopular\",\n",
    "            regionCode=random_region,\n",
    "            videoCategoryId=random_category,  # Pick one random category for the search\n",
    "            pageToken=token  # Use nextPageToken for pagination if available\n",
    "        )\n",
    "    else:\n",
    "        request = youtube.videos().list(\n",
    "            part=\"contentDetails,snippet,statistics,topicDetails\",\n",
    "            maxResults=50,\n",
    "            chart=\"mostPopular\",\n",
    "            regionCode=random_region,\n",
    "            videoCategoryId=random_category  # Pick one random category for the search\n",
    "        )\n",
    "\n",
    "    response = request.execute()\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "def retrieve_200_videos():\n",
    "    all_videos = []  # To store all retrieved videos\n",
    "\n",
    "    # Set a target count\n",
    "    target_count = 200\n",
    "    current_count = 0\n",
    "\n",
    "    # First request\n",
    "    response = retrieve_randomized_videos()\n",
    "\n",
    "    # Loop to gather data until we reach the target count\n",
    "    while current_count < target_count:\n",
    "        try:\n",
    "            # Process current page of results\n",
    "            for item in response.get('items', []):\n",
    "                all_videos.append(item)\n",
    "                current_count += 1\n",
    "                \n",
    "                # Stop if we have reached the target count\n",
    "                if current_count >= target_count:\n",
    "                    break\n",
    "\n",
    "            # Check for nextPageToken\n",
    "            next_page_token = response.get('nextPageToken')\n",
    "            if not next_page_token:\n",
    "                print(\"No more pages available.\")\n",
    "                break  # Exit the loop if no more pages\n",
    "\n",
    "            # Fetch the next page\n",
    "            response = retrieve_randomized_videos(next_page_token)\n",
    "\n",
    "        except HttpError as e:\n",
    "            current_count -= 1\n",
    "            if current_count <= 0:\n",
    "                break\n",
    "\n",
    "\n",
    "    # Display the number of collected videos\n",
    "    print(f\"Total videos collected: {len(all_videos)}\")\n",
    "    return all_videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages available.\n",
      "Total videos collected: 150\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>channelTitle</th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>dislikeCount</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>duration</th>\n",
       "      <th>definition</th>\n",
       "      <th>caption</th>\n",
       "      <th>topicCategories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JQHZL3KefNk</td>\n",
       "      <td>Date</td>\n",
       "      <td>Kai Cenat Got PAYBACK The Helmet Game AGAIN! üò≠üíÄ</td>\n",
       "      <td>[Kai Cenat, Kai, Cenat, Kai Cenat Live, Kai Ce...</td>\n",
       "      <td>2024-11-19T19:39:52Z</td>\n",
       "      <td>67783623</td>\n",
       "      <td>2105665</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2469</td>\n",
       "      <td>PT40S</td>\n",
       "      <td>hd</td>\n",
       "      <td>false</td>\n",
       "      <td>[https://en.wikipedia.org/wiki/Sport]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MkoGUJFtt0Q</td>\n",
       "      <td>Vazho</td>\n",
       "      <td>The World's Most Hilarious and Hostile Mascot</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-17T19:20:51Z</td>\n",
       "      <td>17693526</td>\n",
       "      <td>446088</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1461</td>\n",
       "      <td>PT16S</td>\n",
       "      <td>hd</td>\n",
       "      <td>false</td>\n",
       "      <td>[https://en.wikipedia.org/wiki/Sport]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6enUpPF-WA4</td>\n",
       "      <td>Rowan University</td>\n",
       "      <td>Hey Coach! üëã How many can Coach Jespersen gues...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-14T21:23:59Z</td>\n",
       "      <td>21672109</td>\n",
       "      <td>1592856</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1701</td>\n",
       "      <td>PT1M</td>\n",
       "      <td>hd</td>\n",
       "      <td>false</td>\n",
       "      <td>[https://en.wikipedia.org/wiki/Sport]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xUokiJu4rUE</td>\n",
       "      <td>Autumn Nations Series</td>\n",
       "      <td>HIGHLIGHTS | ITALY V NEW ZEALAND | AUTUMN NATI...</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-11-23T22:48:19Z</td>\n",
       "      <td>461939</td>\n",
       "      <td>3772</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>758</td>\n",
       "      <td>PT5M48S</td>\n",
       "      <td>hd</td>\n",
       "      <td>false</td>\n",
       "      <td>[https://en.wikipedia.org/wiki/Sport]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U2UQ7Io4OuU</td>\n",
       "      <td>Battleground MMA</td>\n",
       "      <td>üïµÔ∏è‚Äç‚ôÇÔ∏èRampage Jackson Admits Cheatingüß¥</td>\n",
       "      <td>[ufc, jaxxon podcast, mma, mma shorts, ufc sho...</td>\n",
       "      <td>2024-11-17T20:49:36Z</td>\n",
       "      <td>6572493</td>\n",
       "      <td>360710</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1206</td>\n",
       "      <td>PT22S</td>\n",
       "      <td>hd</td>\n",
       "      <td>false</td>\n",
       "      <td>[https://en.wikipedia.org/wiki/Mixed_martial_a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id           channelTitle  \\\n",
       "0  JQHZL3KefNk                   Date   \n",
       "1  MkoGUJFtt0Q                  Vazho   \n",
       "2  6enUpPF-WA4       Rowan University   \n",
       "3  xUokiJu4rUE  Autumn Nations Series   \n",
       "4  U2UQ7Io4OuU       Battleground MMA   \n",
       "\n",
       "                                               title  \\\n",
       "0    Kai Cenat Got PAYBACK The Helmet Game AGAIN! üò≠üíÄ   \n",
       "1      The World's Most Hilarious and Hostile Mascot   \n",
       "2  Hey Coach! üëã How many can Coach Jespersen gues...   \n",
       "3  HIGHLIGHTS | ITALY V NEW ZEALAND | AUTUMN NATI...   \n",
       "4              üïµÔ∏è‚Äç‚ôÇÔ∏èRampage Jackson Admits Cheatingüß¥   \n",
       "\n",
       "                                                tags           publishedAt  \\\n",
       "0  [Kai Cenat, Kai, Cenat, Kai Cenat Live, Kai Ce...  2024-11-19T19:39:52Z   \n",
       "1                                               None  2024-11-17T19:20:51Z   \n",
       "2                                               None  2024-11-14T21:23:59Z   \n",
       "3                                               None  2024-11-23T22:48:19Z   \n",
       "4  [ufc, jaxxon podcast, mma, mma shorts, ufc sho...  2024-11-17T20:49:36Z   \n",
       "\n",
       "  viewCount likeCount dislikeCount favoriteCount commentCount duration  \\\n",
       "0  67783623   2105665         None             0         2469    PT40S   \n",
       "1  17693526    446088         None             0         1461    PT16S   \n",
       "2  21672109   1592856         None             0         1701     PT1M   \n",
       "3    461939      3772         None             0          758  PT5M48S   \n",
       "4   6572493    360710         None             0         1206    PT22S   \n",
       "\n",
       "  definition caption                                    topicCategories  \n",
       "0         hd   false              [https://en.wikipedia.org/wiki/Sport]  \n",
       "1         hd   false              [https://en.wikipedia.org/wiki/Sport]  \n",
       "2         hd   false              [https://en.wikipedia.org/wiki/Sport]  \n",
       "3         hd   false              [https://en.wikipedia.org/wiki/Sport]  \n",
       "4         hd   false  [https://en.wikipedia.org/wiki/Mixed_martial_a...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_video_details(all_videos):\n",
    "    \"\"\"\n",
    "    Extract relevant video details from a list of video data and return a structured DataFrame.\n",
    "\n",
    "    Args:\n",
    "        all_videos (list): List of video data dictionaries.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing relevant video details.\n",
    "    \"\"\"\n",
    "    all_video_info = []\n",
    "\n",
    "    # Define the fields to extract from each part of the API response\n",
    "    stats_to_keep = {\n",
    "        'snippet': ['channelTitle', 'title', 'tags', 'publishedAt'],\n",
    "        'statistics': ['viewCount', 'likeCount', 'dislikeCount', 'favoriteCount', 'commentCount'],\n",
    "        'contentDetails': ['duration', 'definition', 'caption'],\n",
    "        'topicDetails': ['topicCategories']\n",
    "    }\n",
    "\n",
    "    # Iterate through all videos in the list\n",
    "    for video in all_videos:\n",
    "        video_info = {}\n",
    "        video_info['video_id'] = video.get('id', None)\n",
    "\n",
    "        # Extract details from each specified part\n",
    "        for part, fields in stats_to_keep.items():\n",
    "            for field in fields:\n",
    "                try:\n",
    "                    # Access nested fields safely\n",
    "                    video_info[field] = video.get(part, {}).get(field, None)\n",
    "                except Exception as e:\n",
    "                    video_info[field] = None\n",
    "\n",
    "        all_video_info.append(video_info)\n",
    "\n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    return pd.DataFrame(all_video_info)\n",
    "\n",
    "# Example Usage:\n",
    "# Assuming `all_videos` contains the aggregated list of video data\n",
    "all_videos = retrieve_200_videos()\n",
    "df = get_video_details(all_videos)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 183\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 40\n",
      "No more pages available.\n",
      "Total videos collected: 150\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 100\n",
      "No more pages available.\n",
      "Total videos collected: 150\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 150\n",
      "No more pages available.\n",
      "Total videos collected: 40\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 50\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 50\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 150\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 39\n",
      "No more pages available.\n",
      "Total videos collected: 50\n",
      "No more pages available.\n",
      "Total videos collected: 30\n",
      "No more pages available.\n",
      "Total videos collected: 150\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 150\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 39\n",
      "No more pages available.\n",
      "Total videos collected: 150\n",
      "No more pages available.\n",
      "Total videos collected: 150\n",
      "No more pages available.\n",
      "Total videos collected: 30\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 123\n",
      "No more pages available.\n",
      "Total videos collected: 50\n",
      "No more pages available.\n",
      "Total videos collected: 100\n",
      "No more pages available.\n",
      "Total videos collected: 100\n",
      "No more pages available.\n",
      "Total videos collected: 159\n",
      "No more pages available.\n",
      "Total videos collected: 100\n",
      "No more pages available.\n",
      "Total videos collected: 100\n",
      "No more pages available.\n",
      "Total videos collected: 100\n",
      "No more pages available.\n",
      "Total videos collected: 100\n",
      "No more pages available.\n",
      "Total videos collected: 50\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 168\n",
      "No more pages available.\n",
      "Total videos collected: 30\n",
      "No more pages available.\n",
      "Total videos collected: 50\n",
      "No more pages available.\n",
      "Total videos collected: 50\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 131\n",
      "No more pages available.\n",
      "Total videos collected: 142\n",
      "No more pages available.\n",
      "Total videos collected: 123\n",
      "No more pages available.\n",
      "Total videos collected: 181\n",
      "No more pages available.\n",
      "Total videos collected: 50\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 100\n",
      "No more pages available.\n",
      "Total videos collected: 185\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 150\n",
      "No more pages available.\n",
      "Total videos collected: 150\n",
      "No more pages available.\n",
      "Total videos collected: 185\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 50\n",
      "No more pages available.\n",
      "Total videos collected: 50\n",
      "No more pages available.\n",
      "Total videos collected: 50\n",
      "No more pages available.\n",
      "Total videos collected: 100\n",
      "No more pages available.\n",
      "Total videos collected: 100\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 183\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 100\n",
      "No more pages available.\n",
      "Total videos collected: 30\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 128\n",
      "No more pages available.\n",
      "Total videos collected: 150\n",
      "No more pages available.\n",
      "Total videos collected: 150\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 150\n",
      "No more pages available.\n",
      "Total videos collected: 39\n",
      "No more pages available.\n",
      "Total videos collected: 150\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 150\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 50\n",
      "No more pages available.\n",
      "Total videos collected: 30\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "No more pages available.\n",
      "Total videos collected: 30\n",
      "No more pages available.\n",
      "Total videos collected: 30\n",
      "No more pages available.\n",
      "Total videos collected: 100\n",
      "No more pages available.\n",
      "Total videos collected: 50\n",
      "No more pages available.\n",
      "Total videos collected: 200\n",
      "          video_id                channelTitle  \\\n",
      "0      b6FTKe-u8XI         Richard Restatement   \n",
      "1      W_19sjpQttw         Succesful Celebrity   \n",
      "2      udVz8GN_DP0                   Kieran Ta   \n",
      "3      iOecn1gSBZY                   M2M Heart   \n",
      "4      u1IuChCXplQ                 9-1-1 house   \n",
      "...            ...                         ...   \n",
      "13593  -n0IhrYBDYk                 LifeofLogos   \n",
      "13594  pr7rKo4qe2Y  Fire Department Chronicles   \n",
      "13595  jqj3z9dU_fY             TRIPLE F.  T.V.   \n",
      "13596  dYutqAk2b2M             ReelRenaissance   \n",
      "13597  8i5zS9s_NnY               Aurora Cinema   \n",
      "\n",
      "                                                   title  \\\n",
      "0      Can I get scout badge?#therookie #viralvideo #...   \n",
      "1      Bill Nye The Science Guy Shows Kai Cenat A Sci...   \n",
      "2      You‚Äôre not gonna hit me again, are you? #super...   \n",
      "3      Open the cash with moneyü§£ü•µ||This is funniest r...   \n",
      "4             He realized that they did not stop deathüò±üò≥   \n",
      "...                                                  ...   \n",
      "13593  Shane Gillis Turns On Tony Hinchcliffe!!!üòÇüòÇüòÇ| ...   \n",
      "13594  Real things I‚Äôve seen as a Paramedic. Ginger s...   \n",
      "13595  AMERICAN DAD GOT IT RIGHTüòÇ #2A #fyp #shorts #s...   \n",
      "13596  This is not a simple cold.#shorts #doctor #series   \n",
      "13597            Louise became Glenn's wife#shorts#funny   \n",
      "\n",
      "                                                    tags  \\\n",
      "0                                                   None   \n",
      "1      [Kai Cenat, Kai, Cenat, ImKaiCenat, AMP, Kai C...   \n",
      "2                                                   None   \n",
      "3                                                   None   \n",
      "4                                                   None   \n",
      "...                                                  ...   \n",
      "13593                                               None   \n",
      "13594                                               None   \n",
      "13595                                               None   \n",
      "13596                                               None   \n",
      "13597                                               None   \n",
      "\n",
      "                publishedAt viewCount likeCount dislikeCount favoriteCount  \\\n",
      "0      2024-11-20T22:42:00Z  10794179    877554         None             0   \n",
      "1      2024-11-17T17:01:47Z  18001205   1219798         None             0   \n",
      "2      2024-11-18T03:13:34Z   9926158    662739         None             0   \n",
      "3      2024-11-16T13:00:53Z  15292454   1128563         None             0   \n",
      "4      2024-11-20T19:52:08Z   7105386    439418         None             0   \n",
      "...                     ...       ...       ...          ...           ...   \n",
      "13593  2024-11-21T18:52:33Z   1399156     54085         None             0   \n",
      "13594  2024-11-19T14:15:33Z   2797964    240984         None             0   \n",
      "13595  2024-11-06T10:49:54Z   9416536    843711         None             0   \n",
      "13596  2024-11-18T15:00:45Z    853024     36275         None             0   \n",
      "13597  2024-11-13T23:00:12Z   6379147    364406         None             0   \n",
      "\n",
      "      commentCount duration definition caption  \\\n",
      "0             1504    PT59S         hd   false   \n",
      "1             5472    PT26S         hd   false   \n",
      "2             1257    PT58S         hd   false   \n",
      "3             2204    PT59S         hd   false   \n",
      "4             1255     PT1M         hd   false   \n",
      "...            ...      ...        ...     ...   \n",
      "13593          339    PT34S         hd   false   \n",
      "13594         1075    PT37S         hd   false   \n",
      "13595         5292    PT35S         hd   false   \n",
      "13596          127    PT58S         hd   false   \n",
      "13597          226    PT59S         hd   false   \n",
      "\n",
      "                                         topicCategories  \n",
      "0      [https://en.wikipedia.org/wiki/Entertainment, ...  \n",
      "1      [https://en.wikipedia.org/wiki/Entertainment, ...  \n",
      "2      [https://en.wikipedia.org/wiki/Entertainment, ...  \n",
      "3      [https://en.wikipedia.org/wiki/Entertainment, ...  \n",
      "4      [https://en.wikipedia.org/wiki/Entertainment, ...  \n",
      "...                                                  ...  \n",
      "13593  [https://en.wikipedia.org/wiki/Entertainment, ...  \n",
      "13594  [https://en.wikipedia.org/wiki/Entertainment, ...  \n",
      "13595  [https://en.wikipedia.org/wiki/Entertainment, ...  \n",
      "13596  [https://en.wikipedia.org/wiki/Entertainment, ...  \n",
      "13597  [https://en.wikipedia.org/wiki/Entertainment, ...  \n",
      "\n",
      "[13598 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for _ in range(100):\n",
    "    all_videos = retrieve_200_videos()\n",
    "    current_df = get_video_details(all_videos)\n",
    "    df = pd.concat([df, current_df],  axis=0, ignore_index=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2578"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.video_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.359800e+04\n",
       "mean     5.373338e+06\n",
       "std      1.084820e+07\n",
       "min      6.958000e+03\n",
       "25%      7.280870e+05\n",
       "50%      2.552836e+06\n",
       "75%      6.193342e+06\n",
       "max      2.172830e+08\n",
       "Name: viewCount, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"viewCount\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"popularity\"] = np.where(df[\"viewCount\"] > 6000000, \"high\", \"low\") # since the median is around 255000000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../../data/raw-data/youtube_data_raw.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include closing.qmd >}} "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
